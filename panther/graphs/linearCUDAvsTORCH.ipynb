{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b803af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import panther.nn as pnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "560f9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, use_pnn=True, k=5, l=5, mode=0):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        if use_pnn:\n",
    "            self.layer = pnn.SKLinear(\n",
    "                input_dim, output_dim, low_rank=k, num_terms=l, mode=mode\n",
    "            )\n",
    "        else:\n",
    "            self.layer = nn.Linear(input_dim, output_dim)\n",
    "        if output_dim == 1:\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce505067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_f1score(model, X_test, y_true, device=\"cuda\", average=\"weighted\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        if outputs.shape[1] > 1:  # Multi-class\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "        else:  # Binary (sigmoid)\n",
    "            preds = (outputs > 0.5).long().squeeze()\n",
    "\n",
    "    y_pred = preds.cpu().numpy()\n",
    "    return f1_score(y_true, y_pred, average=average)\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, epochs=10, lr=0.001, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model on the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - model: A PyTorch model.\n",
    "    - X_train: Input features (numpy or tensor).\n",
    "    - y_train: Labels (numpy or tensor).\n",
    "    - epochs: Number of training epochs.\n",
    "    - lr: Learning rate.\n",
    "    - device: 'cpu' or 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "    - Trained model.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Convert to tensors if not already\n",
    "    if not isinstance(X_train, torch.Tensor):\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    if not isinstance(y_train, torch.Tensor):\n",
    "        y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    print(\"im new\")\n",
    "    criterion = nn.CrossEntropyLoss() if model(X_train[:1]).dim() > 1 else nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    print(\"im new2\")\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        print(\"hi\")\n",
    "        outputs = model(X_train)\n",
    "\n",
    "        if outputs.dim() == 1 or outputs.shape[1] == 1:  # Binary classification\n",
    "            outputs = outputs.squeeze()\n",
    "            y_train = y_train.float()\n",
    "\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"hi2\")\n",
    "        if epoch % (epochs // 5) == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def time_forward_pass(model, X, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    if not isinstance(X, torch.Tensor):\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "    X = X.to(device)\n",
    "\n",
    "    # Warm-up (optional, helps with accurate timing especially on GPU)\n",
    "    with torch.no_grad():\n",
    "        _ = model(X)\n",
    "\n",
    "    # CUDA sync before and after for accurate timing\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(X)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    return elapsed\n",
    "\n",
    "\n",
    "def train_evaluate_time(model, X, y):\n",
    "    # trained_model = train_model(model, X, y)\n",
    "    # print(\"Model trained.\")\n",
    "    f1 = test_model_f1score(model, X, y)\n",
    "    print(\"Model evaluated.\")\n",
    "    elapsed_time = time_forward_pass(model, X)\n",
    "    print(\"Forward pass time measured.\")\n",
    "    return f1, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7581388",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m input_dim = \u001b[32m10\u001b[39m\n\u001b[32m      2\u001b[39m output_dim = \u001b[32m2\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m model = SimpleModel(input_dim, output_dim, use_pnn=\u001b[38;5;28;01mFalse\u001b[39;00m, k=\u001b[32m5\u001b[39m, l=\u001b[32m5\u001b[39m, mode=\u001b[32m0\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\gam3a\\gp\\panther\\.venv\\Lib\\site-packages\\torch\\cuda\\memory.py:218\u001b[39m, in \u001b[36mempty_cache\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[33;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[33;03m`nvidia-smi`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m \u001b[33;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "input_dim = 10\n",
    "output_dim = 2\n",
    "model = SimpleModel(input_dim, output_dim, use_pnn=False, k=5, l=5, mode=0)\n",
    "print(model)\n",
    "X_train = torch.randn(100, input_dim)  # Example training data\n",
    "y_train = torch.randint(0, output_dim, (100,))\n",
    "print(\"Training and evaluating model...\")\n",
    "train_evaluate_time(model, X_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
