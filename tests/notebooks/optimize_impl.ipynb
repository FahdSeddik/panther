{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888a5ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using z_threshold = 1.960 for 95% confidence.\n",
      "Merged dataset has 575 records.\n",
      "Category distribution:\n",
      " runtime_category\n",
      "similar          324\n",
      "model2 faster    175\n",
      "model1 faster     76\n",
      "Name: count, dtype: int64\n",
      "Training accuracy: 0.846\n",
      "Test accuracy: 0.835\n",
      "\n",
      "Decision rules (0=model1 faster, 1=similar, 2=model2 faster):\n",
      "\n",
      "|--- num_terms <= 1.50\n",
      "|   |--- in_features <= 4608.00\n",
      "|   |   |--- out_features <= 24576.00\n",
      "|   |   |   |--- class: 0\n",
      "|   |   |--- out_features >  24576.00\n",
      "|   |   |   |--- class: 1\n",
      "|   |--- in_features >  4608.00\n",
      "|   |   |--- in_features <= 12288.00\n",
      "|   |   |   |--- class: 2\n",
      "|   |   |--- in_features >  12288.00\n",
      "|   |   |   |--- class: 2\n",
      "|--- num_terms >  1.50\n",
      "|   |--- in_features <= 12288.00\n",
      "|   |   |--- low_rank <= 96.00\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- low_rank >  96.00\n",
      "|   |   |   |--- class: 2\n",
      "|   |--- in_features >  12288.00\n",
      "|   |   |--- out_features <= 4608.00\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- out_features >  4608.00\n",
      "|   |   |   |--- class: 1\n",
      "\n",
      "\n",
      "Feature importances:\n",
      "in_features     0.484793\n",
      "num_terms       0.309445\n",
      "out_features    0.104925\n",
      "low_rank        0.100837\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "# from scipy.stats import t\n",
    "\n",
    "# Configuration: replace with your actual file paths\n",
    "csv1_path = \"sklinear_tc_fc_htorch_allcuda.csv\"\n",
    "csv2_path = \"sklinear_tc_fc_htorch_htorch.csv\"\n",
    "feature_cols = [\"in_features\", \"out_features\", \"num_terms\", \"low_rank\"]\n",
    "max_depth = 3\n",
    "random_state = 42\n",
    "\n",
    "speedup_threshold = 0.10  # ±10% relative difference\n",
    "max_std = None  # e.g., 5.0 to drop runs with >5ms std\n",
    "\n",
    "# Confidence-based z-threshold\n",
    "# Choose a confidence level for requiring performance differences:\n",
    "confidence_level = 0.95  # e.g., 0.95 for 95% confidence\n",
    "# For normally distributed differences, z = norm.ppf(1 - alpha/2)\n",
    "alpha = 1 - confidence_level\n",
    "z_thresh = norm.ppf(1 - alpha / 2)\n",
    "# Typical z-thresholds: 68%→1.0, 90%→1.645, 95%→1.96, 99%→2.576\n",
    "print(f\"Using z_threshold = {z_thresh:.3f} for {confidence_level*100:.0f}% confidence.\")\n",
    "\n",
    "# If sample sizes (n_runs) are known and small (<30), consider using Student's t:\n",
    "# df = n_runs - 1; t_threshold = t.ppf(1 - alpha/2, df)\n",
    "\n",
    "# --- Load and clean ---\n",
    "df1 = pd.read_csv(csv1_path).rename(\n",
    "    columns={\"forward_mean_ms\": \"time_model1\", \"forward_std_ms\": \"std_model1\"}\n",
    ")\n",
    "df2 = pd.read_csv(csv2_path).rename(\n",
    "    columns={\"forward_mean_ms\": \"time_model2\", \"forward_std_ms\": \"std_model2\"}\n",
    ")\n",
    "\n",
    "# Drop missing values\n",
    "df1 = df1.dropna(subset=[\"time_model1\", \"std_model1\"])\n",
    "df2 = df2.dropna(subset=[\"time_model2\", \"std_model2\"])\n",
    "\n",
    "# Optionally filter out high-variance runs if max_std is set\n",
    "if max_std is not None:\n",
    "    df1 = df1[df1[\"std_model1\"] <= max_std]\n",
    "    df2 = df2[df2[\"std_model2\"] <= max_std]\n",
    "\n",
    "# Merge on feature columns\n",
    "df = pd.merge(df1, df2, on=feature_cols, how=\"inner\")\n",
    "print(f\"Merged dataset has {len(df)} records.\")\n",
    "\n",
    "# Compute combined standard deviation\n",
    "num_runs = 200\n",
    "df[\"combined_std\"] = np.sqrt((df[\"std_model1\"] ** 2 + df[\"std_model2\"] ** 2) / num_runs)\n",
    "\n",
    "# --- Define runtime categories (0=model1 faster, 1=similar, 2=model2 faster) ---\n",
    "lower = (1 - speedup_threshold) * df[\"time_model2\"]\n",
    "upper = (1 + speedup_threshold) * df[\"time_model2\"]\n",
    "diff = df[\"time_model1\"] - df[\"time_model2\"]\n",
    "\n",
    "# Apply both percentage and confidence criteria\n",
    "cond_fast1 = (df[\"time_model1\"] < lower) & (diff <= -z_thresh * df[\"combined_std\"])\n",
    "cond_fast2 = (df[\"time_model1\"] > upper) & (diff >= z_thresh * df[\"combined_std\"])\n",
    "conditions = [cond_fast1, cond_fast2]\n",
    "choices = [0, 2]\n",
    "df[\"runtime_category\"] = np.select(conditions, choices, default=1)\n",
    "\n",
    "# Display distribution\n",
    "dist = df[\"runtime_category\"].map(\n",
    "    {0: \"model1 faster\", 1: \"similar\", 2: \"model2 faster\"}\n",
    ")\n",
    "print(\"Category distribution:\\n\", dist.value_counts())\n",
    "\n",
    "# --- Feature preparation ---\n",
    "X = df[feature_cols]\n",
    "y = df[\"runtime_category\"]\n",
    "\n",
    "# --- Train multi-class decision tree ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    ")\n",
    "clf = DecisionTreeClassifier(max_depth=max_depth, random_state=random_state)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Training accuracy: {clf.score(X_train,y_train):.3f}\")\n",
    "print(f\"Test accuracy: {clf.score(X_test,y_test):.3f}\")\n",
    "\n",
    "# Print rules\n",
    "tree_rules = export_text(clf, feature_names=feature_cols)\n",
    "print(\"\\nDecision rules (0=model1 faster, 1=similar, 2=model2 faster):\\n\")\n",
    "print(tree_rules)\n",
    "\n",
    "# Feature importances\n",
    "importances = pd.Series(clf.feature_importances_, index=feature_cols)\n",
    "print(\"\\nFeature importances:\")\n",
    "print(importances.sort_values(ascending=False))\n",
    "\n",
    "# --- Considerations for 'similar' category ---\n",
    "# - Exclude 'similar' runs for binary analysis.\n",
    "# - Tune `confidence_level` to adjust sensitivity.\n",
    "# - Use memory as another feature for better classification.\n",
    "# - If n_runs per record available, use Student's t for small-sample corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e09081a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
