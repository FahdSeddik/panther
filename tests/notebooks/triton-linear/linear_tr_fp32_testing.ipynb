{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T00:20:19.257547Z",
     "iopub.status.busy": "2025-05-01T00:20:19.256796Z",
     "iopub.status.idle": "2025-05-01T00:20:22.593548Z",
     "shell.execute_reply": "2025-05-01T00:20:22.592853Z",
     "shell.execute_reply.started": "2025-05-01T00:20:19.257511Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Looking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu118)\nRequirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.11/dist-packages (0.21.0+cu118)\nRequirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu118)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (11.8.89)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (11.8.89)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (11.8.87)\nRequirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (11.11.3.6)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (10.3.0.86)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (11.4.1.48)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (11.7.5.86)\nRequirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (11.8.86)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (1.13.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0) (11.1.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.21.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.21.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.21.0) (2024.2.0)\n"
    }
   ],
   "source": [
    "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T00:51:37.138093Z",
     "iopub.status.busy": "2025-05-01T00:51:37.137749Z",
     "iopub.status.idle": "2025-05-01T00:51:37.174738Z",
     "shell.execute_reply": "2025-05-01T00:51:37.174069Z",
     "shell.execute_reply.started": "2025-05-01T00:51:37.138071Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# def getConfigs(names, ranges, num_stages_range, num_warps_range):\n",
    "#     configs = [\n",
    "#         triton.Config({f'{names[0]}': x0, f'{names[1]}': x1, f'{names[2]}': x2, f'{names[3]}': x3}, num_stages=s, num_warps=w) \\\n",
    "#         for x0 in ranges[0]\\\n",
    "#         for x1 in ranges[1]\\\n",
    "#         for x2 in ranges[2]\\\n",
    "#         for x3 in ranges[3]\\\n",
    "#         for s in num_stages_range\\\n",
    "#         for w in num_warps_range\\\n",
    "#     ]\n",
    "#     return configs\n",
    "\n",
    "# ranges = [[32, 64, 128, 256, 512, 1024], [32, 64, 128, 256, 512, 1024], [32, 64, 128, 256, 512, 1024], [2,4,8]]\n",
    "# num_stages_range = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "# num_warps_range = [2, 4, 8, 16, 32]\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 32,\n",
    "                \"BLOCK_SIZE_K\": 32,\n",
    "                \"BLOCK_SIZE_D2\": 64,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=1,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4)\n",
    "    ],\n",
    "    # configs=getConfigs(['BLOCK_SIZE_BSIZE', 'BLOCK_SIZE_K', 'BLOCK_SIZE_D2', 'GROUP_SIZE_BSIZE'], ranges, num_stages_range, num_warps_range),\n",
    "    key=[\"BSIZE\", \"K\", \"d2\", \"L\"],\n",
    ")\n",
    "@triton.jit\n",
    "def first_pass_kernel(\n",
    "    hin_ptr,\n",
    "    S1s_ptr,\n",
    "    U2s_ptr,\n",
    "    out1_ptr,\n",
    "    out2_ptr,\n",
    "    BSIZE,\n",
    "    K,\n",
    "    d2,\n",
    "    L,\n",
    "    stride_hin_bsize,\n",
    "    stride_hin_d2,\n",
    "    stride_su_l,\n",
    "    stride_su_d2,\n",
    "    stride_su_k,\n",
    "    stride_out_l,\n",
    "    stride_out_bsize,\n",
    "    stride_out_k,\n",
    "    BLOCK_SIZE_BSIZE: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    BLOCK_SIZE_D2: tl.constexpr,\n",
    "    GROUP_SIZE_BSIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=1)\n",
    "    batch_id = tl.program_id(axis=0)\n",
    "\n",
    "    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n",
    "    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n",
    "    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_k\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n",
    "    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n",
    "    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n",
    "    pid_k = (pid % num_pid_in_group) // group_size_bsize\n",
    "\n",
    "    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "    offs_k = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
    "    offs_d2 = tl.arange(0, BLOCK_SIZE_D2)\n",
    "\n",
    "    offs_bsize = tl.max_contiguous(\n",
    "        tl.multiple_of(offs_bsize, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE\n",
    "    )\n",
    "    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_K), BLOCK_SIZE_K)\n",
    "    offs_d2 = tl.max_contiguous(tl.multiple_of(offs_d2, BLOCK_SIZE_D2), BLOCK_SIZE_D2)\n",
    "\n",
    "    hin_ptrs = hin_ptr + (\n",
    "        offs_bsize[:, None] * stride_hin_bsize + offs_d2[None, :] * stride_hin_d2\n",
    "    )\n",
    "\n",
    "    su_tmp = batch_id * stride_su_l + (\n",
    "        offs_d2[:, None] * stride_su_d2 + offs_k[None, :] * stride_su_k\n",
    "    )\n",
    "    S1s_ptrs = S1s_ptr + su_tmp\n",
    "    U2s_ptrs = U2s_ptr + su_tmp\n",
    "\n",
    "    accumulator1 = tl.full(\n",
    "        shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), value=0.0, dtype=tl.float32\n",
    "    )\n",
    "    accumulator2 = tl.full(\n",
    "        shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), value=0.0, dtype=tl.float32\n",
    "    )\n",
    "\n",
    "    for d2_i in range(0, tl.cdiv(d2, BLOCK_SIZE_D2)):\n",
    "        hin_mask = (offs_bsize[:, None] < BSIZE) & (\n",
    "            offs_d2[None, :] < d2 - d2_i * BLOCK_SIZE_D2\n",
    "        )\n",
    "        hin = tl.load(hin_ptrs, mask=hin_mask, other=0.0)\n",
    "\n",
    "        su_mask = (offs_d2[:, None] < d2 - d2_i * BLOCK_SIZE_D2) & (offs_k[None, :] < K)\n",
    "        S1s = tl.load(S1s_ptrs, mask=su_mask, other=0.0)\n",
    "        U2s = tl.load(U2s_ptrs, mask=su_mask, other=0.0)\n",
    "\n",
    "        accumulator1 += tl.dot(hin, S1s, input_precision=\"ieee\")\n",
    "        accumulator2 += tl.dot(hin, U2s, input_precision=\"ieee\")\n",
    "\n",
    "        hin_ptrs += BLOCK_SIZE_D2 * stride_hin_d2\n",
    "        S1s_ptrs += BLOCK_SIZE_D2 * stride_su_d2\n",
    "        U2s_ptrs += BLOCK_SIZE_D2 * stride_su_d2\n",
    "\n",
    "    out_tmp = (\n",
    "        batch_id * stride_out_l\n",
    "        + stride_out_bsize * offs_bsize[:, None]\n",
    "        + stride_out_k * offs_k[None, :]\n",
    "    )\n",
    "    out1_ptrs = out1_ptr + out_tmp\n",
    "    out2_ptrs = out2_ptr + out_tmp\n",
    "\n",
    "    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_k[None, :] < K)\n",
    "\n",
    "    tl.store(out1_ptrs, accumulator1, mask=out_mask)\n",
    "    tl.store(out2_ptrs, accumulator2, mask=out_mask)\n",
    "\n",
    "\n",
    "def first_pass(hin, S1s, U2s):\n",
    "    device = \"cuda\"\n",
    "    # assert hin.shape[1] == S1s.shape[1], \"Incompatible dimensions\"\n",
    "    # assert hin.shape[1] == U2s.shape[1], \"Incompatible dimensions\"\n",
    "    # assert hin.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert S1s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert U2s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert S1s.stride() == U2s.stride(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    BSIZE, d2 = hin.shape\n",
    "    L, _, K = S1s.shape\n",
    "\n",
    "    out1 = torch.empty((L, BSIZE, K), dtype=torch.float32, device=device)\n",
    "    out2 = torch.empty((L, BSIZE, K), dtype=torch.float32, device=device)\n",
    "\n",
    "    # stride_hin_bsize, stride_hin_d2 = hin.stride()\n",
    "    # stride_su_l, stride_su_d2, stride_su_k = S1s.stride()\n",
    "    # stride_out_l, stride_out_bsize, stride_out_k = out1.stride()\n",
    "    stride_hin_bsize, stride_hin_d2 = hin.shape[1], 1\n",
    "    stride_su_l, stride_su_d2, stride_su_k = (\n",
    "        S1s.shape[1] * S1s.shape[2],\n",
    "        S1s.shape[2],\n",
    "        1,\n",
    "    )\n",
    "    stride_out_l, stride_out_bsize, stride_out_k = (\n",
    "        out1.shape[1] * out1.shape[2],\n",
    "        out1.shape[2],\n",
    "        1,\n",
    "    )\n",
    "\n",
    "    # assert out1.stride() == out2.stride(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    grid = lambda META: (\n",
    "        L,\n",
    "        triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"])\n",
    "        * triton.cdiv(K, META[\"BLOCK_SIZE_K\"]),\n",
    "    )\n",
    "\n",
    "    first_pass_kernel[grid](\n",
    "        hin,\n",
    "        S1s,\n",
    "        U2s,\n",
    "        out1,\n",
    "        out2,\n",
    "        BSIZE,\n",
    "        K,\n",
    "        d2,\n",
    "        L,\n",
    "        stride_hin_bsize,\n",
    "        stride_hin_d2,\n",
    "        stride_su_l,\n",
    "        stride_su_d2,\n",
    "        stride_su_k,\n",
    "        stride_out_l,\n",
    "        stride_out_bsize,\n",
    "        stride_out_k,\n",
    "    )\n",
    "\n",
    "    return out1, out2\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 32,\n",
    "                \"BLOCK_SIZE_D1\": 32,\n",
    "                \"BLOCK_SIZE_K\": 32,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=1,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4)\n",
    "    ],\n",
    "    key=[\"BSIZE\", \"d1\", \"K\", \"L\"],\n",
    ")\n",
    "@triton.jit\n",
    "def second_pass_kernel(\n",
    "    in1_ptr,\n",
    "    in2_ptr,\n",
    "    U1s_ptr,\n",
    "    S2s_ptr,\n",
    "    bias_ptr,\n",
    "    out_ptr,\n",
    "    BSIZE,\n",
    "    d1,\n",
    "    K,\n",
    "    L,\n",
    "    stride_in12_l,\n",
    "    stride_in12_bsize,\n",
    "    stride_in12_k,\n",
    "    stride_us_l,\n",
    "    stride_us_k,\n",
    "    stride_us_d1,\n",
    "    stride_bias_bsize,\n",
    "    stride_bias_d1,\n",
    "    stride_out_bsize,\n",
    "    stride_out_d1,\n",
    "    BLOCK_SIZE_BSIZE: tl.constexpr,\n",
    "    BLOCK_SIZE_D1: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_BSIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n",
    "    num_pid_d1 = tl.cdiv(d1, BLOCK_SIZE_D1)\n",
    "    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_d1\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n",
    "    GROUP_SIZE_BSIZE = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n",
    "    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % GROUP_SIZE_BSIZE)\n",
    "    pid_d1 = (pid % num_pid_in_group) // GROUP_SIZE_BSIZE\n",
    "\n",
    "    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "    offs_d1 = pid_d1 * BLOCK_SIZE_D1 + tl.arange(0, BLOCK_SIZE_D1)\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "\n",
    "    offs_bsize = tl.max_contiguous(\n",
    "        tl.multiple_of(offs_bsize, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE\n",
    "    )\n",
    "    offs_d1 = tl.max_contiguous(tl.multiple_of(offs_d1, BLOCK_SIZE_D1), BLOCK_SIZE_D1)\n",
    "    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_K), BLOCK_SIZE_K)\n",
    "\n",
    "    in_tmp = offs_bsize[:, None] * stride_in12_bsize + offs_k[None, :] * stride_in12_k\n",
    "    us_tmp = offs_k[:, None] * stride_us_k + offs_d1[None, :] * stride_us_d1\n",
    "\n",
    "    accumulator = tl.full(\n",
    "        shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_D1), value=0.0, dtype=tl.float32\n",
    "    )\n",
    "\n",
    "    for l in range(0, L):\n",
    "        l_in_offset = l * stride_in12_l\n",
    "        l_us_offset = l * stride_us_l\n",
    "\n",
    "        in1_ptrs = in1_ptr + l_in_offset + in_tmp\n",
    "        in2_ptrs = in2_ptr + l_in_offset + in_tmp\n",
    "\n",
    "        U1s_ptrs = U1s_ptr + l_us_offset + us_tmp\n",
    "        S2s_ptrs = S2s_ptr + l_us_offset + us_tmp\n",
    "\n",
    "        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "            in_mask = offs_k[None, :] < K - k * BLOCK_SIZE_K\n",
    "            in1 = tl.load(in1_ptrs, mask=in_mask, other=0.0)\n",
    "            in2 = tl.load(in2_ptrs, mask=in_mask, other=0.0)\n",
    "\n",
    "            us_mask = offs_k[:, None] < K - k * BLOCK_SIZE_K\n",
    "            U1s = tl.load(U1s_ptrs, mask=us_mask, other=0.0)\n",
    "            S2s = tl.load(S2s_ptrs, mask=us_mask, other=0.0)\n",
    "\n",
    "            accumulator += tl.dot(in1, U1s, input_precision=\"ieee\")\n",
    "            accumulator += tl.dot(in2, S2s, input_precision=\"ieee\")\n",
    "\n",
    "            in_inc = BLOCK_SIZE_K * stride_in12_k\n",
    "            in1_ptrs += in_inc\n",
    "            in2_ptrs += in_inc\n",
    "\n",
    "            us_inc = BLOCK_SIZE_K * stride_us_k\n",
    "            U1s_ptrs += us_inc\n",
    "            S2s_ptrs += us_inc\n",
    "\n",
    "    bias_ptrs = bias_ptr + offs_d1[None, :] * stride_bias_d1\n",
    "    bias_mask = offs_d1[None, :] < d1\n",
    "    bias = tl.load(bias_ptrs, mask=bias_mask, other=0.0)\n",
    "\n",
    "    accumulator *= 1.0 / (2.0 * L)\n",
    "    accumulator += bias\n",
    "\n",
    "    out_ptrs = (\n",
    "        out_ptr\n",
    "        + stride_out_bsize * offs_bsize[:, None]\n",
    "        + stride_out_d1 * offs_d1[None, :]\n",
    "    )\n",
    "    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_d1[None, :] < d1)\n",
    "\n",
    "    tl.store(out_ptrs, accumulator, mask=out_mask)\n",
    "\n",
    "\n",
    "def second_pass(in1, in2, U1s, S2s, bias):\n",
    "    # assert in1.shape[2] == U1s.shape[1], \"Incompatible dimensions\"\n",
    "    # assert in2.shape[2] == S2s.shape[1], \"Incompatible dimensions\"\n",
    "    # assert in1.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert in2.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert U1s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert S2s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert bias.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert U1s.stride() == S2s.stride(), \"Matrix A must be contiguous\"\n",
    "    # assert in1.stride() == in2.stride(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    L, BSIZE, K = in1.shape\n",
    "    _, _, d1 = U1s.shape\n",
    "\n",
    "    out = torch.empty((BSIZE, d1), dtype=torch.float32, device=device)\n",
    "\n",
    "    # stride_in12_l, stride_in12_bsize, stride_in12_k = in1.stride()\n",
    "    # stride_us_l, stride_us_k, stride_us_d1 = U1s.stride()\n",
    "    # stride_bias_bsize, stride_bias_d1 = bias.stride()\n",
    "    # stride_out_bsize, stride_out_d1 = out.stride()\n",
    "    stride_in12_l, stride_in12_bsize, stride_in12_k = (\n",
    "        in1.shape[1] * in1.shape[2],\n",
    "        in1.shape[2],\n",
    "        1,\n",
    "    )\n",
    "    stride_us_l, stride_us_k, stride_us_d1 = (\n",
    "        U1s.shape[1] * U1s.shape[2],\n",
    "        U1s.shape[2],\n",
    "        1,\n",
    "    )\n",
    "    stride_bias_bsize, stride_bias_d1 = bias.shape[1], 1\n",
    "    stride_out_bsize, stride_out_d1 = out.shape[1], 1\n",
    "\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"])\n",
    "        * triton.cdiv(d1, META[\"BLOCK_SIZE_D1\"]),\n",
    "    )\n",
    "\n",
    "    second_pass_kernel[grid](\n",
    "        in1,\n",
    "        in2,\n",
    "        U1s,\n",
    "        S2s,\n",
    "        bias,\n",
    "        out,\n",
    "        BSIZE,\n",
    "        d1,\n",
    "        K,\n",
    "        L,\n",
    "        stride_in12_l,\n",
    "        stride_in12_bsize,\n",
    "        stride_in12_k,\n",
    "        stride_us_l,\n",
    "        stride_us_k,\n",
    "        stride_us_d1,\n",
    "        stride_bias_bsize,\n",
    "        stride_bias_d1,\n",
    "        stride_out_bsize,\n",
    "        stride_out_d1,\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:25:21.094754Z",
     "iopub.status.busy": "2025-05-01T01:25:21.094150Z",
     "iopub.status.idle": "2025-05-01T01:25:21.167412Z",
     "shell.execute_reply": "2025-05-01T01:25:21.166850Z",
     "shell.execute_reply.started": "2025-05-01T01:25:21.094731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 32,\n",
    "                \"BLOCK_SIZE_K\": 32,\n",
    "                \"BLOCK_SIZE_d1\": 32,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=1,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5,\n",
    "        #                num_warps=2),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5,\n",
    "        #                num_warps=2),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3,\n",
    "        #                num_warps=8),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3,\n",
    "        #                num_warps=8),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4)\n",
    "    ],\n",
    "    key=[\"BSIZE\", \"K\", \"d1\", \"L\"],\n",
    ")\n",
    "@triton.jit\n",
    "def first_pass_gU1s_g_S2s_kernel(\n",
    "    g_ptr,\n",
    "    U1s_ptr,\n",
    "    S2s_ptr,\n",
    "    g_U1s_ptr,\n",
    "    g_S2s_ptr,\n",
    "    BSIZE,\n",
    "    K,\n",
    "    d1,\n",
    "    L,\n",
    "    stride_g_bsize,\n",
    "    stride_g_d1,\n",
    "    stride_su_l,\n",
    "    stride_su_d1,\n",
    "    stride_su_k,\n",
    "    stride_out_l,\n",
    "    stride_out_bsize,\n",
    "    stride_out_k,\n",
    "    BLOCK_SIZE_BSIZE: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    BLOCK_SIZE_d1: tl.constexpr,\n",
    "    GROUP_SIZE_BSIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=1)\n",
    "    batch_id = tl.program_id(axis=0)\n",
    "\n",
    "    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n",
    "    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n",
    "    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_k\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n",
    "    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n",
    "    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n",
    "    pid_k = (pid % num_pid_in_group) // group_size_bsize\n",
    "\n",
    "    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "    offs_k = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
    "    offs_d1 = tl.arange(0, BLOCK_SIZE_d1)\n",
    "\n",
    "    g_ptrs = g_ptr + (\n",
    "        offs_bsize[:, None] * stride_g_bsize + offs_d1[None, :] * stride_g_d1\n",
    "    )\n",
    "\n",
    "    su_tmp = batch_id * stride_su_l + (\n",
    "        offs_d1[:, None] * stride_su_d1 + offs_k[None, :] * stride_su_k\n",
    "    )\n",
    "    U1s_ptrs = U1s_ptr + su_tmp\n",
    "    S2s_ptrs = S2s_ptr + su_tmp\n",
    "\n",
    "    accumulator1 = tl.full(\n",
    "        shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), value=0.0, dtype=tl.float32\n",
    "    )\n",
    "    accumulator2 = tl.full(\n",
    "        shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), value=0.0, dtype=tl.float32\n",
    "    )\n",
    "\n",
    "    for d1_i in range(0, tl.cdiv(d1, BLOCK_SIZE_d1)):\n",
    "        g = tl.load(\n",
    "            g_ptrs, mask=(offs_d1[None, :] < d1 - d1_i * BLOCK_SIZE_d1), other=0.0\n",
    "        )\n",
    "\n",
    "        su_mask = offs_d1[:, None] < d1 - d1_i * BLOCK_SIZE_d1\n",
    "        U1s = tl.load(U1s_ptrs, mask=su_mask, other=0.0)\n",
    "        S2s = tl.load(S2s_ptrs, mask=su_mask, other=0.0)\n",
    "\n",
    "        accumulator1 += tl.dot(g, U1s, input_precision=\"ieee\")\n",
    "        accumulator2 += tl.dot(g, S2s, input_precision=\"ieee\")\n",
    "\n",
    "        g_ptrs += BLOCK_SIZE_d1 * stride_g_d1\n",
    "        U1s_ptrs += BLOCK_SIZE_d1 * stride_su_d1\n",
    "        S2s_ptrs += BLOCK_SIZE_d1 * stride_su_d1\n",
    "\n",
    "    out_tmp = (\n",
    "        batch_id * stride_out_l\n",
    "        + stride_out_bsize * offs_bsize[:, None]\n",
    "        + stride_out_k * offs_k[None, :]\n",
    "    )\n",
    "    g_U1s_ptrs = g_U1s_ptr + out_tmp\n",
    "    g_S2s_ptrs = g_S2s_ptr + out_tmp\n",
    "\n",
    "    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_k[None, :] < K)\n",
    "\n",
    "    tl.store(g_U1s_ptrs, accumulator1, mask=out_mask)\n",
    "    tl.store(g_S2s_ptrs, accumulator2, mask=out_mask)\n",
    "\n",
    "\n",
    "def first_pass_gU1s_g_S2s(g, U1s, S2s):\n",
    "    # assert g.shape[1] == U1s.shape[1], \"Incompatible dimensions\"\n",
    "    # assert g.shape[1] == S2s.shape[1], \"Incompatible dimensions\"\n",
    "    # assert g.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert U1s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert S2s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert U1s.stride() == S2s.stride(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    BSIZE, d1 = g.shape\n",
    "    L, _, K = U1s.shape\n",
    "\n",
    "    g_U1s = torch.empty((L, BSIZE, K), dtype=torch.float32, device=\"cuda\")\n",
    "    g_S2s = torch.empty((L, BSIZE, K), dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    # stride_g_bsize, stride_g_d1 = g.stride()\n",
    "    # stride_su_l, stride_su_d1, stride_su_k = U1s.stride()\n",
    "    # stride_out_l, stride_out_bsize, stride_out_k = g_U1s.stride()\n",
    "    stride_g_bsize, stride_g_d1 = g.shape[1], 1\n",
    "    stride_su_l, stride_su_d1, stride_su_k = (\n",
    "        U1s.shape[1] * U1s.shape[2],\n",
    "        U1s.shape[2],\n",
    "        1,\n",
    "    )\n",
    "    stride_out_l, stride_out_bsize, stride_out_k = (\n",
    "        g_U1s.shape[1] * g_U1s.shape[2],\n",
    "        g_U1s.shape[2],\n",
    "        1,\n",
    "    )\n",
    "\n",
    "    # assert g_U1s.stride() == g_S2s.stride(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    grid = lambda META: (\n",
    "        L,\n",
    "        triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"])\n",
    "        * triton.cdiv(K, META[\"BLOCK_SIZE_K\"]),\n",
    "    )\n",
    "\n",
    "    first_pass_gU1s_g_S2s_kernel[grid](\n",
    "        g,\n",
    "        U1s,\n",
    "        S2s,\n",
    "        g_U1s,\n",
    "        g_S2s,\n",
    "        BSIZE,\n",
    "        K,\n",
    "        d1,\n",
    "        L,\n",
    "        stride_g_bsize,\n",
    "        stride_g_d1,\n",
    "        stride_su_l,\n",
    "        stride_su_d1,\n",
    "        stride_su_k,\n",
    "        stride_out_l,\n",
    "        stride_out_bsize,\n",
    "        stride_out_k,\n",
    "    )\n",
    "\n",
    "    return g_U1s, g_S2s\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 32,\n",
    "                \"BLOCK_SIZE_d2\": 32,\n",
    "                \"BLOCK_SIZE_K\": 32,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=1,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5,\n",
    "        #                num_warps=2),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5,\n",
    "        #                num_warps=2),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3,\n",
    "        #                num_warps=8),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3,\n",
    "        #                num_warps=8),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4),\n",
    "        #  triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n",
    "        #                num_warps=4)\n",
    "    ],\n",
    "    key=[\"BSIZE\", \"d2\", \"K\", \"L\"],\n",
    ")\n",
    "@triton.jit\n",
    "def second_pass_gUS11_22_kernel(\n",
    "    g_U1s_ptr,\n",
    "    g_S2s_ptr,\n",
    "    S1s_ptr,\n",
    "    U2s_ptr,\n",
    "    out_ptr,\n",
    "    BSIZE,\n",
    "    d2,\n",
    "    K,\n",
    "    L,\n",
    "    stride_g_U1s2_l,\n",
    "    stride_g_U1s2_bsize,\n",
    "    stride_g_U1s2_k,\n",
    "    stride_us_l,\n",
    "    stride_us_k,\n",
    "    stride_us_d2,\n",
    "    stride_out_bsize,\n",
    "    stride_out_d2,\n",
    "    BLOCK_SIZE_BSIZE: tl.constexpr,\n",
    "    BLOCK_SIZE_d2: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_BSIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n",
    "    num_pid_d2 = tl.cdiv(d2, BLOCK_SIZE_d2)\n",
    "    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_d2\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n",
    "    GROUP_SIZE_BSIZE = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n",
    "    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % GROUP_SIZE_BSIZE)\n",
    "    pid_d2 = (pid % num_pid_in_group) // GROUP_SIZE_BSIZE\n",
    "\n",
    "    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "    offs_d2 = pid_d2 * BLOCK_SIZE_d2 + tl.arange(0, BLOCK_SIZE_d2)\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "\n",
    "    in_tmp = (\n",
    "        offs_bsize[:, None] * stride_g_U1s2_bsize + offs_k[None, :] * stride_g_U1s2_k\n",
    "    )\n",
    "    us_tmp = offs_k[:, None] * stride_us_k + offs_d2[None, :] * stride_us_d2\n",
    "\n",
    "    accumulator = tl.full(\n",
    "        shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_d2), value=0.0, dtype=tl.float32\n",
    "    )\n",
    "\n",
    "    for l in range(0, L):\n",
    "        g_l_offset = l * stride_g_U1s2_l  # Offset for g_U1s and g_S2s\n",
    "        s_l_offset = l * stride_us_l  # Offset for S1s and U2s\n",
    "\n",
    "        g_U1s_ptrs = g_U1s_ptr + g_l_offset + in_tmp\n",
    "        g_S2s_ptrs = g_S2s_ptr + g_l_offset + in_tmp\n",
    "\n",
    "        S1s_ptrs = S1s_ptr + s_l_offset + us_tmp\n",
    "        U2s_ptrs = U2s_ptr + s_l_offset + us_tmp\n",
    "\n",
    "        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "            in_mask = offs_k[None, :] < K - k * BLOCK_SIZE_K\n",
    "            g_U1s = tl.load(g_U1s_ptrs, mask=in_mask, other=0.0)\n",
    "            g_S2s = tl.load(g_S2s_ptrs, mask=in_mask, other=0.0)\n",
    "\n",
    "            us_mask = offs_k[:, None] < K - k * BLOCK_SIZE_K\n",
    "            S1s = tl.load(S1s_ptrs, mask=us_mask, other=0.0)\n",
    "            U2s = tl.load(U2s_ptrs, mask=us_mask, other=0.0)\n",
    "\n",
    "            accumulator += tl.dot(g_U1s, S1s, input_precision=\"ieee\")\n",
    "            accumulator += tl.dot(g_S2s, U2s, input_precision=\"ieee\")\n",
    "\n",
    "            in_inc = BLOCK_SIZE_K * stride_g_U1s2_k\n",
    "            g_U1s_ptrs += in_inc\n",
    "            g_S2s_ptrs += in_inc\n",
    "\n",
    "            us_inc = BLOCK_SIZE_K * stride_us_k\n",
    "            S1s_ptrs += us_inc\n",
    "            U2s_ptrs += us_inc\n",
    "\n",
    "    accumulator *= 1.0 / (2.0 * L)\n",
    "\n",
    "    out_ptrs = (\n",
    "        out_ptr\n",
    "        + stride_out_bsize * offs_bsize[:, None]\n",
    "        + stride_out_d2 * offs_d2[None, :]\n",
    "    )\n",
    "    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_d2[None, :] < d2)\n",
    "\n",
    "    tl.store(out_ptrs, accumulator, mask=out_mask)\n",
    "\n",
    "\n",
    "def second_pass_gUS11_22(g_U1s, g_S2s, S1s, U2s):\n",
    "    # assert g_U1s.shape[2] == S1s.shape[1], \"Incompatible dimensions\"\n",
    "    # assert g_S2s.shape[2] == U2s.shape[1], \"Incompatible dimensions\"\n",
    "    # assert g_U1s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert g_S2s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert S1s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert U2s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert S1s.stride() == U2s.stride(), \"Matrix A must be contiguous\"\n",
    "    # assert g_U1s.stride() == g_S2s.stride(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    L, BSIZE, K = g_U1s.shape\n",
    "    _, _, d2 = S1s.shape\n",
    "\n",
    "    out = torch.empty((BSIZE, d2), dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    # stride_g_U1s2_l, stride_g_U1s2_bsize, stride_g_U1s2_k = g_U1s.stride()\n",
    "    # stride_us_l, stride_us_k, stride_us_d2 = S1s.stride()\n",
    "    # stride_out_bsize, stride_out_d2 = out.stride()\n",
    "    stride_g_U1s2_l, stride_g_U1s2_bsize, stride_g_U1s2_k = (\n",
    "        g_U1s.shape[1] * g_U1s.shape[2],\n",
    "        g_U1s.shape[2],\n",
    "        1,\n",
    "    )\n",
    "    stride_us_l, stride_us_k, stride_us_d2 = (\n",
    "        S1s.shape[1] * S1s.shape[2],\n",
    "        S1s.shape[2],\n",
    "        1,\n",
    "    )\n",
    "    stride_out_bsize, stride_out_d2 = out.shape[1], 1\n",
    "\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"])\n",
    "        * triton.cdiv(d2, META[\"BLOCK_SIZE_d2\"]),\n",
    "    )\n",
    "\n",
    "    second_pass_gUS11_22_kernel[grid](\n",
    "        g_U1s,\n",
    "        g_S2s,\n",
    "        S1s,\n",
    "        U2s,\n",
    "        out,\n",
    "        BSIZE,\n",
    "        d2,\n",
    "        K,\n",
    "        L,\n",
    "        stride_g_U1s2_l,\n",
    "        stride_g_U1s2_bsize,\n",
    "        stride_g_U1s2_k,\n",
    "        stride_us_l,\n",
    "        stride_us_k,\n",
    "        stride_us_d2,\n",
    "        stride_out_bsize,\n",
    "        stride_out_d2,\n",
    "    )\n",
    "\n",
    "    return out  # grad\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_d2\": 32,\n",
    "                \"BLOCK_SIZE_k\": 32,\n",
    "                \"BLOCK_SIZE_BSIZE\": 32,\n",
    "                \"GROUP_SIZE_d2\": 8,\n",
    "            },\n",
    "            num_stages=1,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 256, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 64, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=5, num_warps=2),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_k': 64, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=5, num_warps=2),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 256, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=3, num_warps=8),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=3, num_warps=8),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_k': 64, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 256, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 64, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 32, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4)\n",
    "    ],\n",
    "    key=[\"d2\", \"k\", \"BSIZE\", \"L\"],\n",
    ")\n",
    "@triton.jit\n",
    "def calc_grad_S1s_kernel(\n",
    "    hin_ptr,\n",
    "    g_U1s_ptr,\n",
    "    grad_g_S1s_ptr,\n",
    "    d2,\n",
    "    k,\n",
    "    BSIZE,\n",
    "    L,\n",
    "    stride_hin_bsize,\n",
    "    stride_hin_BSIZE,\n",
    "    stride_su_l,\n",
    "    stride_su_BSIZE,\n",
    "    stride_su_k,\n",
    "    stride_out_l,\n",
    "    stride_out_bsize,\n",
    "    stride_out_k,\n",
    "    BLOCK_SIZE_d2: tl.constexpr,\n",
    "    BLOCK_SIZE_k: tl.constexpr,\n",
    "    BLOCK_SIZE_BSIZE: tl.constexpr,\n",
    "    GROUP_SIZE_d2: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=1)\n",
    "    batch_id = tl.program_id(axis=0)\n",
    "\n",
    "    num_pid_bsize = tl.cdiv(d2, BLOCK_SIZE_d2)\n",
    "    num_pid_k = tl.cdiv(k, BLOCK_SIZE_k)\n",
    "    num_pid_in_group = GROUP_SIZE_d2 * num_pid_k\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_bsize = group_id * GROUP_SIZE_d2\n",
    "    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_d2)\n",
    "    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n",
    "    pid_k = (pid % num_pid_in_group) // group_size_bsize\n",
    "\n",
    "    offs_bsize = pid_bsize * BLOCK_SIZE_d2 + tl.arange(0, BLOCK_SIZE_d2)\n",
    "    offs_k = pid_k * BLOCK_SIZE_k + tl.arange(0, BLOCK_SIZE_k)\n",
    "    offs_BSIZE = tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "\n",
    "    offs_bsize = tl.max_contiguous(\n",
    "        tl.multiple_of(offs_bsize, BLOCK_SIZE_d2), BLOCK_SIZE_d2\n",
    "    )\n",
    "    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_k), BLOCK_SIZE_k)\n",
    "    offs_BSIZE = tl.max_contiguous(\n",
    "        tl.multiple_of(offs_BSIZE, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE\n",
    "    )\n",
    "\n",
    "    hin_ptrs = hin_ptr + (\n",
    "        offs_bsize[:, None] * stride_hin_bsize + offs_BSIZE[None, :] * stride_hin_BSIZE\n",
    "    )\n",
    "\n",
    "    su_tmp = batch_id * stride_su_l + (\n",
    "        offs_BSIZE[:, None] * stride_su_BSIZE + offs_k[None, :] * stride_su_k\n",
    "    )\n",
    "    g_U1s_ptrs = g_U1s_ptr + su_tmp\n",
    "\n",
    "    accumulator1 = tl.full(\n",
    "        shape=(BLOCK_SIZE_d2, BLOCK_SIZE_k), value=0.0, dtype=tl.float32\n",
    "    )\n",
    "    accumulator2 = tl.full(\n",
    "        shape=(BLOCK_SIZE_d2, BLOCK_SIZE_k), value=0.0, dtype=tl.float32\n",
    "    )\n",
    "\n",
    "    for BSIZE_i in range(0, tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)):\n",
    "        hin_mask = (offs_bsize[:, None] < d2) & (\n",
    "            offs_BSIZE[None, :] < BSIZE - BSIZE_i * BLOCK_SIZE_BSIZE\n",
    "        )\n",
    "        hin = tl.load(hin_ptrs, mask=hin_mask, other=0.0)\n",
    "\n",
    "        su_mask = (offs_BSIZE[:, None] < BSIZE - BSIZE_i * BLOCK_SIZE_BSIZE) & (\n",
    "            offs_k[None, :] < k\n",
    "        )\n",
    "        g_U1s = tl.load(g_U1s_ptrs, mask=su_mask, other=0.0)\n",
    "\n",
    "        accumulator1 += tl.dot(hin, g_U1s, input_precision=\"ieee\")\n",
    "\n",
    "        hin_ptrs += BLOCK_SIZE_BSIZE * stride_hin_BSIZE\n",
    "        g_U1s_ptrs += BLOCK_SIZE_BSIZE * stride_su_BSIZE\n",
    "\n",
    "    out_tmp = (\n",
    "        batch_id * stride_out_l\n",
    "        + stride_out_bsize * offs_bsize[:, None]\n",
    "        + stride_out_k * offs_k[None, :]\n",
    "    )\n",
    "    grad_g_S1s_ptrs = grad_g_S1s_ptr + out_tmp\n",
    "\n",
    "    out_mask = (offs_bsize[:, None] < d2) & (offs_k[None, :] < k)\n",
    "\n",
    "    tl.store(grad_g_S1s_ptrs, accumulator1, mask=out_mask)\n",
    "\n",
    "\n",
    "def calc_grad_S1s(hin, g_U1s):\n",
    "    device = \"cuda\"\n",
    "    # assert hin.shape[1] == g_U1s.shape[1], \"Incompatible dimensions\"\n",
    "    # assert hin.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert g_U1s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    d2, BSIZE = hin.shape\n",
    "    L, _, k = g_U1s.shape\n",
    "\n",
    "    grad_g_S1s = torch.empty((L, d2, k), dtype=torch.float32, device=device)\n",
    "\n",
    "    # stride_hin_bsize, stride_hin_BSIZE = hin.stride()\n",
    "    # stride_su_l, stride_su_BSIZE, stride_su_k = g_U1s.stride()\n",
    "    # stride_out_l, stride_out_bsize, stride_out_k = grad_g_S1s.stride()\n",
    "    stride_hin_bsize, stride_hin_BSIZE = hin.shape[1], 1\n",
    "    stride_su_l, stride_su_BSIZE, stride_su_k = (\n",
    "        g_U1s.shape[1] * g_U1s.shape[2],\n",
    "        g_U1s.shape[2],\n",
    "        1,\n",
    "    )\n",
    "    stride_out_l, stride_out_bsize, stride_out_k = (\n",
    "        grad_g_S1s.shape[1] * grad_g_S1s.shape[2],\n",
    "        grad_g_S1s.shape[2],\n",
    "        1,\n",
    "    )\n",
    "\n",
    "    grid = lambda META: (\n",
    "        L,\n",
    "        triton.cdiv(d2, META[\"BLOCK_SIZE_d2\"]) * triton.cdiv(k, META[\"BLOCK_SIZE_k\"]),\n",
    "    )\n",
    "\n",
    "    calc_grad_S1s_kernel[grid](\n",
    "        hin,\n",
    "        g_U1s,\n",
    "        grad_g_S1s,\n",
    "        d2,\n",
    "        k,\n",
    "        BSIZE,\n",
    "        L,\n",
    "        stride_hin_bsize,\n",
    "        stride_hin_BSIZE,\n",
    "        stride_su_l,\n",
    "        stride_su_BSIZE,\n",
    "        stride_su_k,\n",
    "        stride_out_l,\n",
    "        stride_out_bsize,\n",
    "        stride_out_k,\n",
    "    )\n",
    "\n",
    "    return grad_g_S1s\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_K\": 32,\n",
    "                \"BLOCK_SIZE_BSIZE\": 32,\n",
    "                \"BLOCK_SIZE_d2\": 32,\n",
    "                \"GROUP_SIZE_K\": 8,\n",
    "            },\n",
    "            num_stages=1,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=5, num_warps=2),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=5, num_warps=2),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=3, num_warps=8),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=3, num_warps=8),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4)\n",
    "    ],\n",
    "    key=[\"K\", \"d2\", \"BSIZE\", \"L\"],\n",
    ")\n",
    "@triton.jit\n",
    "def first_pass_U2s_hin_kernel(\n",
    "    hin_ptr,\n",
    "    U2s_ptr,\n",
    "    U2s_h_in_ptr,\n",
    "    K,\n",
    "    d2,\n",
    "    BSIZE,\n",
    "    L,\n",
    "    stride_hin_d2,\n",
    "    stride_hin_BSIZE,\n",
    "    stride_su_l,\n",
    "    stride_su_K,\n",
    "    stride_su_d2,\n",
    "    stride_out_l,\n",
    "    stride_out_K,\n",
    "    stride_out_BSIZE,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    BLOCK_SIZE_BSIZE: tl.constexpr,\n",
    "    BLOCK_SIZE_d2: tl.constexpr,\n",
    "    GROUP_SIZE_K: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=1)\n",
    "    batch_id = tl.program_id(axis=0)\n",
    "\n",
    "    num_pid_K = tl.cdiv(K, BLOCK_SIZE_K)\n",
    "    num_pid_BSIZE = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n",
    "    num_pid_in_group = GROUP_SIZE_K * num_pid_BSIZE\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_K = group_id * GROUP_SIZE_K\n",
    "    group_size_BSIZE = min(num_pid_K - first_pid_K, GROUP_SIZE_K)\n",
    "    pid_K = first_pid_K + ((pid % num_pid_in_group) % group_size_BSIZE)\n",
    "    pid_BSIZE = (pid % num_pid_in_group) // group_size_BSIZE\n",
    "\n",
    "    offs_K = pid_K * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
    "    offs_BSIZE = pid_BSIZE * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "    offs_d2 = tl.arange(0, BLOCK_SIZE_d2)\n",
    "\n",
    "    offs_K = tl.max_contiguous(tl.multiple_of(offs_K, BLOCK_SIZE_K), BLOCK_SIZE_K)\n",
    "    offs_BSIZE = tl.max_contiguous(\n",
    "        tl.multiple_of(offs_BSIZE, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE\n",
    "    )\n",
    "    offs_d2 = tl.max_contiguous(tl.multiple_of(offs_d2, BLOCK_SIZE_d2), BLOCK_SIZE_d2)\n",
    "\n",
    "    hin_ptrs = hin_ptr + (\n",
    "        offs_d2[:, None] * stride_hin_d2 + offs_BSIZE[None, :] * stride_hin_BSIZE\n",
    "    )\n",
    "\n",
    "    su_tmp = batch_id * stride_su_l + (\n",
    "        offs_K[:, None] * stride_su_K + offs_d2[None, :] * stride_su_d2\n",
    "    )\n",
    "    U2s_ptrs = U2s_ptr + su_tmp\n",
    "\n",
    "    accumulator1 = tl.full(\n",
    "        shape=(BLOCK_SIZE_K, BLOCK_SIZE_BSIZE), value=0.0, dtype=tl.float32\n",
    "    )\n",
    "\n",
    "    for d2_i in range(0, tl.cdiv(d2, BLOCK_SIZE_d2)):\n",
    "        hin_mask = (offs_d2[:, None] < d2 - d2_i * BLOCK_SIZE_d2) & (\n",
    "            offs_BSIZE[None, :] < BSIZE\n",
    "        )\n",
    "        hin = tl.load(hin_ptrs, mask=hin_mask, other=0.0)\n",
    "\n",
    "        su_mask = (offs_K[:, None] < K) & (offs_d2[None, :] < d2 - d2_i * BLOCK_SIZE_d2)\n",
    "        U2s = tl.load(U2s_ptrs, mask=su_mask, other=0.0)\n",
    "\n",
    "        accumulator1 += tl.dot(U2s, hin, input_precision=\"ieee\")\n",
    "\n",
    "        hin_ptrs += BLOCK_SIZE_d2 * stride_hin_d2\n",
    "        U2s_ptrs += BLOCK_SIZE_d2 * stride_su_d2\n",
    "\n",
    "    out_tmp = (\n",
    "        batch_id * stride_out_l\n",
    "        + stride_out_K * offs_K[:, None]\n",
    "        + stride_out_BSIZE * offs_BSIZE[None, :]\n",
    "    )\n",
    "    U2s_h_in_ptrs = U2s_h_in_ptr + out_tmp\n",
    "\n",
    "    out_mask = (offs_K[:, None] < K) & (offs_BSIZE[None, :] < BSIZE)\n",
    "\n",
    "    tl.store(U2s_h_in_ptrs, accumulator1, mask=out_mask)\n",
    "\n",
    "\n",
    "def first_pass_U2s_hin(U2s, hin):\n",
    "    device = \"cuda\"\n",
    "    # assert U2s.shape[2] == hin.shape[0], \"Incompatible dimensions\"\n",
    "    # assert hin.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert U2s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    L, K, d2 = U2s.shape\n",
    "    _, BSIZE = hin.shape\n",
    "\n",
    "    U2s_h_in = torch.empty((L, K, BSIZE), dtype=torch.float32, device=device)\n",
    "\n",
    "    # stride_hin_d2, stride_hin_BSIZE = hin.stride()\n",
    "    # stride_su_l, stride_su_K, stride_su_d2 = U2s.stride()\n",
    "    # stride_out_l, stride_out_K, stride_out_BSIZE = U2s_h_in.stride()\n",
    "    stride_hin_d2, stride_hin_BSIZE = hin.shape[1], 1\n",
    "    stride_su_l, stride_su_K, stride_su_d2 = (\n",
    "        U2s.shape[1] * U2s.shape[2],\n",
    "        U2s.shape[2],\n",
    "        1,\n",
    "    )\n",
    "    stride_out_l, stride_out_K, stride_out_BSIZE = (\n",
    "        U2s_h_in.shape[1] * U2s_h_in.shape[2],\n",
    "        U2s_h_in.shape[2],\n",
    "        1,\n",
    "    )\n",
    "\n",
    "    BLOCK_SIZE_K, BLOCK_SIZE_BSIZE, BLOCK_SIZE_d2 = 128, 256, 64\n",
    "    GROUP_SIZE_K = 8\n",
    "\n",
    "    grid = lambda META: (\n",
    "        L,\n",
    "        triton.cdiv(K, META[\"BLOCK_SIZE_K\"])\n",
    "        * triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]),\n",
    "    )\n",
    "\n",
    "    first_pass_U2s_hin_kernel[grid](\n",
    "        hin,\n",
    "        U2s,\n",
    "        U2s_h_in,\n",
    "        K,\n",
    "        d2,\n",
    "        BSIZE,\n",
    "        L,\n",
    "        stride_hin_d2,\n",
    "        stride_hin_BSIZE,\n",
    "        stride_su_l,\n",
    "        stride_su_K,\n",
    "        stride_su_d2,\n",
    "        stride_out_l,\n",
    "        stride_out_K,\n",
    "        stride_out_BSIZE,\n",
    "    )\n",
    "\n",
    "    return U2s_h_in\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_K\": 32,\n",
    "                \"BLOCK_SIZE_d1\": 32,\n",
    "                \"BLOCK_SIZE_BSIZE\": 32,\n",
    "                \"GROUP_SIZE_K\": 8,\n",
    "            },\n",
    "            num_stages=1,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 256, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 64, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=5, num_warps=2),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 64, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=5, num_warps=2),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 256, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=3, num_warps=8),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=3, num_warps=8),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 64, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 256, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 64, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 32, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4)\n",
    "    ],\n",
    "    key=[\"K\", \"BSIZE\", \"d1\", \"L\"],\n",
    ")\n",
    "@triton.jit\n",
    "def calc_grad_S2s_kernel(\n",
    "    g_ptr,\n",
    "    U2s_hin_ptr,\n",
    "    grad_S2s_ptr,\n",
    "    K,\n",
    "    BSIZE,\n",
    "    d1,\n",
    "    L,\n",
    "    stride_g_BSIZE,\n",
    "    stride_g_d1,\n",
    "    stride_su_l,\n",
    "    stride_su_K,\n",
    "    stride_su_BSIZE,\n",
    "    stride_out_l,\n",
    "    stride_out_K,\n",
    "    stride_out_d1,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    BLOCK_SIZE_d1: tl.constexpr,\n",
    "    BLOCK_SIZE_BSIZE: tl.constexpr,\n",
    "    GROUP_SIZE_K: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=1)\n",
    "    batch_id = tl.program_id(axis=0)\n",
    "\n",
    "    num_pid_K = tl.cdiv(K, BLOCK_SIZE_K)\n",
    "    num_pid_d1 = tl.cdiv(d1, BLOCK_SIZE_d1)\n",
    "    num_pid_in_group = GROUP_SIZE_K * num_pid_d1\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_K = group_id * GROUP_SIZE_K\n",
    "    group_size_d1 = min(num_pid_K - first_pid_K, GROUP_SIZE_K)\n",
    "    pid_K = first_pid_K + ((pid % num_pid_in_group) % group_size_d1)\n",
    "    pid_d1 = (pid % num_pid_in_group) // group_size_d1\n",
    "\n",
    "    offs_K = pid_K * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
    "    offs_d1 = pid_d1 * BLOCK_SIZE_d1 + tl.arange(0, BLOCK_SIZE_d1)\n",
    "    offs_BSIZE = tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "\n",
    "    offs_K = tl.max_contiguous(tl.multiple_of(offs_K, BLOCK_SIZE_K), BLOCK_SIZE_K)\n",
    "    offs_d1 = tl.max_contiguous(tl.multiple_of(offs_d1, BLOCK_SIZE_d1), BLOCK_SIZE_d1)\n",
    "    offs_BSIZE = tl.max_contiguous(\n",
    "        tl.multiple_of(offs_BSIZE, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE\n",
    "    )\n",
    "\n",
    "    g_ptrs = g_ptr + (\n",
    "        offs_BSIZE[:, None] * stride_g_BSIZE + offs_d1[None, :] * stride_g_d1\n",
    "    )\n",
    "\n",
    "    su_tmp = batch_id * stride_su_l + (\n",
    "        offs_K[:, None] * stride_su_K + offs_BSIZE[None, :] * stride_su_BSIZE\n",
    "    )\n",
    "    U2s_hin_ptrs = U2s_hin_ptr + su_tmp\n",
    "\n",
    "    accumulator1 = tl.full(\n",
    "        shape=(BLOCK_SIZE_K, BLOCK_SIZE_d1), value=0.0, dtype=tl.float32\n",
    "    )\n",
    "\n",
    "    for BSIZE_i in range(0, tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)):\n",
    "        g_mask = (offs_BSIZE[:, None] < BSIZE - BSIZE_i * BLOCK_SIZE_BSIZE) & (\n",
    "            offs_d1[None, :] < d1\n",
    "        )\n",
    "        g = tl.load(g_ptrs, mask=g_mask, other=0.0)\n",
    "\n",
    "        su_mask = (offs_K[:, None] < K) & (\n",
    "            offs_BSIZE[None, :] < BSIZE - BSIZE_i * BLOCK_SIZE_BSIZE\n",
    "        )\n",
    "        U2s_hin = tl.load(U2s_hin_ptrs, mask=su_mask, other=0.0)\n",
    "\n",
    "        accumulator1 += tl.dot(U2s_hin, g, input_precision=\"ieee\")\n",
    "\n",
    "        g_ptrs += BLOCK_SIZE_BSIZE * stride_g_BSIZE\n",
    "        U2s_hin_ptrs += BLOCK_SIZE_BSIZE * stride_su_BSIZE\n",
    "\n",
    "    out_tmp = (\n",
    "        batch_id * stride_out_l\n",
    "        + stride_out_K * offs_K[:, None]\n",
    "        + stride_out_d1 * offs_d1[None, :]\n",
    "    )\n",
    "    grad_S2s_ptrs = grad_S2s_ptr + out_tmp\n",
    "\n",
    "    out_mask = (offs_K[:, None] < K) & (offs_d1[None, :] < d1)\n",
    "\n",
    "    tl.store(grad_S2s_ptrs, accumulator1, mask=out_mask)\n",
    "\n",
    "\n",
    "def calc_grad_S2s(U2s_hin, g):\n",
    "    device = \"cuda\"\n",
    "    # assert U2s_hin.shape[2] == g.shape[0], \"Incompatible dimensions\"\n",
    "    # assert g.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    # assert U2s_hin.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    L, K, BSIZE = U2s_hin.shape\n",
    "    _, d1 = g.shape\n",
    "\n",
    "    grad_S2s = torch.empty((L, K, d1), dtype=torch.float32, device=device)\n",
    "\n",
    "    # stride_g_BSIZE, stride_g_d1 = g.stride()\n",
    "    # stride_su_l, stride_su_K, stride_su_BSIZE = U2s_hin.stride()\n",
    "    # stride_out_l, stride_out_K, stride_out_d1 = grad_S2s.stride()\n",
    "    stride_g_BSIZE, stride_g_d1 = g.shape[1], 1\n",
    "    stride_su_l, stride_su_K, stride_su_BSIZE = (\n",
    "        U2s_hin.shape[1] * U2s_hin.shape[2],\n",
    "        U2s_hin.shape[2],\n",
    "        1,\n",
    "    )\n",
    "    stride_out_l, stride_out_K, stride_out_d1 = (\n",
    "        grad_S2s.shape[1] * grad_S2s.shape[2],\n",
    "        grad_S2s.shape[2],\n",
    "        1,\n",
    "    )\n",
    "\n",
    "    BLOCK_SIZE_K, BLOCK_SIZE_d1, BLOCK_SIZE_BSIZE = 128, 256, 64\n",
    "    GROUP_SIZE_K = 8\n",
    "\n",
    "    grid = lambda META: (\n",
    "        L,\n",
    "        triton.cdiv(K, META[\"BLOCK_SIZE_K\"]) * triton.cdiv(d1, META[\"BLOCK_SIZE_d1\"]),\n",
    "    )\n",
    "\n",
    "    calc_grad_S2s_kernel[grid](\n",
    "        g,\n",
    "        U2s_hin,\n",
    "        grad_S2s,\n",
    "        K,\n",
    "        BSIZE,\n",
    "        d1,\n",
    "        L,\n",
    "        stride_g_BSIZE,\n",
    "        stride_g_d1,\n",
    "        stride_su_l,\n",
    "        stride_su_K,\n",
    "        stride_su_BSIZE,\n",
    "        stride_out_l,\n",
    "        stride_out_K,\n",
    "        stride_out_d1,\n",
    "    )\n",
    "\n",
    "    return grad_S2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T00:34:22.250133Z",
     "iopub.status.busy": "2025-05-01T00:34:22.249297Z",
     "iopub.status.idle": "2025-05-01T00:34:22.259114Z",
     "shell.execute_reply": "2025-05-01T00:34:22.258565Z",
     "shell.execute_reply.started": "2025-05-01T00:34:22.250106Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# DISCLAIMER: THIS FILE NEEDS TO BE CHECKED FOR CORRECTNESS\n",
    "\n",
    "\n",
    "def uniform_dense_sketch(m, n, device=None, dtype=None):\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    return torch.empty(m, n, **factory_kwargs).uniform_(-1, 1)\n",
    "\n",
    "\n",
    "def gaussian_dense_sketch(m, n, device=None, dtype=None):\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    return torch.randn(m, n, **factory_kwargs)\n",
    "\n",
    "\n",
    "def hadamard_sketch(m, device=None, dtype=None):\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    if m & (m - 1) != 0:\n",
    "        raise ValueError(\"m must be a power of 2\")\n",
    "\n",
    "    H = torch.tensor([[1.0]])\n",
    "    while H.shape[0] < m:\n",
    "        H = torch.cat((torch.cat((H, H), dim=1), torch.cat((H, -H), dim=1)), dim=0)\n",
    "\n",
    "    return H / torch.sqrt(torch.tensor(m, **factory_kwargs))\n",
    "\n",
    "\n",
    "def gaussian_orthonormal_sketch(m, n, device=None, dtype=None):\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    return torch.qr(torch.randn(m, n, **factory_kwargs))[0]\n",
    "\n",
    "\n",
    "def gen_U(m, n, device=None, dtype=None):\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    return (torch.randint(0, 2, (m, n), **factory_kwargs) * 2 - 1) / torch.sqrt(\n",
    "        torch.tensor(m, **factory_kwargs)\n",
    "    )\n",
    "\n",
    "\n",
    "def clarkson_woodruff_sketch(m, n, device=None, dtype=None):\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    indices = torch.randint(0, m, (n,), **factory_kwargs)\n",
    "    signs = torch.randint(0, 2, (n,), **factory_kwargs) * 2 - 1\n",
    "    sketch = torch.zeros(m, n, **factory_kwargs)\n",
    "    sketch[indices, torch.arange(n)] = signs\n",
    "    return sketch\n",
    "\n",
    "\n",
    "def sparse_sign_embeddings_sketch(m, n, sparsity=0.1):\n",
    "    mask = torch.rand(m, n) < sparsity\n",
    "    signs = torch.randint(0, 2, (m, n)) * 2 - 1\n",
    "    return mask.float() * signs.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:14:12.128995Z",
     "iopub.status.busy": "2025-05-01T01:14:12.128376Z",
     "iopub.status.idle": "2025-05-01T01:14:12.177481Z",
     "shell.execute_reply": "2025-05-01T01:14:12.176844Z",
     "shell.execute_reply.started": "2025-05-01T01:14:12.128968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Any, Tuple, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from torch.nn import init\n",
    "import triton\n",
    "from torch.library import triton_op, wrap_triton\n",
    "\n",
    "\n",
    "@triton_op(\"panther::forward_op\", mutates_args={})\n",
    "def forward_op(\n",
    "    hin: torch.Tensor,\n",
    "    S1s: torch.Tensor,\n",
    "    S2s: torch.Tensor,\n",
    "    U1s: torch.Tensor,\n",
    "    U2s: torch.Tensor,\n",
    "    bias: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    device = \"cuda\"\n",
    "\n",
    "    # first pass\n",
    "    ############\n",
    "    L = S2s.shape[0]\n",
    "    BSIZE, d2 = hin.shape\n",
    "    L, _, K = S1s.shape\n",
    "\n",
    "    in1 = torch.empty((L, BSIZE, K), dtype=torch.float32, device=device)\n",
    "    in2 = torch.empty((L, BSIZE, K), dtype=torch.float32, device=device)\n",
    "\n",
    "    stride_hin_bsize, stride_hin_d2 = hin.stride(0), hin.stride(1)\n",
    "    stride_su_l, stride_su_d2, stride_su_k = S1s.stride(0), S1s.stride(1), S1s.stride(2)\n",
    "    stride_out_l, stride_out_bsize, stride_out_k = (\n",
    "        in1.stride(0),\n",
    "        in1.stride(1),\n",
    "        in1.stride(2),\n",
    "    )\n",
    "\n",
    "    grid = lambda META: (\n",
    "        L,\n",
    "        triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"])\n",
    "        * triton.cdiv(K, META[\"BLOCK_SIZE_K\"]),\n",
    "    )\n",
    "\n",
    "    wrap_triton(first_pass_kernel)[grid](\n",
    "        hin,\n",
    "        S1s,\n",
    "        U2s,\n",
    "        in1,\n",
    "        in2,\n",
    "        BSIZE,\n",
    "        K,\n",
    "        d2,\n",
    "        L,\n",
    "        stride_hin_bsize,\n",
    "        stride_hin_d2,\n",
    "        stride_su_l,\n",
    "        stride_su_d2,\n",
    "        stride_su_k,\n",
    "        stride_out_l,\n",
    "        stride_out_bsize,\n",
    "        stride_out_k,\n",
    "    )\n",
    "\n",
    "    # torch equivlant\n",
    "    num_terms = S2s.shape[0]\n",
    "    input_torch = hin.unsqueeze(0).expand(num_terms, hin.shape[0], hin.shape[1])\n",
    "    in1_torch = input_torch.bmm(S1s)\n",
    "    in2_torch = input_torch.bmm(U2s)\n",
    "\n",
    "    # compare using torch allclose\n",
    "    outputs_match = torch.allclose(in1_torch, in1)\n",
    "    print(f\"forward in1 and in1_torch Outputs match: {outputs_match}\")\n",
    "    if not outputs_match:\n",
    "        max_diff = torch.max(torch.abs(in1_torch - in1))\n",
    "        print(f\"Max difference: {max_diff.item()}\")\n",
    "        print(f\"out_triton: {in1_torch}\")\n",
    "        print(f\"out_normal: {in1}\")\n",
    "\n",
    "    outputs_match = torch.allclose(in2_torch, in2)\n",
    "    print(f\"forward in2 and in2_torch Outputs match: {outputs_match}\")\n",
    "    if not outputs_match:\n",
    "        max_diff = torch.max(torch.abs(in2_torch - in2))\n",
    "        print(f\"Max difference: {max_diff.item()}\")\n",
    "        print(f\"out_triton: {in2_torch}\")\n",
    "        print(f\"out_normal: {in2}\")\n",
    "\n",
    "    # second pass\n",
    "    #############\n",
    "    bias_unsqueezed = bias.unsqueeze(0)\n",
    "    L, BSIZE, K = in1.shape\n",
    "    _, _, d1 = U1s.shape\n",
    "\n",
    "    out = torch.empty((BSIZE, d1), dtype=torch.float32, device=device)\n",
    "\n",
    "    stride_in12_l, stride_in12_bsize, stride_in12_k = (\n",
    "        in1.stride(0),\n",
    "        in1.stride(1),\n",
    "        in1.stride(2),\n",
    "    )\n",
    "    stride_us_l, stride_us_k, stride_us_d1 = U1s.stride(0), U1s.stride(1), U1s.stride(2)\n",
    "    stride_bias_bsize, stride_bias_d1 = (\n",
    "        bias_unsqueezed.stride(0),\n",
    "        bias_unsqueezed.stride(1),\n",
    "    )\n",
    "    stride_out_bsize, stride_out_d1 = out.stride(0), out.stride(1)\n",
    "\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"])\n",
    "        * triton.cdiv(d1, META[\"BLOCK_SIZE_D1\"]),\n",
    "    )\n",
    "\n",
    "    wrap_triton(second_pass_kernel)[grid](\n",
    "        in1,\n",
    "        in2,\n",
    "        U1s,\n",
    "        S2s,\n",
    "        bias_unsqueezed,\n",
    "        out,\n",
    "        BSIZE,\n",
    "        d1,\n",
    "        K,\n",
    "        L,\n",
    "        stride_in12_l,\n",
    "        stride_in12_bsize,\n",
    "        stride_in12_k,\n",
    "        stride_us_l,\n",
    "        stride_us_k,\n",
    "        stride_us_d1,\n",
    "        stride_bias_bsize,\n",
    "        stride_bias_d1,\n",
    "        stride_out_bsize,\n",
    "        stride_out_d1,\n",
    "    )\n",
    "\n",
    "    torch_out = (\n",
    "        ((input_torch.bmm(S1s)).bmm(U1s)).mean(0) / 2\n",
    "        + ((input_torch.bmm(U2s)).bmm(S2s)).mean(0) / 2\n",
    "        + bias\n",
    "    )\n",
    "    # compare using torch allclose\n",
    "    outputs_match = torch.allclose(torch_out, out)\n",
    "    print(f\"forward out and torch_out Outputs match: {outputs_match}\")\n",
    "    if not outputs_match:\n",
    "        max_diff = torch.max(torch.abs(torch_out - out))\n",
    "        print(f\"Max difference: {max_diff.item()}\")\n",
    "        print(f\"torch_out: {torch_out}\")\n",
    "        print(f\"out: {out}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "@forward_op.register_kernel(\"cpu\")\n",
    "def _(input, S1s, S2s, U1s, U2s, bias):\n",
    "    num_terms = S2s.shape[0]\n",
    "    # Efficiently perform the sum over all l terms\n",
    "    input = input.unsqueeze(0).expand(num_terms, input.shape[0], input.shape[1])\n",
    "    return (\n",
    "        ((input.bmm(S1s)).bmm(U1s)).mean(0) / 2\n",
    "        + ((input.bmm(U2s)).bmm(S2s)).mean(0) / 2\n",
    "        + bias\n",
    "    )\n",
    "\n",
    "\n",
    "@triton_op(\"panther::backward_op\", mutates_args={})\n",
    "def backward_op(\n",
    "    hin: torch.Tensor,\n",
    "    S1s: torch.Tensor,\n",
    "    S2s: torch.Tensor,\n",
    "    U1s: torch.Tensor,\n",
    "    U2s: torch.Tensor,\n",
    "    g: torch.Tensor,\n",
    ") -> List[torch.Tensor]:\n",
    "    device = \"cuda\"\n",
    "    num_terms = S2s.shape[0]\n",
    "\n",
    "    hin = hin.transpose(0, 1)\n",
    "    U1s = U1s.transpose(1, 2)\n",
    "    S1s = S1s.transpose(1, 2)\n",
    "    U2s = U2s.transpose(1, 2)\n",
    "    S2s = S2s.transpose(1, 2)\n",
    "\n",
    "    # first_pass_gU1s_g_S2s\n",
    "    #######################\n",
    "    BSIZE, d1 = g.shape\n",
    "    L, _, K = U1s.shape\n",
    "\n",
    "    g_U1s = torch.empty((L, BSIZE, K), dtype=torch.float32, device=\"cuda\")\n",
    "    g_S2s = torch.empty((L, BSIZE, K), dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    stride_g_bsize, stride_g_d1 = g.stride(0), g.stride(1)\n",
    "    stride_su_l, stride_su_d1, stride_su_k = U1s.stride(0), U1s.stride(1), U1s.stride(2)\n",
    "    stride_out_l, stride_out_bsize, stride_out_k = (\n",
    "        g_U1s.stride(0),\n",
    "        g_U1s.stride(1),\n",
    "        g_U1s.stride(2),\n",
    "    )\n",
    "\n",
    "    grid = lambda META: (\n",
    "        L,\n",
    "        triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"])\n",
    "        * triton.cdiv(K, META[\"BLOCK_SIZE_K\"]),\n",
    "    )\n",
    "\n",
    "    wrap_triton(first_pass_gU1s_g_S2s_kernel)[grid](\n",
    "        g,\n",
    "        U1s,\n",
    "        S2s,\n",
    "        g_U1s,\n",
    "        g_S2s,\n",
    "        BSIZE,\n",
    "        K,\n",
    "        d1,\n",
    "        L,\n",
    "        stride_g_bsize,\n",
    "        stride_g_d1,\n",
    "        stride_su_l,\n",
    "        stride_su_d1,\n",
    "        stride_su_k,\n",
    "        stride_out_l,\n",
    "        stride_out_bsize,\n",
    "        stride_out_k,\n",
    "    )\n",
    "\n",
    "    num_terms = S2s.shape[0]\n",
    "    g_torch_unsqueezed = g.unsqueeze(0).expand(num_terms, g.shape[0], g.shape[1])\n",
    "    g_U1s_torch = g_torch_unsqueezed.bmm(U1s)\n",
    "    g_S2s_torch = g_torch_unsqueezed.bmm(S2s)\n",
    "\n",
    "    # check if the outputs match\n",
    "    outputs_match = torch.allclose(g_U1s_torch, g_U1s)\n",
    "    print(f\"backward g_U1s and g_U1s_torch Outputs match: {outputs_match}\")\n",
    "    if not outputs_match:\n",
    "        max_diff = torch.max(torch.abs(g_U1s_torch - g_U1s))\n",
    "        print(f\"Max difference: {max_diff.item()}\")\n",
    "        print(f\"g_U1s_torch: {g_U1s_torch}\")\n",
    "        print(f\"g_U1s: {g_U1s}\")\n",
    "\n",
    "    outputs_match = torch.allclose(g_S2s_torch, g_S2s)\n",
    "    print(f\"backward g_S2s and g_S2s_torch Outputs match: {outputs_match}\")\n",
    "    if not outputs_match:\n",
    "        max_diff = torch.max(torch.abs(g_S2s_torch - g_S2s))\n",
    "        print(f\"Max difference: {max_diff.item()}\")\n",
    "        print(f\"g_S2s_torch: {g_S2s_torch}\")\n",
    "        print(f\"g_S2s: {g_S2s}\")\n",
    "\n",
    "    # second_pass_gUS11_22\n",
    "    #######################\n",
    "    L, BSIZE, K = g_U1s.shape\n",
    "    _, _, d2 = S1s.shape\n",
    "\n",
    "    grad = torch.empty((BSIZE, d2), dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    stride_g_U1s2_l, stride_g_U1s2_bsize, stride_g_U1s2_k = (\n",
    "        g_U1s.stride(0),\n",
    "        g_U1s.stride(1),\n",
    "        g_U1s.stride(2),\n",
    "    )\n",
    "    stride_us_l, stride_us_k, stride_us_d2 = S1s.stride(0), S1s.stride(1), S1s.stride(2)\n",
    "    stride_out_bsize, stride_out_d2 = grad.stride(0), grad.stride(1)\n",
    "\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"])\n",
    "        * triton.cdiv(d2, META[\"BLOCK_SIZE_d2\"]),\n",
    "    )\n",
    "\n",
    "    wrap_triton(second_pass_gUS11_22_kernel)[grid](\n",
    "        g_U1s,\n",
    "        g_S2s,\n",
    "        S1s,\n",
    "        U2s,\n",
    "        grad,\n",
    "        BSIZE,\n",
    "        d2,\n",
    "        K,\n",
    "        L,\n",
    "        stride_g_U1s2_l,\n",
    "        stride_g_U1s2_bsize,\n",
    "        stride_g_U1s2_k,\n",
    "        stride_us_l,\n",
    "        stride_us_k,\n",
    "        stride_us_d2,\n",
    "        stride_out_bsize,\n",
    "        stride_out_d2,\n",
    "    )\n",
    "\n",
    "    grad_torch = (\n",
    "        g_torch_unsqueezed.bmm(U1s).bmm(S1s).sum(0)\n",
    "        + g_torch_unsqueezed.bmm(S2s).bmm(U2s).sum(0)\n",
    "    ) / (2 * num_terms)\n",
    "\n",
    "    outputs_match = torch.allclose(grad_torch, grad)\n",
    "    print(f\"backward grad and grad_torch Outputs match: {outputs_match}\")\n",
    "    if not outputs_match:\n",
    "        max_diff = torch.max(torch.abs(grad_torch - grad))\n",
    "        print(f\"Max difference: {max_diff.item()}\")\n",
    "        print(f\"grad_torch: {grad_torch}\")\n",
    "        print(f\"grad: {grad}\")\n",
    "\n",
    "    # calc_grad_S1s\n",
    "    ################\n",
    "    d2, BSIZE = hin.shape\n",
    "    L, _, k = g_U1s.shape\n",
    "\n",
    "    grad_S1s = torch.empty((L, d2, k), dtype=torch.float32, device=device)\n",
    "\n",
    "    stride_hin_bsize, stride_hin_BSIZE = hin.stride(0), hin.stride(1)\n",
    "    stride_su_l, stride_su_BSIZE, stride_su_k = (\n",
    "        g_U1s.stride(0),\n",
    "        g_U1s.stride(1),\n",
    "        g_U1s.stride(2),\n",
    "    )\n",
    "    stride_out_l, stride_out_bsize, stride_out_k = (\n",
    "        grad_S1s.stride(0),\n",
    "        grad_S1s.stride(1),\n",
    "        grad_S1s.stride(2),\n",
    "    )\n",
    "\n",
    "    grid = lambda META: (\n",
    "        L,\n",
    "        triton.cdiv(d2, META[\"BLOCK_SIZE_d2\"]) * triton.cdiv(k, META[\"BLOCK_SIZE_k\"]),\n",
    "    )\n",
    "\n",
    "    wrap_triton(calc_grad_S1s_kernel)[grid](\n",
    "        hin,\n",
    "        g_U1s,\n",
    "        grad_S1s,\n",
    "        d2,\n",
    "        k,\n",
    "        BSIZE,\n",
    "        L,\n",
    "        stride_hin_bsize,\n",
    "        stride_hin_BSIZE,\n",
    "        stride_su_l,\n",
    "        stride_su_BSIZE,\n",
    "        stride_su_k,\n",
    "        stride_out_l,\n",
    "        stride_out_bsize,\n",
    "        stride_out_k,\n",
    "    )\n",
    "\n",
    "    input_torch = hin.unsqueeze(0).expand(num_terms, hin.shape[0], hin.shape[1])\n",
    "    grad_S1s_torch = input_torch.bmm(g_torch_unsqueezed.bmm(U1s))\n",
    "\n",
    "    # check if the outputs match\n",
    "    outputs_match = torch.allclose(grad_S1s_torch, grad_S1s)\n",
    "    print(f\"backward grad_S1s and grad_S1s_torch Outputs match: {outputs_match}\")\n",
    "\n",
    "    if not outputs_match:\n",
    "        max_diff = torch.max(torch.abs(grad_S1s_torch - grad_S1s))\n",
    "        print(f\"Max difference: {max_diff.item()}\")\n",
    "        print(f\"grad_S1s_torch: {grad_S1s_torch}\")\n",
    "        print(f\"grad_S1s: {grad_S1s}\")\n",
    "\n",
    "    # first_pass_U2s_hin\n",
    "    ####################\n",
    "    L, K, d2 = U2s.shape\n",
    "    _, BSIZE = hin.shape\n",
    "\n",
    "    U2s_hin = torch.empty((L, K, BSIZE), dtype=torch.float32, device=device)\n",
    "\n",
    "    stride_hin_d2, stride_hin_BSIZE = hin.stride(0), hin.stride(1)\n",
    "    stride_su_l, stride_su_K, stride_su_d2 = U2s.stride(0), U2s.stride(1), U2s.stride(2)\n",
    "    stride_out_l, stride_out_K, stride_out_BSIZE = (\n",
    "        U2s_hin.stride(0),\n",
    "        U2s_hin.stride(1),\n",
    "        U2s_hin.stride(2),\n",
    "    )\n",
    "\n",
    "    grid = lambda META: (\n",
    "        L,\n",
    "        triton.cdiv(K, META[\"BLOCK_SIZE_K\"])\n",
    "        * triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]),\n",
    "    )\n",
    "\n",
    "    wrap_triton(first_pass_U2s_hin_kernel)[grid](\n",
    "        hin,\n",
    "        U2s,\n",
    "        U2s_hin,\n",
    "        K,\n",
    "        d2,\n",
    "        BSIZE,\n",
    "        L,\n",
    "        stride_hin_d2,\n",
    "        stride_hin_BSIZE,\n",
    "        stride_su_l,\n",
    "        stride_su_K,\n",
    "        stride_su_d2,\n",
    "        stride_out_l,\n",
    "        stride_out_K,\n",
    "        stride_out_BSIZE,\n",
    "    )\n",
    "\n",
    "    U2s_hin_torch = U2s.bmm(input_torch)\n",
    "\n",
    "    # check if the outputs match\n",
    "    outputs_match = torch.allclose(U2s_hin_torch, U2s_hin)\n",
    "    print(f\"backward U2s_hin and U2s_hin_torch Outputs match: {outputs_match}\")\n",
    "    if not outputs_match:\n",
    "        max_diff = torch.max(torch.abs(U2s_hin_torch - U2s_hin))\n",
    "        print(f\"Max difference: {max_diff.item()}\")\n",
    "        print(f\"U2s_hin_torch: {U2s_hin_torch}\")\n",
    "        print(f\"U2s_hin: {U2s_hin}\")\n",
    "\n",
    "    # calc_grad_S2s\n",
    "    ###############\n",
    "    L, K, BSIZE = U2s_hin.shape\n",
    "    _, d1 = g.shape\n",
    "\n",
    "    grad_S2s = torch.empty((L, K, d1), dtype=torch.float32, device=device)\n",
    "\n",
    "    stride_g_BSIZE, stride_g_d1 = g.stride(0), g.stride(1)\n",
    "    stride_su_l, stride_su_K, stride_su_BSIZE = (\n",
    "        U2s_hin.stride(0),\n",
    "        U2s_hin.stride(1),\n",
    "        U2s_hin.stride(2),\n",
    "    )\n",
    "    stride_out_l, stride_out_K, stride_out_d1 = (\n",
    "        grad_S2s.stride(0),\n",
    "        grad_S2s.stride(1),\n",
    "        grad_S2s.stride(2),\n",
    "    )\n",
    "\n",
    "    grid = lambda META: (\n",
    "        L,\n",
    "        triton.cdiv(K, META[\"BLOCK_SIZE_K\"]) * triton.cdiv(d1, META[\"BLOCK_SIZE_d1\"]),\n",
    "    )\n",
    "\n",
    "    wrap_triton(calc_grad_S2s_kernel)[grid](\n",
    "        g,\n",
    "        U2s_hin,\n",
    "        grad_S2s,\n",
    "        K,\n",
    "        BSIZE,\n",
    "        d1,\n",
    "        L,\n",
    "        stride_g_BSIZE,\n",
    "        stride_g_d1,\n",
    "        stride_su_l,\n",
    "        stride_su_K,\n",
    "        stride_su_BSIZE,\n",
    "        stride_out_l,\n",
    "        stride_out_K,\n",
    "        stride_out_d1,\n",
    "    )\n",
    "\n",
    "    grad_S2s_torch = (U2s.bmm(input_torch)).bmm(g_torch_unsqueezed)\n",
    "\n",
    "    outputs_match = torch.allclose(grad_S2s_torch, grad_S2s)\n",
    "    print(f\"backward grad_S2s and grad_S2s_torch Outputs match: {outputs_match}\")\n",
    "\n",
    "    if not outputs_match:\n",
    "        max_diff = torch.max(torch.abs(grad_S2s_torch - grad_S2s))\n",
    "        print(f\"Max difference: {max_diff.item()}\")\n",
    "        print(f\"grad_S2s_torch: {grad_S2s_torch}\")\n",
    "        print(f\"grad_S2s: {grad_S2s}\")\n",
    "\n",
    "    return [grad, grad_S1s, grad_S2s, g.sum(0) / (2 * num_terms)]\n",
    "\n",
    "\n",
    "@backward_op.register_kernel(\"cpu\")\n",
    "def _(input, S1s, S2s, U1s, U2s, grad_output):\n",
    "    num_terms = S2s.shape[0]\n",
    "    g = grad_output / (2 * num_terms)\n",
    "    g = g.unsqueeze(0).expand(num_terms, g.shape[0], g.shape[1])\n",
    "    input = (\n",
    "        input.unsqueeze(0)\n",
    "        .expand(num_terms, input.shape[0], input.shape[1])\n",
    "        .transpose(1, 2)\n",
    "    )\n",
    "    U1s = U1s.transpose(1, 2)\n",
    "    S1s = S1s.transpose(1, 2)\n",
    "    U2s = U2s.transpose(1, 2)\n",
    "    S2s = S2s.transpose(1, 2)\n",
    "    t1 = g.bmm(U1s)\n",
    "    grad = t1.bmm(S1s).sum(0) + g.bmm(S2s).bmm(U2s).sum(0)\n",
    "    grad_S2s = (U2s.bmm(input)).bmm(g)\n",
    "    grad_S1s = input.bmm(g.bmm(U1s))\n",
    "\n",
    "    g = g[0]\n",
    "    return [\n",
    "        grad,\n",
    "        grad_S1s,\n",
    "        grad_S2s,\n",
    "        # sum g on batch dimension input.shape[0]\n",
    "        g.reshape(input.shape[2], -1).sum(0),\n",
    "    ]\n",
    "\n",
    "\n",
    "class SketchedLinearFunction_triton(Function):\n",
    "    # Note that forward, setup_context, and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        input: torch.Tensor,\n",
    "        S1s: torch.Tensor,\n",
    "        S2s: torch.Tensor,\n",
    "        U1s: torch.Tensor,\n",
    "        U2s: torch.Tensor,\n",
    "        bias: torch.Tensor,\n",
    "    ):\n",
    "        return forward_op(input, S1s, S2s, U1s, U2s, bias)\n",
    "\n",
    "    @staticmethod\n",
    "    # inputs is a Tuple of all of the inputs passed to forward.\n",
    "    # output is the output of the forward().\n",
    "    def setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any):\n",
    "        input, S1s, S2s, U1s, U2s, bias = inputs\n",
    "        ctx.save_for_backward(input, S1s, S2s, U1s, U2s, bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, *grad_output: Any) -> Any:\n",
    "        # dl/dS2_i = U1_i g h_in^T / 2 * l\n",
    "        # dl/dS1_i = g h_in^T U2_i^T / 2 * l\n",
    "        # dl/dh_in = 1/(2*l) * (sum_{i=1}^{l} (S1_i^T U1_i g) + sum_{i=1}^{l} (U2_i^T S2_i g))\n",
    "        # dl/db = g\n",
    "        hin, S1s, S2s, U1s, U2s, _ = ctx.saved_tensors\n",
    "        grad_input, grad_S1s, grad_S2s, grad_bias = backward_op(\n",
    "            hin, S1s, S2s, U1s, U2s, grad_output[0]\n",
    "        )\n",
    "        return grad_input, grad_S1s, grad_S2s, None, None, grad_bias\n",
    "\n",
    "\n",
    "class SKLinear_triton(nn.Module):\n",
    "    __constants__ = [\"in_features\", \"out_features\", \"num_terms\", \"low_rank\"]\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    num_terms: int\n",
    "    low_rank: int\n",
    "    S1s: torch.Tensor\n",
    "    S2s: torch.Tensor\n",
    "    U1s: torch.Tensor\n",
    "    U2s: torch.Tensor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        num_terms: int,\n",
    "        low_rank: int,\n",
    "        W_init=None,\n",
    "        bias: bool = True,\n",
    "        dtype=None,\n",
    "        device=None,\n",
    "    ):\n",
    "        factory_kwargs = {\"dtype\": dtype, \"device\": device}\n",
    "        super(SKLinear_triton, self).__init__()\n",
    "\n",
    "        # if (\n",
    "        #     2 * num_terms * low_rank * (out_features + in_features)\n",
    "        #     > out_features * in_features\n",
    "        # ):\n",
    "        #     raise ValueError(\n",
    "        #         \"The number of parameters in the sketching layer is larger \"\n",
    "        #         + \"than the number of parameters in the fully connected layer.\"\n",
    "        #     )\n",
    "\n",
    "        self.num_terms = num_terms  # l\n",
    "        self.low_rank = low_rank  # k\n",
    "        self.out_features = out_features\n",
    "        self.in_features = in_features\n",
    "\n",
    "        # Register U1s and U2s as buffers since they are not learnable\n",
    "        self.register_buffer(\n",
    "            \"U1s\",\n",
    "            torch.stack(\n",
    "                [\n",
    "                    gen_U(low_rank, out_features, **factory_kwargs)\n",
    "                    for _ in range(num_terms)\n",
    "                ]\n",
    "            ),\n",
    "        )  # k(low rank)xd1(out) stacked along the zeros dimension (l) -> l x k x d1\n",
    "        self.register_buffer(\n",
    "            \"U2s\",\n",
    "            torch.stack(\n",
    "                [\n",
    "                    gen_U(in_features, low_rank, **factory_kwargs)\n",
    "                    for _ in range(num_terms)\n",
    "                ]\n",
    "            ),\n",
    "        )  # d2xk stacked along the zeros dimension (l) -> l x d2 x k\n",
    "\n",
    "        # W is used to only initialize S\n",
    "        if W_init is None:\n",
    "            W = torch.empty(in_features, out_features, **factory_kwargs)  # d2 * d1\n",
    "            init.kaiming_uniform_(W, a=math.sqrt(5))\n",
    "        else:\n",
    "            W = W_init.T.detach().clone()\n",
    "\n",
    "        # S1s and S2s are precomputed but not updated in the backward pass\n",
    "        self.S1s = nn.Parameter(\n",
    "            torch.stack([torch.matmul(W, self.U1s[i].T) for i in range(num_terms)])\n",
    "        )  # d2xk stacked along the zeros dimension (l) -> l x d2 x k\n",
    "        self.S2s = nn.Parameter(\n",
    "            torch.stack([torch.matmul(self.U2s[i].T, W) for i in range(num_terms)])\n",
    "        )  # kxd1 stacked along the zeros dimension (l) -> l x k x d1\n",
    "\n",
    "        # Bias term initialized with a small standard deviation\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.empty(out_features, **factory_kwargs)\n",
    "            )  # 1 * d1\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(W)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "    def forward(self, h_in):\n",
    "        # TODO: Make sure all the things are contiguos\n",
    "        return SketchedLinearFunction_triton.apply(\n",
    "            h_in, self.S1s, self.S2s, self.U1s, self.U2s, self.bias\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:25:24.297876Z",
     "iopub.status.busy": "2025-05-01T01:25:24.297589Z",
     "iopub.status.idle": "2025-05-01T01:25:26.147286Z",
     "shell.execute_reply": "2025-05-01T01:25:26.146542Z",
     "shell.execute_reply.started": "2025-05-01T01:25:24.297857Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "forward in1 and in1_torch Outputs match: True\nforward in2 and in2_torch Outputs match: True\nforward out and torch_out Outputs match: False\nMax difference: 1.430511474609375e-06\ntorch_out: tensor([[ 0.0104,  1.0086,  0.6721,  ..., -0.3649,  0.1341,  0.5287],\n        [-0.8548, -0.6688,  0.2248,  ...,  0.2155,  1.3069,  2.5197],\n        [ 0.4949,  0.1686,  0.1352,  ..., -1.8021, -1.7875, -0.9252],\n        ...,\n        [ 0.4069, -0.9734, -0.0315,  ..., -0.0174,  1.1333,  0.9707],\n        [-0.7300, -0.3609, -1.6043,  ..., -0.4308, -0.9627, -0.7490],\n        [-0.6174, -0.3168,  0.8720,  ...,  0.3366,  0.1846,  0.9859]],\n       device='cuda:0')\nout: tensor([[ 0.0104,  1.0086,  0.6721,  ..., -0.3649,  0.1341,  0.5287],\n        [-0.8548, -0.6688,  0.2248,  ...,  0.2155,  1.3069,  2.5197],\n        [ 0.4949,  0.1686,  0.1352,  ..., -1.8021, -1.7875, -0.9252],\n        ...,\n        [ 0.4069, -0.9734, -0.0315,  ..., -0.0174,  1.1333,  0.9707],\n        [-0.7300, -0.3609, -1.6043,  ..., -0.4308, -0.9627, -0.7490],\n        [-0.6174, -0.3168,  0.8720,  ...,  0.3366,  0.1846,  0.9859]],\n       device='cuda:0')\nbackward----------------------\n\n\n\n\n\n\nbackward g_U1s and g_U1s_torch Outputs match: True\nbackward g_S2s and g_S2s_torch Outputs match: True\nbackward grad and grad_torch Outputs match: False\nMax difference: 7.152557373046875e-07\ngrad_torch: tensor([[-1.6907, -1.1534, -0.2565,  ...,  0.5138,  0.6020,  1.0107],\n        [-1.6907, -1.1534, -0.2565,  ...,  0.5138,  0.6020,  1.0107],\n        [-1.6907, -1.1534, -0.2565,  ...,  0.5138,  0.6020,  1.0107],\n        ...,\n        [-1.6907, -1.1534, -0.2565,  ...,  0.5138,  0.6020,  1.0107],\n        [-1.6907, -1.1534, -0.2565,  ...,  0.5138,  0.6020,  1.0107],\n        [-1.6907, -1.1534, -0.2565,  ...,  0.5138,  0.6020,  1.0107]],\n       device='cuda:0')\ngrad: tensor([[-1.6907, -1.1534, -0.2565,  ...,  0.5138,  0.6020,  1.0107],\n        [-1.6907, -1.1534, -0.2565,  ...,  0.5138,  0.6020,  1.0107],\n        [-1.6907, -1.1534, -0.2565,  ...,  0.5138,  0.6020,  1.0107],\n        ...,\n        [-1.6907, -1.1534, -0.2565,  ...,  0.5138,  0.6020,  1.0107],\n        [-1.6907, -1.1534, -0.2565,  ...,  0.5138,  0.6020,  1.0107],\n        [-1.6907, -1.1534, -0.2565,  ...,  0.5138,  0.6020,  1.0107]],\n       device='cuda:0')\nbackward grad_S1s and grad_S1s_torch Outputs match: True\nbackward U2s_hin and U2s_hin_torch Outputs match: True\nbackward grad_S2s and grad_S2s_torch Outputs match: True\n"
    }
   ],
   "source": [
    "# Test function for SKLinear_triton with float32\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "in_features = 256\n",
    "out_features = 256\n",
    "num_terms = 3\n",
    "low_rank = 8\n",
    "\n",
    "# Create input tensor with float32 dtype\n",
    "x = torch.randn(batch_size, in_features, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "# Create model with float32 dtype\n",
    "model = SKLinear_triton(\n",
    "    in_features=in_features,\n",
    "    out_features=out_features,\n",
    "    num_terms=num_terms,\n",
    "    low_rank=low_rank,\n",
    "    dtype=torch.float32,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "print(\"backward----------------------\\n\\n\\n\\n\\n\\n\")\n",
    "\n",
    "# # Test backward pass\n",
    "loss = output.sum()\n",
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}