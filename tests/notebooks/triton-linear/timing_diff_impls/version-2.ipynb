{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"github_repos_wildcard\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T02:26:20.685622Z","iopub.execute_input":"2025-05-01T02:26:20.685909Z","iopub.status.idle":"2025-05-01T02:26:20.813526Z","shell.execute_reply.started":"2025-05-01T02:26:20.685886Z","shell.execute_reply":"2025-05-01T02:26:20.812988Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\n# Check if the directory exists before removing\nif os.path.exists(\"panther\"):\n    shutil.rmtree(\"panther\")\n\nGITHUB_TOKEN = secret_value_0\nUSER = \"gaserSami\"\nCLONE_URL = f\"https://{USER}:{GITHUB_TOKEN}@github.com/{USER}/panther.git\"\nget_ipython().system(f\"git clone --branch torch_compile {CLONE_URL}\")\n\nimport sys\nsys.path.append(\"panther\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T02:26:20.814681Z","iopub.execute_input":"2025-05-01T02:26:20.814917Z","iopub.status.idle":"2025-05-01T02:26:23.110443Z","shell.execute_reply.started":"2025-05-01T02:26:20.814899Z","shell.execute_reply":"2025-05-01T02:26:23.109565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T02:26:23.111363Z","iopub.execute_input":"2025-05-01T02:26:23.111590Z","iopub.status.idle":"2025-05-01T02:26:26.300415Z","shell.execute_reply.started":"2025-05-01T02:26:23.111566Z","shell.execute_reply":"2025-05-01T02:26:26.299498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.__version__)\nimport triton\nprint(triton.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T02:26:26.302693Z","iopub.execute_input":"2025-05-01T02:26:26.302994Z","iopub.status.idle":"2025-05-01T02:26:28.035942Z","shell.execute_reply.started":"2025-05-01T02:26:26.302971Z","shell.execute_reply":"2025-05-01T02:26:28.034922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/panther/panther/nn/linear_kernels/forward.py\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n    # Optimized configs for (1, 128) ratio\n    triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 512, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 1024, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    \n    # Optimized configs for (128, 1) ratio\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 512, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    \n    # Configs for (1, 1), (2, 1), (1, 2) ratios\n    triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    \n    # Configs for large batch sizes\n    triton.Config({'BLOCK_SIZE_BSIZE': 1024, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 512, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n    \n    # Fallback configs\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=2, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=2, num_warps=4),\n],\n    key=['BSIZE', 'K', 'd2', 'L'],\n)\n@triton.jit\ndef first_pass_kernel(\n        hin_ptr, S1s_ptr, U2s_ptr, out1_ptr, out2_ptr,\n        BSIZE, K, d2, L,\n        stride_hin_bsize, stride_hin_d2,\n        stride_su_l, stride_su_d2, stride_su_k,\n        stride_out_l, stride_out_bsize, stride_out_k,\n        BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_D2: tl.constexpr,\n        GROUP_SIZE_BSIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=1)\n    batch_id = tl.program_id(axis=0)\n    \n    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_k\n    group_id = pid // num_pid_in_group\n    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n    pid_k = (pid % num_pid_in_group) // group_size_bsize\n\n    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n    offs_k = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_d2 = tl.arange(0, BLOCK_SIZE_D2)\n\n    offs_bsize = tl.max_contiguous(tl.multiple_of(offs_bsize, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_K), BLOCK_SIZE_K)\n    offs_d2 = tl.max_contiguous(tl.multiple_of(offs_d2, BLOCK_SIZE_D2), BLOCK_SIZE_D2)\n    \n    hin_ptrs = hin_ptr + (offs_bsize[:, None] * stride_hin_bsize + offs_d2[None, :] * stride_hin_d2)\n\n    su_tmp = batch_id * stride_su_l + (offs_d2[:, None] * stride_su_d2 + offs_k[None, :] * stride_su_k)\n    S1s_ptrs = S1s_ptr + su_tmp\n    U2s_ptrs = U2s_ptr + su_tmp\n\n    accumulator1 = tl.zeros((BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), dtype=tl.float32)\n    accumulator2 = tl.zeros((BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), dtype=tl.float32)\n    \n    for d2_i in range(0, tl.cdiv(d2, BLOCK_SIZE_D2)):\n        hin_mask = (offs_bsize[:, None] < BSIZE) & (offs_d2[None, :] < d2 - d2_i * BLOCK_SIZE_D2)\n        hin = tl.load(hin_ptrs, mask=hin_mask, other=0.0)\n        \n        su_mask = (offs_d2[:, None] < d2 - d2_i * BLOCK_SIZE_D2) & (offs_k[None, :] < K)\n        S1s = tl.load(S1s_ptrs, mask=su_mask, other=0.0)\n        U2s = tl.load(U2s_ptrs, mask=su_mask, other=0.0)\n        \n        accumulator1 += tl.dot(hin, S1s, allow_tf32=True)\n        accumulator2 += tl.dot(hin, U2s, allow_tf32=True)\n        \n        hin_ptrs += BLOCK_SIZE_D2 * stride_hin_d2\n        S1s_ptrs += BLOCK_SIZE_D2 * stride_su_d2\n        U2s_ptrs += BLOCK_SIZE_D2 * stride_su_d2\n\n    out_tmp = batch_id * stride_out_l + stride_out_bsize * offs_bsize[:, None] + stride_out_k * offs_k[None, :]\n    out1_ptrs = out1_ptr + out_tmp\n    out2_ptrs = out2_ptr + out_tmp\n    \n    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_k[None, :] < K)\n    \n    tl.store(out1_ptrs, accumulator1, mask=out_mask)\n    tl.store(out2_ptrs, accumulator2, mask=out_mask)\n  \n@triton.autotune(\n    configs = [\n    # Optimized configs for (1, 128) ratio\n    triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_D1': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 512, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 1024, 'BLOCK_SIZE_D1': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    \n    # Optimized configs for (128, 1) ratio\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 512, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 1024, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    \n    # Configs for (1, 1), (2, 1), (1, 2) ratios\n    triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_D1': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n    \n    # Configs for large batch sizes\n    triton.Config({'BLOCK_SIZE_BSIZE': 1024, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 512, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n    \n    # Fallback configs\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=2, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=2, num_warps=4),\n],\n    key=['BSIZE', 'd1', 'K', 'L'],\n)\n@triton.jit\ndef second_pass_kernel(\n        in1_ptr, in2_ptr, U1s_ptr, S2s_ptr, bias_ptr, out_ptr,\n        BSIZE, d1, K, L,\n        stride_in12_l, stride_in12_bsize, stride_in12_k,\n        stride_us_l, stride_us_k, stride_us_d1,\n        stride_bias_bsize, stride_bias_d1,\n        stride_out_bsize, stride_out_d1,\n        BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_D1: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n        GROUP_SIZE_BSIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    \n    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n    num_pid_d1 = tl.cdiv(d1, BLOCK_SIZE_D1)\n    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_d1\n    group_id = pid // num_pid_in_group\n    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n    pid_d1 = (pid % num_pid_in_group) // group_size_bsize\n\n    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n    offs_d1 = pid_d1 * BLOCK_SIZE_D1 + tl.arange(0, BLOCK_SIZE_D1)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    offs_bsize = tl.max_contiguous(tl.multiple_of(offs_bsize, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n    offs_d1 = tl.max_contiguous(tl.multiple_of(offs_d1, BLOCK_SIZE_D1), BLOCK_SIZE_D1)\n    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_K), BLOCK_SIZE_K)\n\n    in_tmp = offs_bsize[:, None] * stride_in12_bsize + offs_k[None, :] * stride_in12_k\n    us_tmp = offs_k[:, None] * stride_us_k + offs_d1[None, :] * stride_us_d1\n\n    accumulator = tl.zeros((BLOCK_SIZE_BSIZE, BLOCK_SIZE_D1), dtype=tl.float32)\n    \n    for l in range(0, L):\n        l_in_offset = l * stride_in12_l\n        l_us_offset = l * stride_us_l\n        \n        in1_ptrs = in1_ptr + l_in_offset + in_tmp\n        in2_ptrs = in2_ptr + l_in_offset + in_tmp\n    \n        U1s_ptrs = U1s_ptr + l_us_offset + us_tmp\n        S2s_ptrs = S2s_ptr + l_us_offset + us_tmp\n        \n        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n            k_offset = k * BLOCK_SIZE_K\n            in_mask = offs_k[None, :] < K - k_offset\n            us_mask = offs_k[:, None] < K - k_offset\n            \n            in1 = tl.load(in1_ptrs, mask=in_mask, other=0.0)\n            in2 = tl.load(in2_ptrs, mask=in_mask, other=0.0)\n            \n            U1s = tl.load(U1s_ptrs, mask=us_mask, other=0.0)\n            S2s = tl.load(S2s_ptrs, mask=us_mask, other=0.0)\n            \n            accumulator += tl.dot(in1, U1s, allow_tf32=True)\n            accumulator += tl.dot(in2, S2s, allow_tf32=True)\n\n            in_inc = BLOCK_SIZE_K * stride_in12_k\n            in1_ptrs += in_inc\n            in2_ptrs += in_inc\n            \n            us_inc = BLOCK_SIZE_K * stride_us_k\n            U1s_ptrs += us_inc\n            S2s_ptrs += us_inc\n    \n    bias_ptrs = bias_ptr + offs_d1[None, :] * stride_bias_d1\n    bias_mask = (offs_d1[None, :] < d1)\n    bias = tl.load(bias_ptrs, mask=bias_mask, other=0.0)\n\n    # Scale and add bias in one step\n    scale_factor = 1.0 / (2.0 * L)\n    accumulator = accumulator * scale_factor + bias\n\n    out_ptrs = out_ptr + stride_out_bsize * offs_bsize[:, None] + stride_out_d1 * offs_d1[None, :]\n    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_d1[None, :] < d1)\n    \n    tl.store(out_ptrs, accumulator, mask=out_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T02:26:28.036897Z","iopub.execute_input":"2025-05-01T02:26:28.037320Z","iopub.status.idle":"2025-05-01T02:26:28.048322Z","shell.execute_reply.started":"2025-05-01T02:26:28.037292Z","shell.execute_reply":"2025-05-01T02:26:28.047530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/panther/panther/nn/linear_kernels/backward.py\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        # Optimized configs for (1, 128) ratio\n        triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_BSIZE': 512, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_BSIZE': 1024, 'BLOCK_SIZE_K': 16, 'BLOCK_SIZE_d1': 64, 'GROUP_SIZE_BSIZE': 4}, num_stages=3, num_warps=8),\n        \n        # Optimized configs for (128, 1) ratio\n        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 256, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 512, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n        \n        # Configs for (1, 1), (2, 1), (1, 2) ratios\n        triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 256, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n        \n        # Configs for smaller dimensions (for low_rank values)\n        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 64, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 16, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=4),\n        \n        # Fallback configs\n        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 16, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 4}, num_stages=1, num_warps=4),\n    ],\n    key=['BSIZE', 'K', 'd1', 'L'],\n)\n@triton.jit\ndef first_pass_gU1s_g_S2s_kernel(\n        g_ptr, U1s_ptr, S2s_ptr, g_U1s_ptr, g_S2s_ptr,\n        BSIZE, K, d1, L,\n        stride_g_bsize, stride_g_d1,\n        stride_su_l, stride_su_d1, stride_su_k,\n        stride_out_l, stride_out_bsize, stride_out_k,\n        BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_d1: tl.constexpr,\n        GROUP_SIZE_BSIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=1)\n    batch_id = tl.program_id(axis=0)\n    \n    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_k\n    group_id = pid // num_pid_in_group\n    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n    pid_k = (pid % num_pid_in_group) // group_size_bsize\n\n    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n    offs_k = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_d1 = tl.arange(0, BLOCK_SIZE_d1)\n\n    g_ptrs = g_ptr + (offs_bsize[:, None] * stride_g_bsize + offs_d1[None, :] * stride_g_d1)\n\n    su_tmp = batch_id * stride_su_l + (offs_d1[:, None] * stride_su_d1 + offs_k[None, :] * stride_su_k)\n    U1s_ptrs = U1s_ptr + su_tmp\n    S2s_ptrs = S2s_ptr + su_tmp\n\n    accumulator1 = tl.zeros((BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), dtype=tl.float32)\n    accumulator2 = tl.zeros((BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), dtype=tl.float32)\n    \n    for d1_i in range(0, tl.cdiv(d1, BLOCK_SIZE_d1)):\n        d1_offset = d1_i * BLOCK_SIZE_d1\n        g_mask = (offs_bsize[:, None] < BSIZE) & (offs_d1[None, :] < d1 - d1_offset)\n        g = tl.load(g_ptrs, mask=g_mask, other=0.0)\n        \n        su_mask = (offs_d1[:, None] < d1 - d1_offset) & (offs_k[None, :] < K)\n        U1s = tl.load(U1s_ptrs, mask=su_mask, other=0.0)\n        S2s = tl.load(S2s_ptrs, mask=su_mask, other=0.0)\n        \n        accumulator1 += tl.dot(g, U1s, allow_tf32=True)\n        accumulator2 += tl.dot(g, S2s, allow_tf32=True)\n        \n        g_ptrs += BLOCK_SIZE_d1 * stride_g_d1\n        U1s_ptrs += BLOCK_SIZE_d1 * stride_su_d1\n        S2s_ptrs += BLOCK_SIZE_d1 * stride_su_d1\n\n    out_tmp = batch_id * stride_out_l + stride_out_bsize * offs_bsize[:, None] + stride_out_k * offs_k[None, :]\n    g_U1s_ptrs = g_U1s_ptr + out_tmp\n    g_S2s_ptrs = g_S2s_ptr + out_tmp\n    \n    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_k[None, :] < K)\n    \n    tl.store(g_U1s_ptrs, accumulator1, mask=out_mask)\n    tl.store(g_S2s_ptrs, accumulator2, mask=out_mask)\n  \n@triton.autotune(\n    configs=[\n        # Optimized configs for (1, 128) ratio\n        triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_BSIZE': 512, 'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_BSIZE': 1024, 'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_BSIZE': 4}, num_stages=3, num_warps=8),\n        \n        # Optimized configs for (128, 1) ratio\n        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 512, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n        \n        # Configs for (1, 1), (2, 1), (1, 2) ratios\n        triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=8),\n        \n        # Configs for smaller dimensions (for low_rank values)\n        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_BSIZE': 4}, num_stages=2, num_warps=4),\n        \n        # Fallback configs\n        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_BSIZE': 4}, num_stages=1, num_warps=4),\n    ],\n    key=['BSIZE', 'd2', 'K', 'L'],\n)\n@triton.jit\ndef second_pass_gUS11_22_kernel(\n        g_U1s_ptr, g_S2s_ptr, S1s_ptr, U2s_ptr, out_ptr,\n        BSIZE, d2, K, L,\n        stride_g_U1s2_l, stride_g_U1s2_bsize, stride_g_U1s2_k,\n        stride_us_l, stride_us_k, stride_us_d2,\n        stride_out_bsize, stride_out_d2,\n        BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_d2: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n        GROUP_SIZE_BSIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    \n    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n    num_pid_d2 = tl.cdiv(d2, BLOCK_SIZE_d2)\n    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_d2\n    group_id = pid // num_pid_in_group\n    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n    pid_d2 = (pid % num_pid_in_group) // group_size_bsize\n\n    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n    offs_d2 = pid_d2 * BLOCK_SIZE_d2 + tl.arange(0, BLOCK_SIZE_d2)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    offs_bsize = tl.max_contiguous(tl.multiple_of(offs_bsize, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n    offs_d2 = tl.max_contiguous(tl.multiple_of(offs_d2, BLOCK_SIZE_d2), BLOCK_SIZE_d2)\n    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_K), BLOCK_SIZE_K)\n\n    in_tmp = offs_bsize[:, None] * stride_g_U1s2_bsize + offs_k[None, :] * stride_g_U1s2_k\n    us_tmp = offs_k[:, None] * stride_us_k + offs_d2[None, :] * stride_us_d2\n\n    accumulator = tl.zeros((BLOCK_SIZE_BSIZE, BLOCK_SIZE_d2), dtype=tl.float32)\n    \n    for l in range(0, L):\n        g_l_offset = l * stride_g_U1s2_l\n        s_l_offset = l * stride_us_l\n\n        g_U1s_ptrs = g_U1s_ptr + g_l_offset + in_tmp\n        g_S2s_ptrs = g_S2s_ptr + g_l_offset + in_tmp\n\n        S1s_ptrs = S1s_ptr + s_l_offset + us_tmp\n        U2s_ptrs = U2s_ptr + s_l_offset + us_tmp\n        \n        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n            k_offset = k * BLOCK_SIZE_K\n            in_mask = offs_k[None, :] < K - k_offset\n            us_mask = offs_k[:, None] < K - k_offset\n            \n            g_U1s = tl.load(g_U1s_ptrs, mask=in_mask, other=0.0)\n            g_S2s = tl.load(g_S2s_ptrs, mask=in_mask, other=0.0)\n            \n            S1s = tl.load(S1s_ptrs, mask=us_mask, other=0.0)\n            U2s = tl.load(U2s_ptrs, mask=us_mask, other=0.0)\n            \n            accumulator += tl.dot(g_U1s, S1s, allow_tf32=True)\n            accumulator += tl.dot(g_S2s, U2s, allow_tf32=True)\n\n            in_inc = BLOCK_SIZE_K * stride_g_U1s2_k\n            g_U1s_ptrs += in_inc\n            g_S2s_ptrs += in_inc\n            \n            us_inc = BLOCK_SIZE_K * stride_us_k\n            S1s_ptrs += us_inc\n            U2s_ptrs += us_inc\n\n    out_ptrs = out_ptr + stride_out_bsize * offs_bsize[:, None] + stride_out_d2 * offs_d2[None, :]\n    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_d2[None, :] < d2)\n    \n    tl.store(out_ptrs, accumulator, mask=out_mask)\n  \n@triton.autotune(\n    configs=[\n        # Optimized configs for (1, 128) ratio\n        triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 64, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_k': 32, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_d2': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_d2': 512, 'BLOCK_SIZE_k': 16, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 4}, num_stages=3, num_warps=8),\n        \n        # Optimized configs for (128, 1) ratio\n        triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_k': 256, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_d2': 4}, num_stages=2, num_warps=8),\n        \n        # Configs for (1, 1), (2, 1), (1, 2) ratios\n        triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_d2': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 64, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 4}, num_stages=2, num_warps=8),\n        \n        # Configs for smaller dimensions (for low_rank values)\n        triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 32, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_d2': 4}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_k': 16, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 4}, num_stages=2, num_warps=4),\n        \n        # Fallback configs\n        triton.Config({'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_k': 16, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 4}, num_stages=1, num_warps=4),\n    ],\n    key=['d2', 'k', 'BSIZE', 'L'],\n)\n@triton.jit\ndef calc_grad_S1s_kernel(\n        hin_ptr, g_U1s_ptr, grad_g_S1s_ptr,\n        d2, k, BSIZE, L,\n        stride_hin_bsize, stride_hin_BSIZE,\n        stride_su_l, stride_su_BSIZE, stride_su_k,\n        stride_out_l, stride_out_bsize, stride_out_k,\n        BLOCK_SIZE_d2: tl.constexpr, BLOCK_SIZE_k: tl.constexpr, BLOCK_SIZE_BSIZE: tl.constexpr,\n        GROUP_SIZE_d2: tl.constexpr\n):\n    pid = tl.program_id(axis=1)\n    batch_id = tl.program_id(axis=0)\n    \n    num_pid_bsize = tl.cdiv(d2, BLOCK_SIZE_d2)\n    num_pid_k = tl.cdiv(k, BLOCK_SIZE_k)\n    num_pid_in_group = GROUP_SIZE_d2 * num_pid_k\n    group_id = pid // num_pid_in_group\n    first_pid_bsize = group_id * GROUP_SIZE_d2\n    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_d2)\n    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n    pid_k = (pid % num_pid_in_group) // group_size_bsize\n\n    offs_bsize = pid_bsize * BLOCK_SIZE_d2 + tl.arange(0, BLOCK_SIZE_d2)\n    offs_k = pid_k * BLOCK_SIZE_k + tl.arange(0, BLOCK_SIZE_k)\n    offs_BSIZE = tl.arange(0, BLOCK_SIZE_BSIZE)\n\n    offs_bsize = tl.max_contiguous(tl.multiple_of(offs_bsize, BLOCK_SIZE_d2), BLOCK_SIZE_d2)\n    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_k), BLOCK_SIZE_k)\n    offs_BSIZE = tl.max_contiguous(tl.multiple_of(offs_BSIZE, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n    \n    hin_ptrs = hin_ptr + (offs_bsize[:, None] * stride_hin_bsize + offs_BSIZE[None, :] * stride_hin_BSIZE)\n    su_tmp = batch_id * stride_su_l + (offs_BSIZE[:, None] * stride_su_BSIZE + offs_k[None, :] * stride_su_k)\n    g_U1s_ptrs = g_U1s_ptr + su_tmp\n\n    accumulator = tl.zeros((BLOCK_SIZE_d2, BLOCK_SIZE_k), dtype=tl.float32)\n    \n    for BSIZE_i in range(0, tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)):\n        BSIZE_offset = BSIZE_i * BLOCK_SIZE_BSIZE\n        hin_mask = (offs_bsize[:, None] < d2) & (offs_BSIZE[None, :] < BSIZE - BSIZE_offset)\n        hin = tl.load(hin_ptrs, mask=hin_mask, other=0.0)\n        \n        su_mask = (offs_BSIZE[:, None] < BSIZE - BSIZE_offset) & (offs_k[None, :] < k)\n        g_U1s = tl.load(g_U1s_ptrs, mask=su_mask, other=0.0)\n        \n        accumulator += tl.dot(hin, g_U1s, allow_tf32=True)\n        \n        hin_ptrs += BLOCK_SIZE_BSIZE * stride_hin_BSIZE\n        g_U1s_ptrs += BLOCK_SIZE_BSIZE * stride_su_BSIZE\n\n    out_tmp = batch_id * stride_out_l + stride_out_bsize * offs_bsize[:, None] + stride_out_k * offs_k[None, :]\n    grad_g_S1s_ptrs = grad_g_S1s_ptr + out_tmp\n    \n    out_mask = (offs_bsize[:, None] < d2) & (offs_k[None, :] < k)\n    \n    tl.store(grad_g_S1s_ptrs, accumulator, mask=out_mask)\n  \n@triton.autotune(\n    configs=[\n        # Optimized configs for (1, 128) ratio - large batch size\n        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 512, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_BSIZE': 1024, 'BLOCK_SIZE_d2': 64, 'GROUP_SIZE_K': 4}, num_stages=3, num_warps=8),\n        \n        # Optimized configs for (128, 1) ratio\n        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 256, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 512, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=8),\n        \n        # Configs for (1, 1), (2, 1), (1, 2) ratios\n        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 256, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=8),\n        \n        # Configs for low_rank values\n        triton.Config({'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 64, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_K': 16, 'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=4),\n        \n        # Fallback configs\n        triton.Config({'BLOCK_SIZE_K': 16, 'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 4}, num_stages=1, num_warps=4),\n    ],\n    key=['K', 'd2', 'BSIZE', 'L'],\n)\n@triton.jit\ndef first_pass_U2s_hin_kernel(\n        hin_ptr, U2s_ptr, U2s_h_in_ptr,\n        K, d2, BSIZE, L,\n        stride_hin_d2, stride_hin_BSIZE,\n        stride_su_l, stride_su_K, stride_su_d2,\n        stride_out_l, stride_out_K, stride_out_BSIZE,\n        BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_d2: tl.constexpr,\n        GROUP_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(axis=1)\n    batch_id = tl.program_id(axis=0)\n    \n    num_pid_K = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_BSIZE = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n    num_pid_in_group = GROUP_SIZE_K * num_pid_BSIZE\n    group_id = pid // num_pid_in_group\n    first_pid_K = group_id * GROUP_SIZE_K\n    group_size_BSIZE = min(num_pid_K - first_pid_K, GROUP_SIZE_K)\n    pid_K = first_pid_K + ((pid % num_pid_in_group) % group_size_BSIZE)\n    pid_BSIZE = (pid % num_pid_in_group) // group_size_BSIZE\n\n    offs_K = pid_K * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_BSIZE = pid_BSIZE * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n    offs_d2 = tl.arange(0, BLOCK_SIZE_d2)\n\n    offs_K = tl.max_contiguous(tl.multiple_of(offs_K, BLOCK_SIZE_K), BLOCK_SIZE_K)\n    offs_BSIZE = tl.max_contiguous(tl.multiple_of(offs_BSIZE, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n    offs_d2 = tl.max_contiguous(tl.multiple_of(offs_d2, BLOCK_SIZE_d2), BLOCK_SIZE_d2)\n    \n    hin_ptrs = hin_ptr + (offs_d2[:, None] * stride_hin_d2 + offs_BSIZE[None, :] * stride_hin_BSIZE)\n\n    su_tmp = batch_id * stride_su_l + (offs_K[:, None] * stride_su_K + offs_d2[None, :] * stride_su_d2)\n    U2s_ptrs = U2s_ptr + su_tmp\n\n    accumulator1 = tl.zeros((BLOCK_SIZE_K, BLOCK_SIZE_BSIZE), dtype=tl.float32)\n    \n    for d2_i in range(0, tl.cdiv(d2, BLOCK_SIZE_d2)):\n        d2_offset = d2_i * BLOCK_SIZE_d2\n        hin_mask = (offs_d2[:, None] < d2 - d2_offset) & (offs_BSIZE[None, :] < BSIZE)\n        hin = tl.load(hin_ptrs, mask=hin_mask, other=0.0)\n        \n        su_mask = (offs_K[:, None] < K) & (offs_d2[None, :] < d2 - d2_offset)\n        U2s = tl.load(U2s_ptrs, mask=su_mask, other=0.0)\n        \n        accumulator1 += tl.dot(U2s, hin, allow_tf32=True)\n        \n        hin_ptrs += BLOCK_SIZE_d2 * stride_hin_d2\n        U2s_ptrs += BLOCK_SIZE_d2 * stride_su_d2\n\n    out_tmp = batch_id * stride_out_l + stride_out_K * offs_K[:, None] + stride_out_BSIZE * offs_BSIZE[None, :]\n    U2s_h_in_ptrs = U2s_h_in_ptr + out_tmp\n    \n    out_mask = (offs_K[:, None] < K) & (offs_BSIZE[None, :] < BSIZE)\n    \n    tl.store(U2s_h_in_ptrs, accumulator1, mask=out_mask)\n  \n@triton.autotune(\n    configs=[\n        # Optimized configs for (1, 128) ratio\n        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 256, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 512, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_K': 16, 'BLOCK_SIZE_d1': 1024, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 4}, num_stages=3, num_warps=8),\n        \n        # Optimized configs for (128, 1) ratio\n        triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 64, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_K': 512, 'BLOCK_SIZE_d1': 32, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=8),\n        \n        # Configs for (1, 1), (2, 1), (1, 2) ratios\n        triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 256, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=8),\n        \n        # Configs for small dimensions (for low_rank_options)\n        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 64, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 4}, num_stages=2, num_warps=4),\n        \n        # Fallback config\n        triton.Config({'BLOCK_SIZE_K': 16, 'BLOCK_SIZE_d1': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 4}, num_stages=1, num_warps=4),\n    ],\n    key=['K', 'BSIZE', 'd1', 'L'],\n)\n@triton.jit\ndef calc_grad_S2s_kernel(\n        g_ptr, U2s_hin_ptr, grad_S2s_ptr,\n        K, BSIZE, d1, L,\n        stride_g_BSIZE, stride_g_d1,\n        stride_su_l, stride_su_K, stride_su_BSIZE,\n        stride_out_l, stride_out_K, stride_out_d1,\n        BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_d1: tl.constexpr, BLOCK_SIZE_BSIZE: tl.constexpr,\n        GROUP_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(axis=1)\n    batch_id = tl.program_id(axis=0)\n    \n    num_pid_K = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_d1 = tl.cdiv(d1, BLOCK_SIZE_d1)\n    num_pid_in_group = GROUP_SIZE_K * num_pid_d1\n    group_id = pid // num_pid_in_group\n    first_pid_K = group_id * GROUP_SIZE_K\n    group_size_d1 = min(num_pid_K - first_pid_K, GROUP_SIZE_K)\n    pid_K = first_pid_K + ((pid % num_pid_in_group) % group_size_d1)\n    pid_d1 = (pid % num_pid_in_group) // group_size_d1\n\n    offs_K = pid_K * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_d1 = pid_d1 * BLOCK_SIZE_d1 + tl.arange(0, BLOCK_SIZE_d1)\n    offs_BSIZE = tl.arange(0, BLOCK_SIZE_BSIZE)\n\n    offs_K = tl.max_contiguous(tl.multiple_of(offs_K, BLOCK_SIZE_K), BLOCK_SIZE_K)\n    offs_d1 = tl.max_contiguous(tl.multiple_of(offs_d1, BLOCK_SIZE_d1), BLOCK_SIZE_d1)\n    offs_BSIZE = tl.max_contiguous(tl.multiple_of(offs_BSIZE, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n    \n    g_ptrs = g_ptr + (offs_BSIZE[:, None] * stride_g_BSIZE + offs_d1[None, :] * stride_g_d1)\n\n    su_tmp = batch_id * stride_su_l + (offs_K[:, None] * stride_su_K + offs_BSIZE[None, :] * stride_su_BSIZE)\n    U2s_hin_ptrs = U2s_hin_ptr + su_tmp\n\n    accumulator = tl.zeros((BLOCK_SIZE_K, BLOCK_SIZE_d1), dtype=tl.float32)\n    \n    for BSIZE_i in range(0, tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)):\n        BSIZE_offset = BSIZE_i * BLOCK_SIZE_BSIZE\n        g_mask = (offs_BSIZE[:, None] < BSIZE - BSIZE_offset) & (offs_d1[None, :] < d1)\n        g = tl.load(g_ptrs, mask=g_mask, other=0.0)\n        \n        su_mask = (offs_K[:, None] < K) & (offs_BSIZE[None, :] < BSIZE - BSIZE_offset)\n        U2s_hin = tl.load(U2s_hin_ptrs, mask=su_mask, other=0.0)\n        \n        accumulator += tl.dot(U2s_hin, g, allow_tf32=True)\n        \n        g_ptrs += BLOCK_SIZE_BSIZE * stride_g_BSIZE\n        U2s_hin_ptrs += BLOCK_SIZE_BSIZE * stride_su_BSIZE\n\n    out_tmp = batch_id * stride_out_l + stride_out_K * offs_K[:, None] + stride_out_d1 * offs_d1[None, :]\n    grad_S2s_ptrs = grad_S2s_ptr + out_tmp\n    \n    out_mask = (offs_K[:, None] < K) & (offs_d1[None, :] < d1)\n    \n    tl.store(grad_S2s_ptrs, accumulator, mask=out_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T02:26:28.049304Z","iopub.execute_input":"2025-05-01T02:26:28.049668Z","iopub.status.idle":"2025-05-01T02:26:28.079871Z","shell.execute_reply.started":"2025-05-01T02:26:28.049646Z","shell.execute_reply":"2025-05-01T02:26:28.079367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/panther/panther/nn/linear_tr.py\n%%writefile /kaggle/working/panther/panther/nn/linear_tr.py\nimport math\nfrom typing import Any, Tuple, List, Optional\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.nn import init\nimport triton\nfrom panther.random import scaled_sign_sketch as gen_U\nfrom torch.library import triton_op, wrap_triton\nfrom .linear_kernels import (\n    first_pass_kernel,\n    second_pass_kernel,\n    first_pass_gU1s_g_S2s_kernel,\n    second_pass_gUS11_22_kernel,\n    calc_grad_S1s_kernel,\n    first_pass_U2s_hin_kernel,\n    calc_grad_S2s_kernel\n)\n\n# Optimized for the parameter space:\n# ratios = [(1, 128), (128, 1), (1, 1), (2, 1), (1, 2)]\n# base_sizes = [256, 512, 1024, 8192, 16384]\n# num_terms_options = [1, 2, 3]\n# low_rank_options = [10, 15, 20, 50, 100, 150]\n\n@triton_op(\"panther::forward_op\", mutates_args={})\ndef forward_op(hin: torch.Tensor, S1s: torch.Tensor, S2s: torch.Tensor, U1s: torch.Tensor, U2s: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:\n    device = 'cuda'\n    \n    # first pass\n    ############\n    L = S2s.shape[0]\n    BSIZE, d2 = hin.shape\n    L, _, K = S1s.shape\n    \n    # Pre-allocate output tensors with the right data type for better precision\n    in1 = torch.empty((L, BSIZE, K), dtype=torch.float32, device=device)\n    in2 = torch.empty((L, BSIZE, K), dtype=torch.float32, device=device)\n    \n    # Extract strides for efficient memory access\n    stride_hin_bsize, stride_hin_d2 = hin.stride(0), hin.stride(1)\n    stride_su_l, stride_su_d2, stride_su_k = S1s.stride(0), S1s.stride(1), S1s.stride(2)\n    stride_out_l, stride_out_bsize, stride_out_k = in1.stride(0), in1.stride(1), in1.stride(2)\n    \n    # Grid configuration optimized for the given parameter space\n    grid = lambda META: (L, triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]) * triton.cdiv(K, META[\"BLOCK_SIZE_K\"]), )\n    \n    # First pass kernel call optimized for speed\n    wrap_triton(first_pass_kernel)[grid](\n        hin, S1s, U2s, in1, in2,\n        BSIZE, K, d2, L,\n        stride_hin_bsize, stride_hin_d2,\n        stride_su_l, stride_su_d2, stride_su_k,\n        stride_out_l, stride_out_bsize, stride_out_k,\n    )\n\n    # second pass\n    #############\n    bias_unsqueezed = bias.unsqueeze(0)\n    L, BSIZE, K = in1.shape\n    _, _, d1 = U1s.shape\n    \n    out = torch.empty((BSIZE, d1), dtype=torch.float32, device=device)\n\n    stride_in12_l, stride_in12_bsize, stride_in12_k = in1.stride(0), in1.stride(1), in1.stride(2)\n    stride_us_l, stride_us_k, stride_us_d1 = U1s.stride(0), U1s.stride(1), U1s.stride(2)\n    stride_bias_bsize, stride_bias_d1 = bias_unsqueezed.stride(0), bias_unsqueezed.stride(1)\n    stride_out_bsize, stride_out_d1 = out.stride(0), out.stride(1)\n    \n    # Grid configuration optimized for the given parameter space\n    grid = lambda META: (triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]) * triton.cdiv(d1, META[\"BLOCK_SIZE_D1\"]), )\n    \n    # Second pass kernel call optimized for speed\n    wrap_triton(second_pass_kernel)[grid](\n        in1, in2, U1s, S2s, bias_unsqueezed, out,\n        BSIZE, d1, K, L,\n        stride_in12_l, stride_in12_bsize, stride_in12_k,\n        stride_us_l, stride_us_k, stride_us_d1,\n        stride_bias_bsize, stride_bias_d1,\n        stride_out_bsize, stride_out_d1,\n    )\n    \n    return out\n\n@forward_op.register_kernel(\"cpu\")\ndef _(input, S1s, S2s, U1s, U2s, bias):\n    num_terms = S2s.shape[0]\n    # Efficiently perform the sum over all l terms\n    input = input.unsqueeze(0).expand(num_terms, input.shape[0], input.shape[1])\n    return (\n        ((input.bmm(S1s)).bmm(U1s)).mean(0) / 2\n        + ((input.bmm(U2s)).bmm(S2s)).mean(0) / 2\n        + bias\n    )\n    \n@triton_op(\"panther::backward_op\", mutates_args={})\ndef backward_op(hin: torch.Tensor, S1s: torch.Tensor, S2s: torch.Tensor, U1s: torch.Tensor, U2s: torch.Tensor, g: torch.Tensor) -> List[torch.Tensor]:\n    device = 'cuda'\n    num_terms = S2s.shape[0]\n        \n    hin = hin.transpose(0, 1)\n    U1s = U1s.transpose(1, 2)\n    S1s = S1s.transpose(1, 2)\n    U2s = U2s.transpose(1, 2)\n    S2s = S2s.transpose(1, 2)\n\n    # Pre-scale gradient for better numerical stability\n    g = g / (2 * num_terms)\n    \n    # first_pass_gU1s_g_S2s\n    #######################\n    BSIZE, d1 = g.shape\n    L, _, K = U1s.shape\n    \n    # Pre-allocate output tensors for better memory efficiency\n    g_U1s = torch.empty((L, BSIZE, K), dtype=torch.float32, device='cuda')\n    g_S2s = torch.empty((L, BSIZE, K), dtype=torch.float32, device='cuda')\n\n    stride_g_bsize, stride_g_d1 = g.stride(0), g.stride(1)\n    stride_su_l, stride_su_d1, stride_su_k = U1s.stride(0), U1s.stride(1), U1s.stride(2)\n    stride_out_l, stride_out_bsize, stride_out_k = g_U1s.stride(0), g_U1s.stride(1), g_U1s.stride(2)\n    \n    # Grid configuration optimized for the parameter space\n    grid = lambda META: (L, triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]) * triton.cdiv(K, META[\"BLOCK_SIZE_K\"]), )\n    \n    wrap_triton(first_pass_gU1s_g_S2s_kernel)[grid](\n        g, U1s, S2s, g_U1s, g_S2s,\n        BSIZE, K, d1, L,\n        stride_g_bsize, stride_g_d1,\n        stride_su_l, stride_su_d1, stride_su_k,\n        stride_out_l, stride_out_bsize, stride_out_k\n    )\n\n    # second_pass_gUS11_22\n    #######################\n    L, BSIZE, K = g_U1s.shape\n    _, _, d2 = S1s.shape\n    \n    grad = torch.empty((BSIZE, d2), dtype=torch.float32, device='cuda')\n\n    stride_g_U1s2_l, stride_g_U1s2_bsize, stride_g_U1s2_k = g_U1s.stride(0), g_U1s.stride(1), g_U1s.stride(2)\n    stride_us_l, stride_us_k, stride_us_d2 = S1s.stride(0), S1s.stride(1), S1s.stride(2)\n    stride_out_bsize, stride_out_d2 = grad.stride(0), grad.stride(1)\n    \n    grid = lambda META: (triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]) * triton.cdiv(d2, META[\"BLOCK_SIZE_d2\"]), )\n    \n    wrap_triton(second_pass_gUS11_22_kernel)[grid](\n        g_U1s, g_S2s, S1s, U2s, grad,\n        BSIZE, d2, K, L,\n        stride_g_U1s2_l, stride_g_U1s2_bsize, stride_g_U1s2_k,\n        stride_us_l, stride_us_k, stride_us_d2,\n        stride_out_bsize, stride_out_d2,\n    )\n        \n    # calc_grad_S1s\n    ################\n    d2, BSIZE = hin.shape\n    L, _, k = g_U1s.shape\n    \n    grad_S1s = torch.empty((L, d2, k), dtype=torch.float32, device=device)\n\n    stride_hin_bsize, stride_hin_BSIZE = hin.stride(0), hin.stride(1)\n    stride_su_l, stride_su_BSIZE, stride_su_k = g_U1s.stride(0), g_U1s.stride(1), g_U1s.stride(2)\n    stride_out_l, stride_out_bsize, stride_out_k = grad_S1s.stride(0), grad_S1s.stride(1), grad_S1s.stride(2)\n    \n    grid = lambda META: (L, triton.cdiv(d2, META[\"BLOCK_SIZE_d2\"]) * triton.cdiv(k, META[\"BLOCK_SIZE_k\"]), )\n    \n    wrap_triton(calc_grad_S1s_kernel)[grid](\n        hin, g_U1s, grad_S1s,\n        d2, k, BSIZE, L,\n        stride_hin_bsize, stride_hin_BSIZE,\n        stride_su_l, stride_su_BSIZE, stride_su_k,\n        stride_out_l, stride_out_bsize, stride_out_k\n    )\n    \n    # first_pass_U2s_hin\n    ####################\n    L, K, d2 = U2s.shape\n    _, BSIZE = hin.shape\n    \n    U2s_hin = torch.empty((L, K, BSIZE), dtype=torch.float32, device=device)\n\n    stride_hin_d2, stride_hin_BSIZE = hin.stride(0), hin.stride(1)\n    stride_su_l, stride_su_K, stride_su_d2 = U2s.stride(0), U2s.stride(1), U2s.stride(2)\n    stride_out_l, stride_out_K, stride_out_BSIZE = U2s_hin.stride(0), U2s_hin.stride(1), U2s_hin.stride(2)\n    \n    grid = lambda META: (L, triton.cdiv(K, META[\"BLOCK_SIZE_K\"]) * triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]), )\n    \n    wrap_triton(first_pass_U2s_hin_kernel)[grid](\n        hin, U2s, U2s_hin,\n        K, d2, BSIZE, L,\n        stride_hin_d2, stride_hin_BSIZE,\n        stride_su_l, stride_su_K, stride_su_d2,\n        stride_out_l, stride_out_K, stride_out_BSIZE\n    )\n    \n    # calc_grad_S2s\n    ###############\n    L, K, BSIZE = U2s_hin.shape\n    _, d1 = g.shape\n    \n    grad_S2s = torch.empty((L, K, d1), dtype=torch.float32, device=device)\n\n    stride_g_BSIZE, stride_g_d1 = g.stride(0), g.stride(1)\n    stride_su_l, stride_su_K, stride_su_BSIZE = U2s_hin.stride(0), U2s_hin.stride(1), U2s_hin.stride(2)\n    stride_out_l, stride_out_K, stride_out_d1 = grad_S2s.stride(0), grad_S2s.stride(1), grad_S2s.stride(2)\n    \n    grid = lambda META: (L, triton.cdiv(K, META[\"BLOCK_SIZE_K\"]) * triton.cdiv(d1, META[\"BLOCK_SIZE_d1\"]), )\n    \n    wrap_triton(calc_grad_S2s_kernel)[grid](\n        g, U2s_hin, grad_S2s,\n        K, BSIZE, d1, L,\n        stride_g_BSIZE, stride_g_d1,\n        stride_su_l, stride_su_K, stride_su_BSIZE,\n        stride_out_l, stride_out_K, stride_out_d1\n    )\n\n    return [\n        grad,\n        grad_S1s,\n        grad_S2s,\n        g.sum(0)\n    ]\n    \n@backward_op.register_kernel(\"cpu\")\ndef _(input, S1s, S2s, U1s, U2s, grad_output):\n    num_terms = S2s.shape[0]\n    g = grad_output / (2 * num_terms)\n    g = g.unsqueeze(0).expand(num_terms, g.shape[0], g.shape[1])\n    input = (\n        input.unsqueeze(0)\n        .expand(num_terms, input.shape[0], input.shape[1])\n        .transpose(1, 2)\n    )\n    U1s = U1s.transpose(1, 2)\n    S1s = S1s.transpose(1, 2)\n    U2s = U2s.transpose(1, 2)\n    S2s = S2s.transpose(1, 2)\n    t1 = g.bmm(U1s)\n    grad = t1.bmm(S1s).sum(0) + g.bmm(S2s).bmm(U2s).sum(0)\n    grad_S2s = (U2s.bmm(input)).bmm(g)\n    grad_S1s = input.bmm(g.bmm(U1s))\n\n    g = g[0]\n    return [\n        grad,\n        grad_S1s,\n        grad_S2s,\n        # sum g on batch dimension input.shape[0]\n        g.reshape(input.shape[2], -1).sum(0)\n    ]\n    \nclass SketchedLinearFunction_triton(Function):\n    @staticmethod\n    def forward(\n        input: torch.Tensor,\n        S1s: torch.Tensor,\n        S2s: torch.Tensor,\n        U1s: torch.Tensor,\n        U2s: torch.Tensor,\n        bias: torch.Tensor,\n    ): \n        # Use mixed precision for better performance when appropriate\n        input_dtype = input.dtype\n        compute_dtype = torch.float32\n        \n        # Cast inputs to compute dtype if needed\n        if input_dtype != compute_dtype:\n            input = input.to(compute_dtype)\n            S1s = S1s.to(compute_dtype)\n            S2s = S2s.to(compute_dtype)\n            U1s = U1s.to(compute_dtype)\n            U2s = U2s.to(compute_dtype)\n            bias = bias.to(compute_dtype)\n        \n        result = forward_op(input, S1s, S2s, U1s, U2s, bias)\n        \n        # Cast result back to input dtype if needed\n        if input_dtype != compute_dtype:\n            result = result.to(input_dtype)\n            \n        return result\n\n    @staticmethod\n    def setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any):\n        input, S1s, S2s, U1s, U2s, bias = inputs\n        ctx.save_for_backward(input, S1s, S2s, U1s, U2s, bias)\n\n    @staticmethod\n    def backward(ctx: Any, *grad_output: Any) -> Any:\n        hin, S1s, S2s, U1s, U2s, _ = ctx.saved_tensors\n        \n        # Use mixed precision for better performance\n        grad_dtype = grad_output[0].dtype\n        compute_dtype = torch.float32\n        \n        # Cast to compute dtype if needed\n        if grad_dtype != compute_dtype:\n            grad_output_casted = grad_output[0].to(compute_dtype)\n            hin = hin.to(compute_dtype)\n            S1s = S1s.to(compute_dtype)\n            S2s = S2s.to(compute_dtype)\n            U1s = U1s.to(compute_dtype)\n            U2s = U2s.to(compute_dtype)\n        else:\n            grad_output_casted = grad_output[0]\n        \n        grad_input, grad_S1s, grad_S2s, grad_bias = backward_op(hin, S1s, S2s, U1s, U2s, grad_output_casted)\n        \n        # Cast results back to original dtype if needed\n        if grad_dtype != compute_dtype:\n            grad_input = grad_input.to(grad_dtype)\n            grad_S1s = grad_S1s.to(grad_dtype)\n            grad_S2s = grad_S2s.to(grad_dtype)\n            grad_bias = grad_bias.to(grad_dtype)\n            \n        return grad_input, grad_S1s, grad_S2s, None, None, grad_bias\n\nclass SKLinear_triton(nn.Module):\n    __constants__ = [\"in_features\", \"out_features\", \"num_terms\", \"low_rank\"]\n    in_features: int\n    out_features: int\n    num_terms: int\n    low_rank: int\n    S1s: torch.Tensor\n    S2s: torch.Tensor\n    U1s: torch.Tensor\n    U2s: torch.Tensor\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        num_terms: int,\n        low_rank: int,\n        W_init=None,\n        bias: bool = True,\n        dtype=None,\n        device=None,\n    ):\n        factory_kwargs = {\"dtype\": dtype, \"device\": device}\n        super(SKLinear_triton, self).__init__()\n\n        self.num_terms = num_terms # l\n        self.low_rank = low_rank # k\n        self.out_features = out_features\n        self.in_features = in_features\n\n        # Register U1s and U2s as buffers since they are not learnable\n        # Based on the parameter space, optimize the initialization\n        self.register_buffer(\n            \"U1s\",\n            torch.stack(\n                [\n                    gen_U(low_rank, out_features, **factory_kwargs)\n                    for _ in range(num_terms)\n                ]\n            ),\n        )  # k(low rank)xd1(out) stacked along the zeros dimension (l) -> l x k x d1\n        self.register_buffer(\n            \"U2s\",\n            torch.stack(\n                [\n                    gen_U(in_features, low_rank, **factory_kwargs)\n                    for _ in range(num_terms)\n                ]\n            ),\n        )  # d2xk stacked along the zeros dimension (l) -> l x d2 x k\n\n        # W is used to only initialize S\n        if W_init is None:\n            W = torch.empty(in_features, out_features, **factory_kwargs) # d2 * d1\n            init.kaiming_uniform_(W, a=math.sqrt(5))\n        else:\n            W = W_init.T.detach().clone()\n\n        # S1s and S2s are precomputed but not updated in the backward pass\n        self.S1s = nn.Parameter(\n            torch.stack([torch.matmul(W, self.U1s[i].T) for i in range(num_terms)])\n        )  # d2xk stacked along the zeros dimension (l) -> l x d2 x k\n        self.S2s = nn.Parameter(\n            torch.stack([torch.matmul(self.U2s[i].T, W) for i in range(num_terms)])\n        )  # kxd1 stacked along the zeros dimension (l) -> l x k x d1\n\n        # Bias term initialized with a small standard deviation\n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_features, **factory_kwargs)) #1 * d1\n            fan_in, _ = init._calculate_fan_in_and_fan_out(W)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            init.uniform_(self.bias, -bound, bound)\n        else:\n            self.register_parameter(\"bias\", None)\n\n    def forward(self, h_in):\n        # Optimize memory contiguity for better performance\n        h_in = h_in.contiguous()\n        \n        # Make sure all tensors are contiguous for optimal kernel performance\n        S1s_contiguous = self.S1s.contiguous()\n        S2s_contiguous = self.S2s.contiguous() \n        U1s_contiguous = self.U1s.contiguous()\n        U2s_contiguous = self.U2s.contiguous()\n        bias_contiguous = self.bias.contiguous() if self.bias is not None else self.bias\n        \n        return SketchedLinearFunction_triton.apply(\n            h_in, S1s_contiguous, S2s_contiguous, U1s_contiguous, U2s_contiguous, bias_contiguous\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T02:26:28.080662Z","iopub.execute_input":"2025-05-01T02:26:28.080888Z","iopub.status.idle":"2025-05-01T02:26:28.108742Z","shell.execute_reply.started":"2025-05-01T02:26:28.080871Z","shell.execute_reply":"2025-05-01T02:26:28.108009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!export LC_ALL=\"en_US.UTF-8\"\n!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n!ldconfig /usr/lib64-nvidia","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T02:26:28.109484Z","iopub.execute_input":"2025-05-01T02:26:28.109700Z","iopub.status.idle":"2025-05-01T02:26:28.663117Z","shell.execute_reply.started":"2025-05-01T02:26:28.109675Z","shell.execute_reply":"2025-05-01T02:26:28.662425Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# checking ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom panther.nn.linear import SKLinear\nfrom panther.nn.linear_tr import SKLinear_triton\n\ndef compare_linear_implementations(batch_size=64, in_features=128, out_features=64, num_terms=4, low_rank=8, raise_error=True):\n    \"\"\"\n    Compare the outputs of the forward pass and gradients between regular SKLinear and Triton-accelerated SKLinear_triton.\n    \n    Args:\n        batch_size: Batch size for the input tensor\n        in_features: Number of input features\n        out_features: Number of output features\n        num_terms: Number of terms (l) in the sketched linear layer\n        low_rank: The low-rank dimension (k) in the sketched linear layer\n        raise_error: If True, raise AssertionError when comparisons fail\n    \n    Returns:\n        bool: True if all comparisons pass, False otherwise\n    \n    Raises:\n        AssertionError: If any comparison fails and raise_error is True\n    \"\"\"\n    # Set seed for reproducibility\n    torch.manual_seed(42)\n    \n    # Create identical layers\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Generate the same initialization for both layers\n    W_init = torch.randn(out_features, in_features, device=device)\n    \n    # Create both layer implementations with identical parameters\n    sk_linear = SKLinear(in_features, out_features, num_terms, low_rank, W_init=W_init, device=device)\n    sk_linear_triton = SKLinear_triton(in_features, out_features, num_terms, low_rank, W_init=W_init, device=device)\n    \n    # Ensure parameters match exactly by copying from sk_linear to sk_linear_triton\n    with torch.no_grad():\n        sk_linear_triton.S1s.copy_(sk_linear.S1s)\n        sk_linear_triton.S2s.copy_(sk_linear.S2s)\n        sk_linear_triton.U1s.copy_(sk_linear.U1s)\n        sk_linear_triton.U2s.copy_(sk_linear.U2s)\n        sk_linear_triton.bias.copy_(sk_linear.bias)\n    \n    # Create identical input\n    input_data = torch.randn(batch_size, in_features, device=device, requires_grad=True)\n    input_data_triton = input_data.clone().detach().requires_grad_(True)\n    \n    # Forward pass\n    output = sk_linear(input_data)\n    output_triton = sk_linear_triton(input_data_triton)\n    \n    # Check if forward outputs match\n    forward_match = torch.allclose(output, output_triton, rtol=1e-4, atol=1e-5)\n    print(f\"Forward outputs match: {forward_match}\")\n    if not forward_match:\n        max_diff = torch.max(torch.abs(output - output_triton))\n        error_msg = f\"Forward outputs don't match. Max difference: {max_diff.item()}\"\n        print(error_msg)\n        if raise_error:\n            assert forward_match, error_msg\n    \n    # Create identical gradients\n    grad_output = torch.randn_like(output)\n    grad_output_triton = grad_output.clone()\n    \n    # Backward pass\n    output.backward(grad_output)\n    output_triton.backward(grad_output_triton)\n    \n    # Check if input gradients match\n    input_grad_match = torch.allclose(input_data.grad, input_data_triton.grad, rtol=1e-4, atol=1e-5)\n    print(f\"Input gradients match: {input_grad_match}\")\n    if not input_grad_match:\n        max_diff = torch.max(torch.abs(input_data.grad - input_data_triton.grad))\n        error_msg = f\"Input gradients don't match. Max difference: {max_diff.item()}\"\n        print(error_msg)\n        if raise_error:\n            assert input_grad_match, error_msg\n    \n    # Check if S1s gradients match\n    S1s_grad_match = torch.allclose(sk_linear.S1s.grad, sk_linear_triton.S1s.grad, rtol=1e-4, atol=1e-5)\n    print(f\"S1s gradients match: {S1s_grad_match}\")\n    if not S1s_grad_match:\n        max_diff = torch.max(torch.abs(sk_linear.S1s.grad - sk_linear_triton.S1s.grad))\n        error_msg = f\"S1s gradients don't match. Max difference: {max_diff.item()}\"\n        print(error_msg)\n        if raise_error:\n            assert S1s_grad_match, error_msg\n    \n    # Check if S2s gradients match\n    S2s_grad_match = torch.allclose(sk_linear.S2s.grad, sk_linear_triton.S2s.grad, rtol=1e-4, atol=1e-5)\n    print(f\"S2s gradients match: {S2s_grad_match}\")\n    if not S2s_grad_match:\n        max_diff = torch.max(torch.abs(sk_linear.S2s.grad - sk_linear_triton.S2s.grad))\n        error_msg = f\"S2s gradients don't match. Max difference: {max_diff.item()}\"\n        print(error_msg)\n        if raise_error:\n            assert S2s_grad_match, error_msg\n    \n    # U1s and U2s are buffers, not parameters, so they may not have gradients\n    # Only check if both gradients exist\n    if hasattr(sk_linear.U1s, 'grad') and hasattr(sk_linear_triton.U1s, 'grad') and sk_linear.U1s.grad is not None and sk_linear_triton.U1s.grad is not None:\n        U1s_grad_match = torch.allclose(sk_linear.U1s.grad, sk_linear_triton.U1s.grad, rtol=1e-4, atol=1e-5)\n        print(f\"U1s gradients match: {U1s_grad_match}\")\n        if not U1s_grad_match:\n            max_diff = torch.max(torch.abs(sk_linear.U1s.grad - sk_linear_triton.U1s.grad))\n            error_msg = f\"U1s gradients don't match. Max difference: {max_diff.item()}\"\n            print(error_msg)\n            if raise_error:\n                assert U1s_grad_match, error_msg\n    else:\n        print(\"U1s gradients not available - skipping comparison\")\n        U1s_grad_match = True  # Skip this comparison\n    \n    # Check if U2s gradients match - only if both exist\n    if hasattr(sk_linear.U2s, 'grad') and hasattr(sk_linear_triton.U2s, 'grad') and sk_linear.U2s.grad is not None and sk_linear_triton.U2s.grad is not None:\n        U2s_grad_match = torch.allclose(sk_linear.U2s.grad, sk_linear_triton.U2s.grad, rtol=1e-4, atol=1e-5)\n        print(f\"U2s gradients match: {U2s_grad_match}\")\n        if not U2s_grad_match:\n            max_diff = torch.max(torch.abs(sk_linear.U2s.grad - sk_linear_triton.U2s.grad))\n            error_msg = f\"U2s gradients don't match. Max difference: {max_diff.item()}\"\n            print(error_msg)\n            if raise_error:\n                assert U2s_grad_match, error_msg\n    else:\n        print(\"U2s gradients not available - skipping comparison\")\n        U2s_grad_match = True  # Skip this comparison\n    \n    # Check if bias gradients match\n    bias_grad_match = torch.allclose(sk_linear.bias.grad, sk_linear_triton.bias.grad, rtol=1e-4, atol=1e-5)\n    print(f\"Bias gradients match: {bias_grad_match}\")\n    if not bias_grad_match:\n        max_diff = torch.max(torch.abs(sk_linear.bias.grad - sk_linear_triton.bias.grad))\n        error_msg = f\"Bias gradients don't match. Max difference: {max_diff.item()}\"\n        print(error_msg)\n        if raise_error:\n            assert bias_grad_match, error_msg\n    \n    # Return overall result\n    all_matches = all([forward_match, input_grad_match, S1s_grad_match, S2s_grad_match, U1s_grad_match, U2s_grad_match, bias_grad_match])\n    if raise_error:\n        assert all_matches, \"At least one comparison failed. See detailed errors above.\"\n    return all_matches\n\n\nif __name__ == \"__main__\":\n    print(\"Comparing SKLinear and SKLinear_triton implementations...\")\n    \n    # Test with default parameters\n    try:\n        result = compare_linear_implementations()\n        print(f\"All checks passed: {result}\")\n    except AssertionError as e:\n        print(f\"Default test failed: {e}\")\n    \n    # Test with different parameters\n    try:\n        result_small = compare_linear_implementations(batch_size=32, in_features=64, out_features=32, num_terms=2, low_rank=4)\n        print(f\"Small model checks passed: {result_small}\")\n    except AssertionError as e:\n        print(f\"Small model test failed: {e}\")\n    \n    try:\n        result_large = compare_linear_implementations(batch_size=128, in_features=256, out_features=128, num_terms=8, low_rank=16)\n        print(f\"Large model checks passed: {result_large}\")\n    except AssertionError as e:\n        print(f\"Large model test failed: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T02:31:58.457672Z","iopub.execute_input":"2025-05-01T02:31:58.458372Z","iopub.status.idle":"2025-05-01T02:31:58.499886Z","shell.execute_reply.started":"2025-05-01T02:31:58.458346Z","shell.execute_reply":"2025-05-01T02:31:58.499168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport numpy as np\nimport torch\nimport torch._dynamo\nimport torch._inductor.config as config\nimport itertools\nimport pandas as pd\n\n# Configure torch\nconfig.max_autotune_gemm = False\ntorch._dynamo.config.cache_size_limit = 2**16\ntorch._dynamo.config.accumulated_cache_size_limit = 2**16\n\ndef is_valid_params(in_features, out_features, num_terms, low_rank):\n    \"\"\"\n    Check if parameter combination is valid:\n    A combination is invalid if 2 * num_terms * low_rank * (out_features + in_features) >= out_features * in_features\n    \"\"\"\n    return 2 * num_terms * low_rank * (out_features + in_features) < out_features * in_features\n\nclass BenchmarkParams:\n    def __init__(self,\n                 in_features=256, \n                 out_features=256,\n                 num_terms=3,\n                 low_rank=8,\n                 batch_size=64, \n                 num_runs=200,\n                 warmup=15,\n                 device='cuda',\n                 dtype=torch.float16):\n        self.in_features = in_features\n        self.out_features = out_features\n        self.num_terms = num_terms\n        self.low_rank = low_rank\n        self.batch_size = batch_size\n        self.num_runs = num_runs\n        self.warmup = warmup\n        self.device = device\n        self.dtype = dtype\n\ndef benchmark_model(model, x, model_name, params):\n    \"\"\"\n    Generic benchmarking function for any PyTorch model.\n    \n    Args:\n        model: The PyTorch model to benchmark\n        x: Input tensor\n        model_name: Name of the model for logging\n        params: Benchmark parameters\n    \n    Returns:\n        Dictionary with benchmark results\n    \"\"\"\n    # Compile the model\n    model_compiled = torch.compile(\n        model,\n        backend=\"inductor\",\n        fullgraph=True,\n        dynamic=False\n    )\n    \n    # Benchmark forward pass\n    print(f\"\\n=== {model_name} FORWARD PASS BENCHMARK ===\")\n    \n    # Warmup runs for forward pass\n    model_compiled.eval()\n    with torch.no_grad():\n        for _ in range(params.warmup):\n            _ = model_compiled(x)\n    \n    torch.cuda.synchronize()\n    \n    # Actual timed runs for forward\n    forward_times = []\n    with torch.no_grad():\n        for _ in range(params.num_runs):\n            torch.cuda.synchronize()\n            start = time.perf_counter()\n            _ = model_compiled(x)\n            torch.cuda.synchronize()\n            end = time.perf_counter()\n            \n            forward_times.append((end - start) * 1000)  # Convert to ms\n    \n    mean_forward = np.mean(forward_times)\n    std_forward = np.std(forward_times)\n    print(f\"{model_name} forward: {mean_forward:.3f}  {std_forward:.3f} ms\")\n    \n    # Benchmark backward pass\n    print(f\"\\n=== {model_name} BACKWARD PASS BENCHMARK ===\")\n    \n    # Warmup runs for backward pass\n    model_compiled.train()\n    for _ in range(params.warmup):\n        out = model_compiled(x)\n        loss = out.sum()\n        loss.backward()\n        x.grad.zero_()\n    \n    torch.cuda.synchronize()\n    \n    # Actual timed runs for backward\n    backward_times = []\n    for _ in range(params.num_runs):\n        out = model_compiled(x)\n        loss = out.sum()\n        \n        torch.cuda.synchronize()\n        start = time.perf_counter()\n        loss.backward()\n        torch.cuda.synchronize()\n        end = time.perf_counter()\n        \n        backward_times.append((end - start) * 1000)  # Convert to ms\n        x.grad.zero_()\n    \n    mean_backward = np.mean(backward_times)\n    std_backward = np.std(backward_times)\n    print(f\"{model_name} backward: {mean_backward:.3f}  {std_backward:.3f} ms\")\n    \n    return {\n        \"forward\": {\n            \"mean\": mean_forward,\n            \"std\": std_forward,\n            \"times\": forward_times\n        },\n        \"backward\": {\n            \"mean\": mean_backward,\n            \"std\": std_backward,\n            \"times\": backward_times\n        }\n    }\n\ndef benchmark_model_factory(model_factory, model_name, params):\n    \"\"\"\n    Benchmark a model using a factory function.\n    \n    Args:\n        model_factory: Function that creates the model\n        model_name: Name of the model for logging\n        params: Benchmark parameters\n    \n    Returns:\n        Dictionary with benchmark results\n    \"\"\"\n    # Create the model\n    torch.manual_seed(42)\n    model = model_factory(params)\n    \n    # Create input tensor for benchmarking\n    x = torch.randn(params.batch_size, params.in_features, \n                  dtype=params.dtype, device=params.device, requires_grad=True)\n    \n    return benchmark_model(model, x, model_name, params)\n\nif __name__ == \"__main__\":\n    import torch.nn as nn\n    from panther.nn import SKLinear, SKLinear_triton\n    \n    # Parameter combinations to test\n    ratios = [(1, 128), (128, 1), (1, 1), (2, 1), (1, 2)]\n    base_sizes = [256, 512, 1024, 8192, 16384]\n    num_terms_options = [1, 2, 3]\n    low_rank_options = [10, 15, 20, 50, 100, 150]\n    \n    # Define model factories\n    def create_sklinear_triton(p):\n        return SKLinear_triton(p.in_features, p.out_features, \n                             p.num_terms, p.low_rank, \n                             dtype=p.dtype, device=p.device)\n    \n    models_to_benchmark = [\n        (create_sklinear_triton, \"SKLinear_triton\")\n    ]\n    \n    # Prepare data structure to store all results\n    results_data = []\n    \n    # Iterate through all parameter combinations\n    total_combinations = len(ratios) * len(base_sizes) * len(num_terms_options) * len(low_rank_options)\n    current_combo = 0\n    \n    for ratio, base_size in itertools.product(ratios, base_sizes):\n        ratio_in, ratio_out = ratio\n        \n        # Calculate actual dimensions based on ratio and base size\n        if ratio_in == 1:\n            in_features = base_size\n            out_features = base_size * ratio_out\n        else:\n            out_features = base_size\n            in_features = base_size * ratio_in\n        \n        for num_terms, low_rank in itertools.product(num_terms_options, low_rank_options):\n            current_combo += 1\n            print(f\"\\n\\n{'='*20} COMBINATION {current_combo}/{total_combinations} {'='*20}\")\n            print(f\"In features: {in_features}, Out features: {out_features}, Ratio: {ratio_in}:{ratio_out}\")\n            print(f\"Base size: {base_size}, Num terms: {num_terms}, Low rank: {low_rank}\")\n            \n            # Check if parameters are valid\n            is_valid = is_valid_params(in_features, out_features, num_terms, low_rank)\n            \n            if not is_valid:\n                print(f\"INVALID COMBINATION: 2 * {num_terms} * {low_rank} * ({out_features} + {in_features}) >= {out_features} * {in_features}\")\n                print(\"Skipping benchmarks for this invalid combination\")\n                \n                # Add invalid entry to results data\n                for model_name in [m[1] for m in models_to_benchmark]:\n                    results_data.append({\n                        'model': model_name,\n                        'in_features': in_features,\n                        'out_features': out_features,\n                        'ratio': f\"{ratio_in}:{ratio_out}\",\n                        'base_size': base_size,\n                        'num_terms': num_terms,\n                        'low_rank': low_rank,\n                        'forward_mean_ms': float('nan'),\n                        'forward_std_ms': float('nan'),\n                        'backward_mean_ms': float('nan'),\n                        'backward_std_ms': float('nan'),\n                        'is_valid': False,\n                        'error': \"Invalid parameter combination\"\n                    })\n                continue\n            \n            # Create parameter object for this combination\n            params = BenchmarkParams(\n                in_features=in_features,\n                out_features=out_features,\n                num_terms=num_terms,\n                low_rank=low_rank\n            )\n            \n            all_results = {}\n            for model_factory, model_name in models_to_benchmark:\n                print(f\"\\n{'='*20} Benchmarking {model_name} {'='*20}\")\n                try:\n                    results = benchmark_model_factory(model_factory, model_name, params)\n                    all_results[model_name] = results\n                    \n                    # Add result to our data collection\n                    results_data.append({\n                        'model': model_name,\n                        'in_features': in_features,\n                        'out_features': out_features,\n                        'ratio': f\"{ratio_in}:{ratio_out}\",\n                        'base_size': base_size,\n                        'num_terms': num_terms,\n                        'low_rank': low_rank,\n                        'forward_mean_ms': results['forward']['mean'],\n                        'forward_std_ms': results['forward']['std'],\n                        'backward_mean_ms': results['backward']['mean'],\n                        'backward_std_ms': results['backward']['std'],\n                        'is_valid': True\n                    })\n                except Exception as e:\n                    print(f\"Error benchmarking {model_name}: {e}\")\n                    # Add error entry to data\n                    results_data.append({\n                        'model': model_name,\n                        'in_features': in_features,\n                        'out_features': out_features,\n                        'ratio': f\"{ratio_in}:{ratio_out}\",\n                        'base_size': base_size,\n                        'num_terms': num_terms,\n                        'low_rank': low_rank,\n                        'forward_mean_ms': float('nan'),\n                        'forward_std_ms': float('nan'),\n                        'backward_mean_ms': float('nan'),\n                        'backward_std_ms': float('nan'),\n                        'is_valid': True,\n                        'error': str(e)\n                    })\n            \n            # Print comparative summary for this combination\n            if all_results:\n                print(\"\\n\" + \"=\"*60)\n                print(f\"{'='*20} SUMMARY FOR CURRENT COMBINATION {'='*20}\")\n                print(\"=\"*60)\n                print(f\"{'Model':<20} {'Forward (ms)':<25} {'Backward (ms)':<25}\")\n                print(\"-\"*60)\n                \n                for model_name, results in all_results.items():\n                    fwd = f\"{results['forward']['mean']:.3f}  {results['forward']['std']:.3f}\"\n                    bwd = f\"{results['backward']['mean']:.3f}  {results['backward']['std']:.3f}\"\n                    print(f\"{model_name:<20} {fwd:<25} {bwd:<25}\")\n    \n    # Create a DataFrame with all results\n    df = pd.DataFrame(results_data)\n    \n    # Save results to CSV\n    results_file = \"benchmark_results.csv\"\n    df.to_csv(results_file, index=False)\n    print(f\"\\nAll benchmark results saved to {results_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T02:26:32.038497Z","iopub.execute_input":"2025-05-01T02:26:32.039043Z","iopub.status.idle":"2025-05-01T02:26:32.164458Z","shell.execute_reply.started":"2025-05-01T02:26:32.039024Z","shell.execute_reply":"2025-05-01T02:26:32.163697Z"}},"outputs":[],"execution_count":null}]}