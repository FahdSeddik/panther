{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba4b0e09",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-05T00:12:02.598924Z",
     "iopub.status.busy": "2025-05-05T00:12:02.598625Z",
     "iopub.status.idle": "2025-05-05T00:12:02.725395Z",
     "shell.execute_reply": "2025-05-05T00:12:02.724783Z"
    },
    "papermill": {
     "duration": 0.135157,
     "end_time": "2025-05-05T00:12:02.726840",
     "exception": false,
     "start_time": "2025-05-05T00:12:02.591683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"github_repos_wildcard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3aa9bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:12:02.737634Z",
     "iopub.status.busy": "2025-05-05T00:12:02.737080Z",
     "iopub.status.idle": "2025-05-05T00:12:05.364099Z",
     "shell.execute_reply": "2025-05-05T00:12:05.363226Z"
    },
    "papermill": {
     "duration": 2.63397,
     "end_time": "2025-05-05T00:12:05.365770",
     "exception": false,
     "start_time": "2025-05-05T00:12:02.731800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'panther'...\r\n",
      "remote: Enumerating objects: 789, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (210/210), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (54/54), done.\u001b[K\r\n",
      "remote: Total 789 (delta 190), reused 156 (delta 156), pack-reused 579 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (789/789), 27.54 MiB | 17.49 MiB/s, done.\r\n",
      "Resolving deltas: 100% (454/454), done.\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "GITHUB_TOKEN = secret_value_0\n",
    "USER = \"gaserSami\"\n",
    "CLONE_URL = f\"https://{USER}:{GITHUB_TOKEN}@github.com/{USER}/panther.git\"\n",
    "get_ipython().system(f\"git clone --branch torch_compile {CLONE_URL}\")\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"panther\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "072673d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:12:05.378056Z",
     "iopub.status.busy": "2025-05-05T00:12:05.377547Z",
     "iopub.status.idle": "2025-05-05T00:14:29.299548Z",
     "shell.execute_reply": "2025-05-05T00:14:29.298455Z"
    },
    "papermill": {
     "duration": 143.929836,
     "end_time": "2025-05-05T00:14:29.301336",
     "exception": false,
     "start_time": "2025-05-05T00:12:05.371500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\r\n",
      "Collecting torch==2.6.0\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\r\n",
      "Collecting torchvision==0.21.0\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\r\n",
      "Collecting torchaudio==2.6.0\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (4.13.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2025.3.2)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.6.0)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.6.0)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.6.0)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch==2.6.0)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.6.0)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.6.0)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch==2.6.0)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.6.0)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.6.0)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch==2.6.0)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch==2.6.0)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting triton==3.2.0 (from torch==2.6.0)\r\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (1.13.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0) (11.1.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.21.0) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.21.0) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.21.0) (2024.2.0)\r\n",
      "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m928.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchaudio, torchvision\r\n",
      "  Attempting uninstall: triton\r\n",
      "    Found existing installation: triton 3.1.0\r\n",
      "    Uninstalling triton-3.1.0:\r\n",
      "      Successfully uninstalled triton-3.1.0\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.5.1+cu124\r\n",
      "    Uninstalling torch-2.5.1+cu124:\r\n",
      "      Successfully uninstalled torch-2.5.1+cu124\r\n",
      "  Attempting uninstall: torchaudio\r\n",
      "    Found existing installation: torchaudio 2.5.1+cu124\r\n",
      "    Uninstalling torchaudio-2.5.1+cu124:\r\n",
      "      Successfully uninstalled torchaudio-2.5.1+cu124\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.20.1+cu124\r\n",
      "    Uninstalling torchvision-0.20.1+cu124:\r\n",
      "      Successfully uninstalled torchvision-0.20.1+cu124\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu118 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118 torchaudio-2.6.0+cu118 torchvision-0.21.0+cu118 triton-3.2.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "973a7720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:14:29.390286Z",
     "iopub.status.busy": "2025-05-05T00:14:29.389949Z",
     "iopub.status.idle": "2025-05-05T00:14:32.012085Z",
     "shell.execute_reply": "2025-05-05T00:14:32.010897Z"
    },
    "papermill": {
     "duration": 2.668839,
     "end_time": "2025-05-05T00:14:32.013568",
     "exception": false,
     "start_time": "2025-05-05T00:14:29.344729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "import triton\n",
    "\n",
    "print(triton.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b314be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:14:32.155394Z",
     "iopub.status.busy": "2025-05-05T00:14:32.154500Z",
     "iopub.status.idle": "2025-05-05T00:14:34.308825Z",
     "shell.execute_reply": "2025-05-05T00:14:34.307927Z"
    },
    "papermill": {
     "duration": 2.200893,
     "end_time": "2025-05-05T00:14:34.310494",
     "exception": false,
     "start_time": "2025-05-05T00:14:32.109601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!export LC_ALL=\"en_US.UTF-8\"\n",
    "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "!ldconfig /usr/lib64-nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17543893",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:14:34.402722Z",
     "iopub.status.busy": "2025-05-05T00:14:34.402391Z",
     "iopub.status.idle": "2025-05-05T00:14:34.413862Z",
     "shell.execute_reply": "2025-05-05T00:14:34.413144Z"
    },
    "papermill": {
     "duration": 0.059158,
     "end_time": "2025-05-05T00:14:34.415174",
     "exception": false,
     "start_time": "2025-05-05T00:14:34.356016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/panther/panther/nn/linear_kernels/forward.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/panther/panther/nn/linear_kernels/forward.py\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=1, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4)\n",
    "    ],\n",
    "    key=['BSIZE', 'K', 'd2', 'L'],\n",
    ")\n",
    "@triton.jit\n",
    "def first_pass_kernel(\n",
    "        hin_ptr, S1s_ptr, U2s_ptr, out1_ptr, out2_ptr,\n",
    "        BSIZE, K, d2, L,\n",
    "        stride_hin_bsize, stride_hin_d2,\n",
    "        stride_su_l, stride_su_d2, stride_su_k,\n",
    "        stride_out_l, stride_out_bsize, stride_out_k,\n",
    "        BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_D2: tl.constexpr,\n",
    "        GROUP_SIZE_BSIZE: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(axis=1)\n",
    "    batch_id = tl.program_id(axis=0)\n",
    "    \n",
    "    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n",
    "    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n",
    "    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_k\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n",
    "    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n",
    "    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n",
    "    pid_k = (pid % num_pid_in_group) // group_size_bsize\n",
    "\n",
    "    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "    offs_k = pid_k *  BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
    "    offs_d2 = tl.arange(0, BLOCK_SIZE_D2)\n",
    "\n",
    "    offs_bsize = tl.max_contiguous(tl.multiple_of(offs_bsize, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n",
    "    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_K), BLOCK_SIZE_K)\n",
    "    offs_d2 = tl.max_contiguous(tl.multiple_of(offs_d2, BLOCK_SIZE_D2), BLOCK_SIZE_D2)\n",
    "    \n",
    "    hin_ptrs = hin_ptr + (offs_bsize[:, None] * stride_hin_bsize + offs_d2[None, :] * stride_hin_d2)\n",
    "\n",
    "    su_tmp = batch_id * stride_su_l + (offs_d2[:, None] * stride_su_d2 + offs_k[None, :] * stride_su_k)\n",
    "    S1s_ptrs = S1s_ptr + su_tmp\n",
    "    U2s_ptrs = U2s_ptr + su_tmp\n",
    "\n",
    "    accumulator1 = tl.full(shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), value=0.0, dtype=tl.float32)\n",
    "    accumulator2 = tl.full(shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), value=0.0, dtype=tl.float32)\n",
    "    \n",
    "    for d2_i in range(0, tl.cdiv(d2, BLOCK_SIZE_D2)):\n",
    "        hin_mask = (offs_bsize[:, None] < BSIZE) & (offs_d2[None, :] < d2 - d2_i * BLOCK_SIZE_D2)\n",
    "        hin = tl.load(hin_ptrs, mask=hin_mask, other=0.0)\n",
    "        \n",
    "        su_mask = (offs_d2[:, None] < d2 - d2_i * BLOCK_SIZE_D2) & (offs_k[None, :] < K)\n",
    "        S1s = tl.load(S1s_ptrs, mask=su_mask, other=0.0)\n",
    "        U2s = tl.load(U2s_ptrs, mask=su_mask, other=0.0)\n",
    "        \n",
    "        accumulator1 += tl.dot(hin, S1s, input_precision=\"ieee\")\n",
    "        accumulator2 += tl.dot(hin, U2s, input_precision=\"ieee\")\n",
    "        \n",
    "        hin_ptrs += BLOCK_SIZE_D2 * stride_hin_d2\n",
    "        S1s_ptrs += BLOCK_SIZE_D2 * stride_su_d2\n",
    "        U2s_ptrs += BLOCK_SIZE_D2 * stride_su_d2\n",
    "\n",
    "    out_tmp = batch_id * stride_out_l + stride_out_bsize * offs_bsize[:, None] + stride_out_k * offs_k[None, :]\n",
    "    out1_ptrs = out1_ptr + out_tmp\n",
    "    out2_ptrs = out2_ptr + out_tmp\n",
    "    \n",
    "    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_k[None, :] < K)\n",
    "    \n",
    "    tl.store(out1_ptrs, accumulator1, mask=out_mask)\n",
    "    tl.store(out2_ptrs, accumulator2, mask=out_mask)\n",
    "  \n",
    "@triton.autotune(\n",
    "    configs = [\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=1, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4)\n",
    "    ],\n",
    "    key=['BSIZE', 'd1', 'K', 'L'],\n",
    ")\n",
    "@triton.jit\n",
    "def second_pass_kernel(\n",
    "        in1_ptr, in2_ptr, U1s_ptr, S2s_ptr, bias_ptr, out_ptr,\n",
    "        BSIZE, d1, K, L,\n",
    "        stride_in12_l, stride_in12_bsize, stride_in12_k,\n",
    "        stride_us_l, stride_us_k, stride_us_d1,\n",
    "        stride_bias_bsize, stride_bias_d1,\n",
    "        stride_out_bsize, stride_out_d1,\n",
    "        BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_D1: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "        GROUP_SIZE_BSIZE: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    \n",
    "    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n",
    "    num_pid_d1 = tl.cdiv(d1, BLOCK_SIZE_D1)\n",
    "    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_d1\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n",
    "    GROUP_SIZE_BSIZE = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n",
    "    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % GROUP_SIZE_BSIZE)\n",
    "    pid_d1 = (pid % num_pid_in_group) // GROUP_SIZE_BSIZE\n",
    "\n",
    "    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "    offs_d1 = pid_d1 *  BLOCK_SIZE_D1 + tl.arange(0, BLOCK_SIZE_D1)\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "\n",
    "    offs_bsize = tl.max_contiguous(tl.multiple_of(offs_bsize, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n",
    "    offs_d1 = tl.max_contiguous(tl.multiple_of(offs_d1, BLOCK_SIZE_D1), BLOCK_SIZE_D1)\n",
    "    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_K), BLOCK_SIZE_K)\n",
    "\n",
    "    in_tmp = offs_bsize[:, None] * stride_in12_bsize + offs_k[None, :] * stride_in12_k\n",
    "    us_tmp = offs_k[:, None] * stride_us_k + offs_d1[None, :] * stride_us_d1\n",
    "\n",
    "    accumulator = tl.full(shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_D1), value=0.0, dtype=tl.float32)\n",
    "    \n",
    "    for l in range(0, L):\n",
    "        l_in_offset = l * stride_in12_l\n",
    "        l_us_offset = l * stride_us_l\n",
    "        \n",
    "        in1_ptrs = in1_ptr + l_in_offset + in_tmp\n",
    "        in2_ptrs = in2_ptr + l_in_offset + in_tmp\n",
    "    \n",
    "        U1s_ptrs = U1s_ptr + l_us_offset + us_tmp\n",
    "        S2s_ptrs = S2s_ptr + l_us_offset + us_tmp\n",
    "        \n",
    "        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "            in_mask = offs_k[None, :] < K - k * BLOCK_SIZE_K\n",
    "            in1 = tl.load(in1_ptrs, mask=in_mask, other=0.0)\n",
    "            in2 = tl.load(in2_ptrs, mask=in_mask, other=0.0)\n",
    "            \n",
    "            us_mask = offs_k[:, None] < K - k * BLOCK_SIZE_K\n",
    "            U1s = tl.load(U1s_ptrs, mask=us_mask, other=0.0)\n",
    "            S2s = tl.load(S2s_ptrs, mask=us_mask, other=0.0)\n",
    "            \n",
    "            accumulator += tl.dot(in1, U1s, input_precision=\"ieee\")\n",
    "            accumulator += tl.dot(in2, S2s, input_precision=\"ieee\")\n",
    "\n",
    "            in_inc = BLOCK_SIZE_K * stride_in12_k\n",
    "            in1_ptrs += in_inc\n",
    "            in2_ptrs += in_inc\n",
    "            \n",
    "            us_inc = BLOCK_SIZE_K * stride_us_k\n",
    "            U1s_ptrs += us_inc\n",
    "            S2s_ptrs += us_inc\n",
    "    \n",
    "    bias_ptrs = bias_ptr + offs_d1[None, :] * stride_bias_d1\n",
    "    bias_mask = (offs_d1[None, :] < d1)\n",
    "    bias = tl.load(bias_ptrs, mask=bias_mask, other=0.0)\n",
    "\n",
    "    accumulator *= (1.0/ (2.0 * L))\n",
    "    accumulator += bias\n",
    "\n",
    "    out_ptrs = out_ptr + stride_out_bsize * offs_bsize[:, None] + stride_out_d1 * offs_d1[None, :]\n",
    "    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_d1[None, :] < d1)\n",
    "    \n",
    "    tl.store(out_ptrs, accumulator, mask=out_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2e1e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:14:34.509705Z",
     "iopub.status.busy": "2025-05-05T00:14:34.509322Z",
     "iopub.status.idle": "2025-05-05T00:14:34.524928Z",
     "shell.execute_reply": "2025-05-05T00:14:34.524226Z"
    },
    "papermill": {
     "duration": 0.065961,
     "end_time": "2025-05-05T00:14:34.526102",
     "exception": false,
     "start_time": "2025-05-05T00:14:34.460141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/panther/panther/nn/linear_kernels/backward.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/panther/panther/nn/linear_kernels/backward.py\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=1, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4)\n",
    "    ],\n",
    "    key=['BSIZE', 'K', 'd1', 'L'],\n",
    ")\n",
    "@triton.jit\n",
    "def first_pass_gU1s_g_S2s_kernel(\n",
    "        g_ptr, U1s_ptr, S2s_ptr, g_U1s_ptr, g_S2s_ptr,\n",
    "        BSIZE, K, d1, L,\n",
    "        stride_g_bsize, stride_g_d1,\n",
    "        stride_su_l, stride_su_d1, stride_su_k,\n",
    "        stride_out_l, stride_out_bsize, stride_out_k,\n",
    "        BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_d1: tl.constexpr,\n",
    "        GROUP_SIZE_BSIZE: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(axis=1)\n",
    "    batch_id = tl.program_id(axis=0)\n",
    "    \n",
    "    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n",
    "    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n",
    "    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_k\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n",
    "    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n",
    "    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n",
    "    pid_k = (pid % num_pid_in_group) // group_size_bsize\n",
    "\n",
    "    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "    offs_k = pid_k *  BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
    "    offs_d1 = tl.arange(0, BLOCK_SIZE_d1)\n",
    "\n",
    "    g_ptrs = g_ptr + (offs_bsize[:, None] * stride_g_bsize + offs_d1[None, :] * stride_g_d1)\n",
    "\n",
    "    su_tmp = batch_id * stride_su_l + (offs_d1[:, None] * stride_su_d1 + offs_k[None, :] * stride_su_k)\n",
    "    U1s_ptrs = U1s_ptr + su_tmp\n",
    "    S2s_ptrs = S2s_ptr + su_tmp\n",
    "\n",
    "    accumulator1 = tl.full(shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), value=0.0, dtype=tl.float32)\n",
    "    accumulator2 = tl.full(shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), value=0.0, dtype=tl.float32)\n",
    "    \n",
    "    \n",
    "    for d1_i in range(0, tl.cdiv(d1, BLOCK_SIZE_d1)):\n",
    "        g = tl.load(g_ptrs, mask=(offs_d1[None, :] < d1 - d1_i * BLOCK_SIZE_d1), other=0.0)\n",
    "        \n",
    "        su_mask = (offs_d1[:, None] < d1 - d1_i * BLOCK_SIZE_d1)\n",
    "        U1s = tl.load(U1s_ptrs, mask=su_mask, other=0.0)\n",
    "        S2s = tl.load(S2s_ptrs, mask=su_mask, other=0.0)\n",
    "        \n",
    "        accumulator1 += tl.dot(g, U1s, input_precision=\"ieee\")\n",
    "        accumulator2 += tl.dot(g, S2s, input_precision=\"ieee\")\n",
    "        \n",
    "        g_ptrs += BLOCK_SIZE_d1 * stride_g_d1\n",
    "        U1s_ptrs += BLOCK_SIZE_d1 * stride_su_d1\n",
    "        S2s_ptrs += BLOCK_SIZE_d1 * stride_su_d1\n",
    "\n",
    "    out_tmp = batch_id * stride_out_l + stride_out_bsize * offs_bsize[:, None] + stride_out_k * offs_k[None, :]\n",
    "    g_U1s_ptrs = g_U1s_ptr + out_tmp\n",
    "    g_S2s_ptrs = g_S2s_ptr + out_tmp\n",
    "    \n",
    "    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_k[None, :] < K)\n",
    "    \n",
    "    tl.store(g_U1s_ptrs, accumulator1, mask=out_mask)\n",
    "    tl.store(g_S2s_ptrs, accumulator2, mask=out_mask)\n",
    "\n",
    "  \n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=1, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4)\n",
    "    ],\n",
    "    key=['BSIZE', 'd2', 'K', 'L'],\n",
    ")\n",
    "@triton.jit\n",
    "def second_pass_gUS11_22_kernel(\n",
    "        g_U1s_ptr, g_S2s_ptr, S1s_ptr, U2s_ptr, out_ptr,\n",
    "        BSIZE, d2, K, L,\n",
    "        stride_g_U1s2_l, stride_g_U1s2_bsize, stride_g_U1s2_k,\n",
    "        stride_us_l, stride_us_k, stride_us_d2,\n",
    "        stride_out_bsize, stride_out_d2,\n",
    "        BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_d2: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "        GROUP_SIZE_BSIZE: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    \n",
    "    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n",
    "    num_pid_d2 = tl.cdiv(d2, BLOCK_SIZE_d2)\n",
    "    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_d2\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n",
    "    GROUP_SIZE_BSIZE = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n",
    "    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % GROUP_SIZE_BSIZE)\n",
    "    pid_d2 = (pid % num_pid_in_group) // GROUP_SIZE_BSIZE\n",
    "\n",
    "    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "    offs_d2 = pid_d2 *  BLOCK_SIZE_d2 + tl.arange(0, BLOCK_SIZE_d2)\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "\n",
    "    in_tmp = offs_bsize[:, None] * stride_g_U1s2_bsize + offs_k[None, :] * stride_g_U1s2_k\n",
    "    us_tmp = offs_k[:, None] * stride_us_k + offs_d2[None, :] * stride_us_d2\n",
    "\n",
    "    accumulator = tl.full(shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_d2), value=0.0, dtype=tl.float32)\n",
    "    \n",
    "    for l in range(0, L):\n",
    "        g_l_offset = l * stride_g_U1s2_l  # Offset for g_U1s and g_S2s\n",
    "        s_l_offset = l * stride_us_l      # Offset for S1s and U2s\n",
    "\n",
    "        g_U1s_ptrs = g_U1s_ptr + g_l_offset + in_tmp\n",
    "        g_S2s_ptrs = g_S2s_ptr + g_l_offset + in_tmp\n",
    "\n",
    "        S1s_ptrs = S1s_ptr + s_l_offset + us_tmp\n",
    "        U2s_ptrs = U2s_ptr + s_l_offset + us_tmp\n",
    "        \n",
    "        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "            in_mask = offs_k[None, :] < K - k * BLOCK_SIZE_K\n",
    "            g_U1s = tl.load(g_U1s_ptrs, mask=in_mask, other=0.0)\n",
    "            g_S2s = tl.load(g_S2s_ptrs, mask=in_mask, other=0.0)\n",
    "            \n",
    "            us_mask = offs_k[:, None] < K - k * BLOCK_SIZE_K\n",
    "            S1s = tl.load(S1s_ptrs, mask=us_mask, other=0.0)\n",
    "            U2s = tl.load(U2s_ptrs, mask=us_mask, other=0.0)\n",
    "            \n",
    "            accumulator += tl.dot(g_U1s, S1s, input_precision=\"ieee\")\n",
    "            accumulator += tl.dot(g_S2s, U2s, input_precision=\"ieee\")\n",
    "\n",
    "            in_inc = BLOCK_SIZE_K * stride_g_U1s2_k\n",
    "            g_U1s_ptrs += in_inc\n",
    "            g_S2s_ptrs += in_inc\n",
    "            \n",
    "            us_inc = BLOCK_SIZE_K * stride_us_k\n",
    "            S1s_ptrs += us_inc\n",
    "            U2s_ptrs += us_inc\n",
    "    \n",
    "    # accumulator *= (1.0/ (2.0 * L))\n",
    "\n",
    "    out_ptrs = out_ptr + stride_out_bsize * offs_bsize[:, None] + stride_out_d2 * offs_d2[None, :]\n",
    "    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_d2[None, :] < d2)\n",
    "    \n",
    "    tl.store(out_ptrs, accumulator, mask=out_mask)\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_k': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=1, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 256, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 64, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_k': 64, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 256, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_k': 64, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 256, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 64, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 32, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4)\n",
    "    ],\n",
    "    key=['d2', 'K', 'BSIZE', 'L'],\n",
    ")\n",
    "@triton.jit\n",
    "def calc_grad_S1s_kernel(\n",
    "        hin_ptr, g_U1s_ptr, grad_g_S1s_ptr,\n",
    "        d2, k, BSIZE, L,\n",
    "        stride_hin_bsize, stride_hin_BSIZE,\n",
    "        stride_su_l, stride_su_BSIZE, stride_su_k,\n",
    "        stride_out_l, stride_out_bsize, stride_out_k,\n",
    "        BLOCK_SIZE_d2: tl.constexpr, BLOCK_SIZE_k: tl.constexpr, BLOCK_SIZE_BSIZE: tl.constexpr,\n",
    "        GROUP_SIZE_d2: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(axis=1)\n",
    "    batch_id = tl.program_id(axis=0)\n",
    "    \n",
    "    num_pid_bsize = tl.cdiv(d2, BLOCK_SIZE_d2)\n",
    "    num_pid_k = tl.cdiv(k, BLOCK_SIZE_k)\n",
    "    num_pid_in_group = GROUP_SIZE_d2 * num_pid_k\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_bsize = group_id * GROUP_SIZE_d2\n",
    "    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_d2)\n",
    "    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n",
    "    pid_k = (pid % num_pid_in_group) // group_size_bsize\n",
    "\n",
    "    offs_bsize = pid_bsize * BLOCK_SIZE_d2 + tl.arange(0, BLOCK_SIZE_d2)\n",
    "    offs_k = pid_k *  BLOCK_SIZE_k + tl.arange(0, BLOCK_SIZE_k)\n",
    "    offs_BSIZE = tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "\n",
    "    offs_bsize = tl.max_contiguous(tl.multiple_of(offs_bsize, BLOCK_SIZE_d2), BLOCK_SIZE_d2)\n",
    "    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_k), BLOCK_SIZE_k)\n",
    "    offs_BSIZE = tl.max_contiguous(tl.multiple_of(offs_BSIZE, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n",
    "    \n",
    "    hin_ptrs = hin_ptr + (offs_bsize[:, None] * stride_hin_bsize + offs_BSIZE[None, :] * stride_hin_BSIZE)\n",
    "\n",
    "    su_tmp = batch_id * stride_su_l + (offs_BSIZE[:, None] * stride_su_BSIZE + offs_k[None, :] * stride_su_k)\n",
    "    g_U1s_ptrs = g_U1s_ptr + su_tmp\n",
    "\n",
    "    accumulator1 = tl.full(shape=(BLOCK_SIZE_d2, BLOCK_SIZE_k), value=0.0, dtype=tl.float32)\n",
    "    accumulator2 = tl.full(shape=(BLOCK_SIZE_d2, BLOCK_SIZE_k), value=0.0, dtype=tl.float32)\n",
    "    \n",
    "    for BSIZE_i in range(0, tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)):\n",
    "        hin_mask = (offs_bsize[:, None] < d2) & (offs_BSIZE[None, :] < BSIZE - BSIZE_i * BLOCK_SIZE_BSIZE)\n",
    "        hin = tl.load(hin_ptrs, mask=hin_mask, other=0.0)\n",
    "        \n",
    "        su_mask = (offs_BSIZE[:, None] < BSIZE - BSIZE_i * BLOCK_SIZE_BSIZE) & (offs_k[None, :] < k)\n",
    "        g_U1s = tl.load(g_U1s_ptrs, mask=su_mask, other=0.0)\n",
    "        \n",
    "        accumulator1 += tl.dot(hin, g_U1s, input_precision=\"ieee\")\n",
    "        \n",
    "        hin_ptrs += BLOCK_SIZE_BSIZE * stride_hin_BSIZE\n",
    "        g_U1s_ptrs += BLOCK_SIZE_BSIZE * stride_su_BSIZE\n",
    "\n",
    "    out_tmp = batch_id * stride_out_l + stride_out_bsize * offs_bsize[:, None] + stride_out_k * offs_k[None, :]\n",
    "    grad_g_S1s_ptrs = grad_g_S1s_ptr + out_tmp\n",
    "    \n",
    "    out_mask = (offs_bsize[:, None] < d2) & (offs_k[None, :] < k)\n",
    "    \n",
    "    tl.store(grad_g_S1s_ptrs, accumulator1, mask=out_mask)\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=1, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4)\n",
    "    ],\n",
    "    key=['K', 'd2', 'BSIZE', 'L'],\n",
    ")\n",
    "@triton.jit\n",
    "def first_pass_U2s_hin_kernel(\n",
    "        hin_ptr, U2s_ptr, U2s_h_in_ptr,\n",
    "        K, d2, BSIZE, L,\n",
    "        stride_hin_d2, stride_hin_BSIZE,\n",
    "        stride_su_l, stride_su_K, stride_su_d2,\n",
    "        stride_out_l, stride_out_K, stride_out_BSIZE,\n",
    "        BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_d2: tl.constexpr,\n",
    "        GROUP_SIZE_K: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(axis=1)\n",
    "    batch_id = tl.program_id(axis=0)\n",
    "    \n",
    "    num_pid_K = tl.cdiv(K, BLOCK_SIZE_K)\n",
    "    num_pid_BSIZE = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n",
    "    num_pid_in_group = GROUP_SIZE_K * num_pid_BSIZE\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_K = group_id * GROUP_SIZE_K\n",
    "    group_size_BSIZE = min(num_pid_K - first_pid_K, GROUP_SIZE_K)\n",
    "    pid_K = first_pid_K + ((pid % num_pid_in_group) % group_size_BSIZE)\n",
    "    pid_BSIZE = (pid % num_pid_in_group) // group_size_BSIZE\n",
    "\n",
    "    offs_K = pid_K * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
    "    offs_BSIZE = pid_BSIZE *  BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "    offs_d2 = tl.arange(0, BLOCK_SIZE_d2)\n",
    "\n",
    "    offs_K = tl.max_contiguous(tl.multiple_of(offs_K, BLOCK_SIZE_K), BLOCK_SIZE_K)\n",
    "    offs_BSIZE = tl.max_contiguous(tl.multiple_of(offs_BSIZE, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n",
    "    offs_d2 = tl.max_contiguous(tl.multiple_of(offs_d2, BLOCK_SIZE_d2), BLOCK_SIZE_d2)\n",
    "    \n",
    "    hin_ptrs = hin_ptr + (offs_d2[:, None] * stride_hin_d2 + offs_BSIZE[None, :] * stride_hin_BSIZE)\n",
    "\n",
    "    su_tmp = batch_id * stride_su_l + (offs_K[:, None] * stride_su_K + offs_d2[None, :] * stride_su_d2)\n",
    "    U2s_ptrs = U2s_ptr + su_tmp\n",
    "\n",
    "    accumulator1 = tl.full(shape=(BLOCK_SIZE_K, BLOCK_SIZE_BSIZE), value=0.0, dtype=tl.float32)\n",
    "    \n",
    "    for d2_i in range(0, tl.cdiv(d2, BLOCK_SIZE_d2)):\n",
    "        hin_mask = (offs_d2[:, None] < d2 - d2_i * BLOCK_SIZE_d2) & (offs_BSIZE[None, :] < BSIZE)\n",
    "        hin = tl.load(hin_ptrs, mask=hin_mask, other=0.0)\n",
    "        \n",
    "        su_mask = (offs_K[:, None] < K) & (offs_d2[None, :] < d2 - d2_i * BLOCK_SIZE_d2)\n",
    "        U2s = tl.load(U2s_ptrs, mask=su_mask, other=0.0)\n",
    "        \n",
    "        accumulator1 += tl.dot(U2s, hin, input_precision=\"ieee\")\n",
    "        \n",
    "        hin_ptrs += BLOCK_SIZE_d2 * stride_hin_d2\n",
    "        U2s_ptrs += BLOCK_SIZE_d2 * stride_su_d2\n",
    "\n",
    "    out_tmp = batch_id * stride_out_l + stride_out_K * offs_K[:, None] + stride_out_BSIZE * offs_BSIZE[None, :]\n",
    "    U2s_h_in_ptrs = U2s_h_in_ptr + out_tmp\n",
    "    \n",
    "    out_mask = (offs_K[:, None] < K) & (offs_BSIZE[None, :] < BSIZE)\n",
    "    \n",
    "    tl.store(U2s_h_in_ptrs, accumulator1, mask=out_mask)\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=1, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 256, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 64, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 64, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 256, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 64, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 256, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 64, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 32, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4)\n",
    "    ],\n",
    "    key=['K', 'BSIZE', 'd1', 'L'],\n",
    ")\n",
    "@triton.jit\n",
    "def calc_grad_S2s_kernel(\n",
    "        g_ptr, U2s_hin_ptr, grad_S2s_ptr,\n",
    "        K, BSIZE, d1, L,\n",
    "        stride_g_BSIZE, stride_g_d1,\n",
    "        stride_su_l, stride_su_K, stride_su_BSIZE,\n",
    "        stride_out_l, stride_out_K, stride_out_d1,\n",
    "        BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_d1: tl.constexpr, BLOCK_SIZE_BSIZE: tl.constexpr,\n",
    "        GROUP_SIZE_K: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(axis=1)\n",
    "    batch_id = tl.program_id(axis=0)\n",
    "    \n",
    "    num_pid_K = tl.cdiv(K, BLOCK_SIZE_K)\n",
    "    num_pid_d1 = tl.cdiv(d1, BLOCK_SIZE_d1)\n",
    "    num_pid_in_group = GROUP_SIZE_K * num_pid_d1\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_K = group_id * GROUP_SIZE_K\n",
    "    group_size_d1 = min(num_pid_K - first_pid_K, GROUP_SIZE_K)\n",
    "    pid_K = first_pid_K + ((pid % num_pid_in_group) % group_size_d1)\n",
    "    pid_d1 = (pid % num_pid_in_group) // group_size_d1\n",
    "\n",
    "    offs_K = pid_K * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
    "    offs_d1 = pid_d1 *  BLOCK_SIZE_d1 + tl.arange(0, BLOCK_SIZE_d1)\n",
    "    offs_BSIZE = tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "\n",
    "    offs_K = tl.max_contiguous(tl.multiple_of(offs_K, BLOCK_SIZE_K), BLOCK_SIZE_K)\n",
    "    offs_d1 = tl.max_contiguous(tl.multiple_of(offs_d1, BLOCK_SIZE_d1), BLOCK_SIZE_d1)\n",
    "    offs_BSIZE = tl.max_contiguous(tl.multiple_of(offs_BSIZE, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n",
    "    \n",
    "    g_ptrs = g_ptr + (offs_BSIZE[:, None] * stride_g_BSIZE + offs_d1[None, :] * stride_g_d1)\n",
    "\n",
    "    su_tmp = batch_id * stride_su_l + (offs_K[:, None] * stride_su_K + offs_BSIZE[None, :] * stride_su_BSIZE)\n",
    "    U2s_hin_ptrs = U2s_hin_ptr + su_tmp\n",
    "\n",
    "    accumulator1 = tl.full(shape=(BLOCK_SIZE_K, BLOCK_SIZE_d1), value=0.0, dtype=tl.float32)\n",
    "    \n",
    "    for BSIZE_i in range(0, tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)):\n",
    "        g_mask = (offs_BSIZE[:, None] < BSIZE - BSIZE_i * BLOCK_SIZE_BSIZE) & (offs_d1[None, :] < d1)\n",
    "        g = tl.load(g_ptrs, mask=g_mask, other=0.0)\n",
    "        \n",
    "        su_mask = (offs_K[:, None] < K) & (offs_BSIZE[None, :] < BSIZE - BSIZE_i * BLOCK_SIZE_BSIZE)\n",
    "        U2s_hin = tl.load(U2s_hin_ptrs, mask=su_mask, other=0.0)\n",
    "        \n",
    "        accumulator1 += tl.dot(U2s_hin, g, input_precision=\"ieee\")\n",
    "        \n",
    "        g_ptrs += BLOCK_SIZE_BSIZE * stride_g_BSIZE\n",
    "        U2s_hin_ptrs += BLOCK_SIZE_BSIZE * stride_su_BSIZE\n",
    "\n",
    "    out_tmp = batch_id * stride_out_l + stride_out_K * offs_K[:, None] + stride_out_d1 * offs_d1[None, :]\n",
    "    grad_S2s_ptrs = grad_S2s_ptr + out_tmp\n",
    "    \n",
    "    out_mask = (offs_K[:, None] < K) & (offs_d1[None, :] < d1)\n",
    "    \n",
    "    tl.store(grad_S2s_ptrs, accumulator1, mask=out_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea090bae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:14:34.617526Z",
     "iopub.status.busy": "2025-05-05T00:14:34.617202Z",
     "iopub.status.idle": "2025-05-05T00:14:34.627855Z",
     "shell.execute_reply": "2025-05-05T00:14:34.627193Z"
    },
    "papermill": {
     "duration": 0.05855,
     "end_time": "2025-05-05T00:14:34.629104",
     "exception": false,
     "start_time": "2025-05-05T00:14:34.570554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/panther/panther/nn/linear_tr.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/panther/panther/nn/linear_tr.py\n",
    "import math\n",
    "from typing import Any, Tuple, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from torch.nn import init\n",
    "import triton\n",
    "from panther.random import scaled_sign_sketch as gen_U\n",
    "from torch.library import triton_op, wrap_triton\n",
    "from .linear_kernels import (\n",
    "    first_pass_kernel,\n",
    "    second_pass_kernel,\n",
    "    first_pass_gU1s_g_S2s_kernel,\n",
    "    second_pass_gUS11_22_kernel,\n",
    "    calc_grad_S1s_kernel,\n",
    "    first_pass_U2s_hin_kernel,\n",
    "    calc_grad_S2s_kernel\n",
    ")\n",
    "\n",
    "@triton_op(\"panther::forward_op\", mutates_args={})\n",
    "def forward_op(hin: torch.Tensor, S1s: torch.Tensor, S2s: torch.Tensor, U1s: torch.Tensor, U2s: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:\n",
    "    device = 'cuda'\n",
    "    \n",
    "    # first pass\n",
    "    ############\n",
    "    L = S2s.shape[0]\n",
    "    BSIZE, d2 = hin.shape\n",
    "    L, _, K = S1s.shape\n",
    "    \n",
    "    in1 = torch.empty((L, BSIZE, K), dtype=torch.float32, device=device)\n",
    "    in2 = torch.empty((L, BSIZE, K), dtype=torch.float32, device=device)\n",
    "    \n",
    "    stride_hin_bsize, stride_hin_d2 = hin.stride(0), hin.stride(1)\n",
    "    stride_su_l, stride_su_d2, stride_su_k = S1s.stride(0), S1s.stride(1), S1s.stride(2)\n",
    "    stride_out_l, stride_out_bsize, stride_out_k = in1.stride(0), in1.stride(1), in1.stride(2)\n",
    "    \n",
    "    grid = lambda META: (L, triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]) * triton.cdiv(K, META[\"BLOCK_SIZE_K\"]), )\n",
    "    \n",
    "    wrap_triton(first_pass_kernel)[grid](\n",
    "        hin, S1s, U2s, in1, in2,\n",
    "        BSIZE, K, d2, L,\n",
    "        stride_hin_bsize, stride_hin_d2,\n",
    "        stride_su_l, stride_su_d2, stride_su_k,\n",
    "        stride_out_l, stride_out_bsize, stride_out_k,\n",
    "    )\n",
    "\n",
    "    in1 = in1.contiguous()\n",
    "    in2 = in2.contiguous()\n",
    "\n",
    "    # second pass\n",
    "    #############\n",
    "    bias_unsqueezed = bias.unsqueeze(0)\n",
    "    L, BSIZE, K = in1.shape\n",
    "    _, _, d1 = U1s.shape\n",
    "    \n",
    "    out = torch.empty((BSIZE, d1), dtype=torch.float32, device=device)\n",
    "\n",
    "    stride_in12_l, stride_in12_bsize, stride_in12_k = in1.stride(0), in1.stride(1), in1.stride(2)\n",
    "    stride_us_l, stride_us_k, stride_us_d1 = U1s.stride(0), U1s.stride(1), U1s.stride(2)\n",
    "    stride_bias_bsize, stride_bias_d1 = bias_unsqueezed.stride(0), bias_unsqueezed.stride(1)\n",
    "    stride_out_bsize, stride_out_d1 = out.stride(0), out.stride(1)\n",
    "    \n",
    "    grid = lambda META: (triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]) * triton.cdiv(d1, META[\"BLOCK_SIZE_D1\"]), )\n",
    "    \n",
    "    wrap_triton(second_pass_kernel)[grid](\n",
    "        in1, in2, U1s, S2s, bias_unsqueezed, out,\n",
    "        BSIZE, d1, K, L,\n",
    "        stride_in12_l, stride_in12_bsize, stride_in12_k,\n",
    "        stride_us_l, stride_us_k, stride_us_d1,\n",
    "        stride_bias_bsize, stride_bias_d1,\n",
    "        stride_out_bsize, stride_out_d1,\n",
    "    )\n",
    "    \n",
    "    return out.contiguous()\n",
    "\n",
    "@forward_op.register_kernel(\"cpu\")\n",
    "def _(input, S1s, S2s, U1s, U2s, bias):\n",
    "    num_terms = S2s.shape[0]\n",
    "    # Efficiently perform the sum over all l terms\n",
    "    input = input.unsqueeze(0).expand(num_terms, input.shape[0], input.shape[1])\n",
    "    return (\n",
    "        ((input.bmm(S1s)).bmm(U1s)).mean(0) / 2\n",
    "        + ((input.bmm(U2s)).bmm(S2s)).mean(0) / 2\n",
    "        + bias\n",
    "    )\n",
    "    \n",
    "@triton_op(\"panther::backward_op\", mutates_args={})\n",
    "def backward_op(hin: torch.Tensor, S1s: torch.Tensor, S2s: torch.Tensor, U1s: torch.Tensor, U2s: torch.Tensor, g: torch.Tensor) -> List[torch.Tensor]:\n",
    "    device = 'cuda'\n",
    "    num_terms = S2s.shape[0]\n",
    "        \n",
    "    # Transpose and make contiguous\n",
    "    hin = hin.transpose(0, 1).contiguous()\n",
    "    U1s = U1s.transpose(1, 2).contiguous()\n",
    "    S1s = S1s.transpose(1, 2).contiguous()\n",
    "    U2s = U2s.transpose(1, 2).contiguous()\n",
    "    S2s = S2s.transpose(1, 2).contiguous()\n",
    "\n",
    "    g = g.contiguous() / (2 * num_terms)\n",
    "    \n",
    "    # first_pass_gU1s_g_S2s\n",
    "    #######################\n",
    "    BSIZE, d1 = g.shape\n",
    "    L, _, K = U1s.shape\n",
    "\n",
    "    # TO UNREMOVE:\n",
    "    g_U1s = torch.empty((L, BSIZE, K), dtype=torch.float32, device='cuda')\n",
    "    g_S2s = torch.empty((L, BSIZE, K), dtype=torch.float32, device='cuda')\n",
    "\n",
    "    # TO REMOVE:\n",
    "    # g_U1s = torch.rand((L, BSIZE, K), dtype=torch.float32, device='cuda')\n",
    "    # g_S2s = torch.rand((L, BSIZE, K), dtype=torch.float32, device='cuda')\n",
    "\n",
    "    stride_g_bsize, stride_g_d1 = g.stride(0), g.stride(1)\n",
    "    stride_su_l, stride_su_d1, stride_su_k = U1s.stride(0), U1s.stride(1), U1s.stride(2)\n",
    "    stride_out_l, stride_out_bsize, stride_out_k = g_U1s.stride(0), g_U1s.stride(1), g_U1s.stride(2)\n",
    "    \n",
    "    grid = lambda META: (L, triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]) * triton.cdiv(K, META[\"BLOCK_SIZE_K\"]), )\n",
    "    \n",
    "    wrap_triton(first_pass_gU1s_g_S2s_kernel)[grid](\n",
    "        g, U1s, S2s, g_U1s, g_S2s,\n",
    "        BSIZE, K, d1, L,\n",
    "        stride_g_bsize, stride_g_d1,\n",
    "        stride_su_l, stride_su_d1, stride_su_k,\n",
    "        stride_out_l, stride_out_bsize, stride_out_k\n",
    "    )\n",
    "\n",
    "    g_U1s = g_U1s.contiguous()\n",
    "    g_S2s = g_S2s.contiguous()\n",
    "\n",
    "    # second_pass_gUS11_22\n",
    "    #######################\n",
    "    L, BSIZE, K = g_U1s.shape\n",
    "    _, _, d2 = S1s.shape\n",
    "    \n",
    "    grad = torch.empty((BSIZE, d2), dtype=torch.float32, device='cuda')\n",
    "\n",
    "    stride_g_U1s2_l, stride_g_U1s2_bsize, stride_g_U1s2_k = g_U1s.stride(0), g_U1s.stride(1), g_U1s.stride(2)\n",
    "    stride_us_l, stride_us_k, stride_us_d2 = S1s.stride(0), S1s.stride(1), S1s.stride(2)\n",
    "    stride_out_bsize, stride_out_d2 = grad.stride(0), grad.stride(1)\n",
    "    \n",
    "    grid = lambda META: (triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]) * triton.cdiv(d2, META[\"BLOCK_SIZE_d2\"]), )\n",
    "    \n",
    "    wrap_triton(second_pass_gUS11_22_kernel)[grid](\n",
    "        g_U1s, g_S2s, S1s, U2s, grad,\n",
    "        BSIZE, d2, K, L,\n",
    "        stride_g_U1s2_l, stride_g_U1s2_bsize, stride_g_U1s2_k,\n",
    "        stride_us_l, stride_us_k, stride_us_d2,\n",
    "        stride_out_bsize, stride_out_d2,\n",
    "    )\n",
    "\n",
    "    grad = grad.contiguous()\n",
    "\n",
    "    # calc_grad_S1s\n",
    "    ################\n",
    "    d2, BSIZE = hin.shape\n",
    "    L, _, k = g_U1s.shape\n",
    "    \n",
    "    grad_S1s = torch.empty((L, d2, k), dtype=torch.float32, device=device)\n",
    "\n",
    "    stride_hin_bsize, stride_hin_BSIZE = hin.stride(0), hin.stride(1)\n",
    "    stride_su_l, stride_su_BSIZE, stride_su_k = g_U1s.stride(0), g_U1s.stride(1), g_U1s.stride(2)\n",
    "    stride_out_l, stride_out_bsize, stride_out_k = grad_S1s.stride(0), grad_S1s.stride(1), grad_S1s.stride(2)\n",
    "    \n",
    "    grid = lambda META: (L, triton.cdiv(d2, META[\"BLOCK_SIZE_d2\"]) * triton.cdiv(k, META[\"BLOCK_SIZE_k\"]), )\n",
    "    \n",
    "    wrap_triton(calc_grad_S1s_kernel)[grid](\n",
    "        hin, g_U1s, grad_S1s,\n",
    "        d2, k, BSIZE, L,\n",
    "        stride_hin_bsize, stride_hin_BSIZE,\n",
    "        stride_su_l, stride_su_BSIZE, stride_su_k,\n",
    "        stride_out_l, stride_out_bsize, stride_out_k\n",
    "    )\n",
    "\n",
    "    grad_S1s = grad_S1s.contiguous()\n",
    "    \n",
    "    # first_pass_U2s_hin\n",
    "    ####################\n",
    "    L, K, d2 = U2s.shape\n",
    "    _, BSIZE = hin.shape\n",
    "\n",
    "    # TO UNREMOVE:\n",
    "    U2s_hin = torch.empty((L, K, BSIZE), dtype=torch.float32, device=device)\n",
    "    # TO REMOVE:\n",
    "    # U2s_hin = torch.randn((L, K, BSIZE), dtype=torch.float32, device=device)\n",
    "\n",
    "    stride_hin_d2, stride_hin_BSIZE = hin.stride(0), hin.stride(1)\n",
    "    stride_su_l, stride_su_K, stride_su_d2 = U2s.stride(0), U2s.stride(1), U2s.stride(2)\n",
    "    stride_out_l, stride_out_K, stride_out_BSIZE = U2s_hin.stride(0), U2s_hin.stride(1), U2s_hin.stride(2)\n",
    "    \n",
    "    grid = lambda META: (L, triton.cdiv(K, META[\"BLOCK_SIZE_K\"]) * triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]), )\n",
    "    \n",
    "    wrap_triton(first_pass_U2s_hin_kernel)[grid](\n",
    "        hin, U2s, U2s_hin,\n",
    "        K, d2, BSIZE, L,\n",
    "        stride_hin_d2, stride_hin_BSIZE,\n",
    "        stride_su_l, stride_su_K, stride_su_d2,\n",
    "        stride_out_l, stride_out_K, stride_out_BSIZE\n",
    "    )\n",
    "\n",
    "    U2s_hin = U2s_hin.contiguous()\n",
    "    \n",
    "    # calc_grad_S2s\n",
    "    ###############\n",
    "    L, K, BSIZE = U2s_hin.shape\n",
    "    _, d1 = g.shape\n",
    "    \n",
    "    grad_S2s = torch.empty((L, K, d1), dtype=torch.float32, device=device)\n",
    "\n",
    "    stride_g_BSIZE, stride_g_d1 = g.stride(0), g.stride(1)\n",
    "    stride_su_l, stride_su_K, stride_su_BSIZE = U2s_hin.stride(0), U2s_hin.stride(1), U2s_hin.stride(2)\n",
    "    stride_out_l, stride_out_K, stride_out_d1 = grad_S2s.stride(0), grad_S2s.stride(1), grad_S2s.stride(2)\n",
    "    \n",
    "    grid = lambda META: (L, triton.cdiv(K, META[\"BLOCK_SIZE_K\"]) * triton.cdiv(d1, META[\"BLOCK_SIZE_d1\"]), )\n",
    "    \n",
    "    wrap_triton(calc_grad_S2s_kernel)[grid](\n",
    "        g, U2s_hin, grad_S2s,\n",
    "        K, BSIZE, d1, L,\n",
    "        stride_g_BSIZE, stride_g_d1,\n",
    "        stride_su_l, stride_su_K, stride_su_BSIZE,\n",
    "        stride_out_l, stride_out_K, stride_out_d1\n",
    "    )\n",
    "\n",
    "    grad_S2s = grad_S2s.contiguous()\n",
    "\n",
    "    return [\n",
    "        grad,\n",
    "        grad_S1s,\n",
    "        grad_S2s,\n",
    "        g.sum(0)\n",
    "    ]\n",
    "    \n",
    "@backward_op.register_kernel(\"cpu\")\n",
    "def _(input, S1s, S2s, U1s, U2s, grad_output):\n",
    "    num_terms = S2s.shape[0]\n",
    "    g = grad_output / (2 * num_terms)\n",
    "    g = g.unsqueeze(0).expand(num_terms, g.shape[0], g.shape[1])\n",
    "    input = (\n",
    "        input.unsqueeze(0)\n",
    "        .expand(num_terms, input.shape[0], input.shape[1])\n",
    "        .transpose(1, 2)\n",
    "    )\n",
    "    U1s = U1s.transpose(1, 2)\n",
    "    S1s = S1s.transpose(1, 2)\n",
    "    U2s = U2s.transpose(1, 2)\n",
    "    S2s = S2s.transpose(1, 2)\n",
    "    t1 = g.bmm(U1s)\n",
    "    grad = t1.bmm(S1s).sum(0) + g.bmm(S2s).bmm(U2s).sum(0)\n",
    "    grad_S2s = (U2s.bmm(input)).bmm(g)\n",
    "    grad_S1s = input.bmm(g.bmm(U1s))\n",
    "\n",
    "    g = g[0]\n",
    "    return [\n",
    "        grad,\n",
    "        grad_S1s,\n",
    "        grad_S2s,\n",
    "        # sum g on batch dimension input.shape[0]\n",
    "        g.reshape(input.shape[2], -1).sum(0)\n",
    "    ]\n",
    "    \n",
    "class SketchedLinearFunction_triton(Function):\n",
    "    # Note that forward, setup_context, and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        input: torch.Tensor,\n",
    "        S1s: torch.Tensor,\n",
    "        S2s: torch.Tensor,\n",
    "        U1s: torch.Tensor,\n",
    "        U2s: torch.Tensor,\n",
    "        bias: torch.Tensor,\n",
    "    ): \n",
    "        return forward_op(input, S1s, S2s, U1s, U2s, bias)\n",
    "\n",
    "    @staticmethod\n",
    "    # inputs is a Tuple of all of the inputs passed to forward.\n",
    "    # output is the output of the forward().\n",
    "    def setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any):\n",
    "        input, S1s, S2s, U1s, U2s, bias = inputs\n",
    "        ctx.save_for_backward(input, S1s, S2s, U1s, U2s, bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, *grad_output: Any) -> Any:\n",
    "        # dl/dS2_i = U1_i g h_in^T / 2 * l\n",
    "        # dl/dS1_i = g h_in^T U2_i^T / 2 * l\n",
    "        # dl/dh_in = 1/(2*l) * (sum_{i=1}^{l} (S1_i^T U1_i g) + sum_{i=1}^{l} (U2_i^T S2_i g))\n",
    "        # dl/db = g\n",
    "        hin, S1s, S2s, U1s, U2s, _ = ctx.saved_tensors\n",
    "        grad_input, grad_S1s, grad_S2s, grad_bias = backward_op(hin, S1s, S2s, U1s, U2s, grad_output[0])\n",
    "        return grad_input, grad_S1s, grad_S2s, None, None, grad_bias\n",
    "        \n",
    "\n",
    "\n",
    "class SKLinear_triton(nn.Module):\n",
    "    __constants__ = [\"in_features\", \"out_features\", \"num_terms\", \"low_rank\"]\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    num_terms: int\n",
    "    low_rank: int\n",
    "    S1s: torch.Tensor\n",
    "    S2s: torch.Tensor\n",
    "    U1s: torch.Tensor\n",
    "    U2s: torch.Tensor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        num_terms: int,\n",
    "        low_rank: int,\n",
    "        W_init=None,\n",
    "        bias: bool = True,\n",
    "        dtype=None,\n",
    "        device=None,\n",
    "    ):\n",
    "        factory_kwargs = {\"dtype\": dtype, \"device\": device}\n",
    "        super(SKLinear_triton, self).__init__()\n",
    "\n",
    "        # if (\n",
    "        #     2 * num_terms * low_rank * (out_features + in_features)\n",
    "        #     > out_features * in_features\n",
    "        # ):\n",
    "        #     raise ValueError(\n",
    "        #         \"The number of parameters in the sketching layer is larger \"\n",
    "        #         + \"than the number of parameters in the fully connected layer.\"\n",
    "        #     )\n",
    "\n",
    "        self.num_terms = num_terms # l\n",
    "        self.low_rank = low_rank # k\n",
    "        self.out_features = out_features\n",
    "        self.in_features = in_features\n",
    "\n",
    "        # Register U1s and U2s as buffers since they are not learnable\n",
    "        self.register_buffer(\n",
    "            \"U1s\",\n",
    "            torch.stack(\n",
    "                [\n",
    "                    gen_U(low_rank, out_features, **factory_kwargs)\n",
    "                    for _ in range(num_terms)\n",
    "                ]\n",
    "            ),\n",
    "        )  # k(low rank)xd1(out) stacked along the zeros dimension (l) -> l x k x d1\n",
    "        self.register_buffer(\n",
    "            \"U2s\",\n",
    "            torch.stack(\n",
    "                [\n",
    "                    gen_U(in_features, low_rank, **factory_kwargs)\n",
    "                    for _ in range(num_terms)\n",
    "                ]\n",
    "            ),\n",
    "        )  # d2xk stacked along the zeros dimension (l) -> l x d2 x k\n",
    "\n",
    "        # W is used to only initialize S\n",
    "        if W_init is None:\n",
    "            W = torch.empty(in_features, out_features, **factory_kwargs) # d2 * d1\n",
    "            init.kaiming_uniform_(W, a=math.sqrt(5))\n",
    "        else:\n",
    "            W = W_init.T.detach().clone()\n",
    "\n",
    "        # S1s and S2s are precomputed but not updated in the backward pass\n",
    "        self.S1s = nn.Parameter(\n",
    "            torch.stack([torch.matmul(W, self.U1s[i].T) for i in range(num_terms)])\n",
    "        )  # d2xk stacked along the zeros dimension (l) -> l x d2 x k\n",
    "        self.S2s = nn.Parameter(\n",
    "            torch.stack([torch.matmul(self.U2s[i].T, W) for i in range(num_terms)])\n",
    "        )  # kxd1 stacked along the zeros dimension (l) -> l x k x d1\n",
    "\n",
    "        # Bias term initialized with a small standard deviation\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features, **factory_kwargs)) #1 * d1\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(W)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "    def forward(self, h_in):\n",
    "        # TODO: Make sure all the things are contiguos\n",
    "        return SketchedLinearFunction_triton.apply(\n",
    "            h_in.contiguous(), self.S1s.contiguous(), self.S2s.contiguous(), self.U1s.contiguous(), self.U2s.contiguous(), self.bias.contiguous()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65b3fa",
   "metadata": {
    "papermill": {
     "duration": 0.044718,
     "end_time": "2025-05-05T00:14:34.719280",
     "exception": false,
     "start_time": "2025-05-05T00:14:34.674562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# checking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d8b6622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:14:34.809122Z",
     "iopub.status.busy": "2025-05-05T00:14:34.808821Z",
     "iopub.status.idle": "2025-05-05T00:14:34.815532Z",
     "shell.execute_reply": "2025-05-05T00:14:34.815007Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.053372,
     "end_time": "2025-05-05T00:14:34.816707",
     "exception": false,
     "start_time": "2025-05-05T00:14:34.763335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from panther.nn.linear import SKLinear\n",
    "# from panther.nn.linear_tr import SKLinear_triton\n",
    "\n",
    "# def compare_linear_implementations(batch_size=64, in_features=128, out_features=64, num_terms=4, low_rank=8, raise_error=True):\n",
    "#     """\n",
    "#     Compare the outputs of the forward pass and gradients between regular SKLinear and Triton-accelerated SKLinear_triton.\n",
    "\n",
    "#     Args:\n",
    "#         batch_size: Batch size for the input tensor\n",
    "#         in_features: Number of input features\n",
    "#         out_features: Number of output features\n",
    "#         num_terms: Number of terms (l) in the sketched linear layer\n",
    "#         low_rank: The low-rank dimension (k) in the sketched linear layer\n",
    "#         raise_error: If True, raise AssertionError when comparisons fail\n",
    "\n",
    "#     Returns:\n",
    "#         bool: True if all comparisons pass, False otherwise\n",
    "\n",
    "#     Raises:\n",
    "#         AssertionError: If any comparison fails and raise_error is True\n",
    "#     """\n",
    "#     # Set seed for reproducibility\n",
    "#     torch.manual_seed(42)\n",
    "\n",
    "#     # Create identical layers\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     # Generate the same initialization for both layers\n",
    "#     W_init = torch.randn(out_features, in_features, device=device)\n",
    "\n",
    "#     # Create both layer implementations with identical parameters\n",
    "#     sk_linear = SKLinear(in_features, out_features, num_terms, low_rank, W_init=W_init, device=device)\n",
    "#     sk_linear_triton = SKLinear_triton(in_features, out_features, num_terms, low_rank, W_init=W_init, device=device)\n",
    "\n",
    "#     # Ensure parameters match exactly by copying from sk_linear to sk_linear_triton\n",
    "#     with torch.no_grad():\n",
    "#         sk_linear_triton.S1s.copy_(sk_linear.S1s)\n",
    "#         sk_linear_triton.S2s.copy_(sk_linear.S2s)\n",
    "#         sk_linear_triton.U1s.copy_(sk_linear.U1s)\n",
    "#         sk_linear_triton.U2s.copy_(sk_linear.U2s)\n",
    "#         sk_linear_triton.bias.copy_(sk_linear.bias)\n",
    "\n",
    "#     # Create identical input\n",
    "#     input_data = torch.randn(batch_size, in_features, device=device, requires_grad=True)\n",
    "#     input_data_triton = input_data.clone().detach().requires_grad_(True)\n",
    "\n",
    "#     # Forward pass\n",
    "#     output = sk_linear(input_data)\n",
    "#     output_triton = sk_linear_triton(input_data_triton)\n",
    "\n",
    "#     # Check if forward outputs match\n",
    "#     forward_match = torch.allclose(output, output_triton, rtol=1e-4, atol=1e-5)\n",
    "#     print(f\"Forward outputs match: {forward_match}\")\n",
    "#     if not forward_match:\n",
    "#         max_diff = torch.max(torch.abs(output - output_triton))\n",
    "#         error_msg = f\"Forward outputs don't match. Max difference: {max_diff.item()}\"\n",
    "#         print(error_msg)\n",
    "#         if raise_error:\n",
    "#             assert forward_match, error_msg\n",
    "\n",
    "#     # Create identical gradients\n",
    "#     grad_output = torch.randn_like(output)\n",
    "#     grad_output_triton = grad_output.clone()\n",
    "\n",
    "#     # Backward pass\n",
    "#     output.backward(grad_output)\n",
    "#     output_triton.backward(grad_output_triton)\n",
    "\n",
    "#     # Check if input gradients match\n",
    "#     input_grad_match = torch.allclose(input_data.grad, input_data_triton.grad, rtol=1e-4, atol=1e-5)\n",
    "#     print(f\"Input gradients match: {input_grad_match}\")\n",
    "#     if not input_grad_match:\n",
    "#         max_diff = torch.max(torch.abs(input_data.grad - input_data_triton.grad))\n",
    "#         error_msg = f\"Input gradients don't match. Max difference: {max_diff.item()}\"\n",
    "#         print(error_msg)\n",
    "#         if raise_error:\n",
    "#             assert input_grad_match, error_msg\n",
    "\n",
    "#     # Check if S1s gradients match\n",
    "#     S1s_grad_match = torch.allclose(sk_linear.S1s.grad, sk_linear_triton.S1s.grad, rtol=1e-4, atol=1e-5)\n",
    "#     print(f\"S1s gradients match: {S1s_grad_match}\")\n",
    "#     if not S1s_grad_match:\n",
    "#         max_diff = torch.max(torch.abs(sk_linear.S1s.grad - sk_linear_triton.S1s.grad))\n",
    "#         error_msg = f\"S1s gradients don't match. Max difference: {max_diff.item()}\"\n",
    "#         print(error_msg)\n",
    "#         if raise_error:\n",
    "#             assert S1s_grad_match, error_msg\n",
    "\n",
    "#     # Check if S2s gradients match\n",
    "#     S2s_grad_match = torch.allclose(sk_linear.S2s.grad, sk_linear_triton.S2s.grad, rtol=1e-4, atol=1e-5)\n",
    "#     print(f\"S2s gradients match: {S2s_grad_match}\")\n",
    "#     if not S2s_grad_match:\n",
    "#         max_diff = torch.max(torch.abs(sk_linear.S2s.grad - sk_linear_triton.S2s.grad))\n",
    "#         error_msg = f\"S2s gradients don't match. Max difference: {max_diff.item()}\"\n",
    "#         print(error_msg)\n",
    "#         if raise_error:\n",
    "#             assert S2s_grad_match, error_msg\n",
    "\n",
    "#     # U1s and U2s are buffers, not parameters, so they may not have gradients\n",
    "#     # Only check if both gradients exist\n",
    "#     if hasattr(sk_linear.U1s, 'grad') and hasattr(sk_linear_triton.U1s, 'grad') and sk_linear.U1s.grad is not None and sk_linear_triton.U1s.grad is not None:\n",
    "#         U1s_grad_match = torch.allclose(sk_linear.U1s.grad, sk_linear_triton.U1s.grad, rtol=1e-4, atol=1e-5)\n",
    "#         print(f\"U1s gradients match: {U1s_grad_match}\")\n",
    "#         if not U1s_grad_match:\n",
    "#             max_diff = torch.max(torch.abs(sk_linear.U1s.grad - sk_linear_triton.U1s.grad))\n",
    "#             error_msg = f\"U1s gradients don't match. Max difference: {max_diff.item()}\"\n",
    "#             print(error_msg)\n",
    "#             if raise_error:\n",
    "#                 assert U1s_grad_match, error_msg\n",
    "#     else:\n",
    "#         print(\"U1s gradients not available - skipping comparison\")\n",
    "#         U1s_grad_match = True  # Skip this comparison\n",
    "\n",
    "#     # Check if U2s gradients match - only if both exist\n",
    "#     if hasattr(sk_linear.U2s, 'grad') and hasattr(sk_linear_triton.U2s, 'grad') and sk_linear.U2s.grad is not None and sk_linear_triton.U2s.grad is not None:\n",
    "#         U2s_grad_match = torch.allclose(sk_linear.U2s.grad, sk_linear_triton.U2s.grad, rtol=1e-4, atol=1e-5)\n",
    "#         print(f\"U2s gradients match: {U2s_grad_match}\")\n",
    "#         if not U2s_grad_match:\n",
    "#             max_diff = torch.max(torch.abs(sk_linear.U2s.grad - sk_linear_triton.U2s.grad))\n",
    "#             error_msg = f\"U2s gradients don't match. Max difference: {max_diff.item()}\"\n",
    "#             print(error_msg)\n",
    "#             if raise_error:\n",
    "#                 assert U2s_grad_match, error_msg\n",
    "#     else:\n",
    "#         print(\"U2s gradients not available - skipping comparison\")\n",
    "#         U2s_grad_match = True  # Skip this comparison\n",
    "\n",
    "#     # Check if bias gradients match\n",
    "#     bias_grad_match = torch.allclose(sk_linear.bias.grad, sk_linear_triton.bias.grad, rtol=1e-4, atol=1e-5)\n",
    "#     print(f\"Bias gradients match: {bias_grad_match}\")\n",
    "#     if not bias_grad_match:\n",
    "#         max_diff = torch.max(torch.abs(sk_linear.bias.grad - sk_linear_triton.bias.grad))\n",
    "#         error_msg = f\"Bias gradients don't match. Max difference: {max_diff.item()}\"\n",
    "#         print(error_msg)\n",
    "#         if raise_error:\n",
    "#             assert bias_grad_match, error_msg\n",
    "\n",
    "#     # Return overall result\n",
    "#     all_matches = all([forward_match, input_grad_match, S1s_grad_match, S2s_grad_match, U1s_grad_match, U2s_grad_match, bias_grad_match])\n",
    "#     if raise_error:\n",
    "#         assert all_matches, \"At least one comparison failed. See detailed errors above.\"\n",
    "#     return all_matches\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Comparing SKLinear and SKLinear_triton implementations...\")\n",
    "\n",
    "#     # Test with default parameters\n",
    "#     try:\n",
    "#         result = compare_linear_implementations()\n",
    "#         print(f\"All checks passed: {result}\")\n",
    "#     except AssertionError as e:\n",
    "#         print(f\"Default test failed: {e}\")\n",
    "\n",
    "#     # Test with different parameters\n",
    "#     try:\n",
    "#         result_small = compare_linear_implementations(batch_size=32, in_features=64, out_features=32, num_terms=2, low_rank=4)\n",
    "#         print(f\"Small model checks passed: {result_small}\")\n",
    "#     except AssertionError as e:\n",
    "#         print(f\"Small model test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47097683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:14:34.908629Z",
     "iopub.status.busy": "2025-05-05T00:14:34.908318Z",
     "iopub.status.idle": "2025-05-05T00:14:34.917457Z",
     "shell.execute_reply": "2025-05-05T00:14:34.916749Z"
    },
    "papermill": {
     "duration": 0.057421,
     "end_time": "2025-05-05T00:14:34.918719",
     "exception": false,
     "start_time": "2025-05-05T00:14:34.861298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/panther/tests/run.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/panther/tests/run.py\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch._dynamo\n",
    "import torch._inductor.config as config\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from panther.nn import SKLinear\n",
    "\n",
    "# Configure torch\n",
    "config.max_autotune_gemm = False\n",
    "torch._dynamo.config.cache_size_limit = 2**16\n",
    "torch._dynamo.config.accumulated_cache_size_limit = 2**16\n",
    "\n",
    "def is_valid_params(in_features, out_features, num_terms, low_rank):\n",
    "    """\n",
    "    Check if parameter combination is valid:\n",
    "    A combination is invalid if 2 * num_terms * low_rank * (out_features + in_features) >= out_features * in_features\n",
    "    """\n",
    "    return 2 * num_terms * low_rank * (out_features + in_features) < out_features * in_features\n",
    "\n",
    "class BenchmarkParams:\n",
    "    def __init__(self,\n",
    "                 in_features=256, \n",
    "                 out_features=256,\n",
    "                 num_terms=3,\n",
    "                 low_rank=8,\n",
    "                 batch_size=64, \n",
    "                 num_runs=200,\n",
    "                 warmup=15,\n",
    "                 device='cuda',\n",
    "                 dtype=torch.float32):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_terms = num_terms\n",
    "        self.low_rank = low_rank\n",
    "        self.batch_size = batch_size\n",
    "        self.num_runs = num_runs\n",
    "        self.warmup = warmup\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "def benchmark_model(model, x, model_name, params):\n",
    "    """\n",
    "    Generic benchmarking function for any PyTorch model.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to benchmark\n",
    "        x: Input tensor\n",
    "        model_name: Name of the model for logging\n",
    "        params: Benchmark parameters\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with benchmark results\n",
    "    """\n",
    "    # Compile the model\n",
    "    model_compiled = torch.compile(\n",
    "        model,\n",
    "        backend=\"inductor\",\n",
    "        fullgraph=True,\n",
    "        dynamic=False\n",
    "    )\n",
    "    \n",
    "    # Benchmark forward pass\n",
    "    print(f\"\\n=== {model_name} FORWARD PASS BENCHMARK ===\")\n",
    "    \n",
    "    # Warmup runs for forward pass\n",
    "    # model_compiled.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for _ in range(params.warmup):\n",
    "    #         _ = model_compiled(x)\n",
    "    \n",
    "    # torch.cuda.synchronize()\n",
    "    \n",
    "    # Actual timed runs for forward\n",
    "    forward_times = []\n",
    "    # with torch.no_grad():\n",
    "    #     for _ in range(params.num_runs):\n",
    "    #         torch.cuda.synchronize()\n",
    "    #         start = time.perf_counter()\n",
    "    #         _ = model_compiled(x)\n",
    "    #         torch.cuda.synchronize()\n",
    "    #         end = time.perf_counter()\n",
    "            \n",
    "    #         forward_times.append((end - start) * 1000)  # Convert to ms\n",
    "    \n",
    "    # mean_forward = np.mean(forward_times)\n",
    "    # std_forward = np.std(forward_times)\n",
    "    mean_forward = 0\n",
    "    std_forward = 0\n",
    "    print(f\"{model_name} forward: {mean_forward:.3f} ± {std_forward:.3f} ms\")\n",
    "    \n",
    "    # Benchmark backward pass\n",
    "    # print(f\"\\n=== {model_name} BACKWARD PASS BENCHMARK ===\")\n",
    "    \n",
    "    # # Warmup runs for backward pass\n",
    "    model_compiled.train()\n",
    "    for _ in range(params.warmup):\n",
    "        out = model_compiled(x)\n",
    "        loss = out.sum()\n",
    "        loss.backward()\n",
    "        x.grad.zero_()\n",
    "    \n",
    "    # torch.cuda.synchronize()\n",
    "    \n",
    "    # # Actual timed runs for backward\n",
    "    backward_times = []\n",
    "    for _ in range(params.num_runs):\n",
    "        out = model_compiled(x)\n",
    "        loss = out.sum()\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        loss.backward()\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        backward_times.append((end - start) * 1000)  # Convert to ms\n",
    "        x.grad.zero_()\n",
    "    \n",
    "    mean_backward = np.mean(backward_times)\n",
    "    std_backward = np.std(backward_times)\n",
    "    print(f\"{model_name} backward: {mean_backward:.3f} ± {std_backward:.3f} ms\")\n",
    "    \n",
    "    return {\n",
    "        \"forward\": {\n",
    "            \"mean\": mean_forward,\n",
    "            \"std\": std_forward,\n",
    "            \"times\": forward_times\n",
    "        },\n",
    "        \"backward\": {\n",
    "            \"mean\": mean_backward,\n",
    "            \"std\": std_backward,\n",
    "            \"times\": backward_times\n",
    "        }\n",
    "    }\n",
    "\n",
    "def benchmark_model_factory(model_factory, model_name, params):\n",
    "    """\n",
    "    Benchmark a model using a factory function.\n",
    "    \n",
    "    Args:\n",
    "        model_factory: Function that creates the model\n",
    "        model_name: Name of the model for logging\n",
    "        params: Benchmark parameters\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with benchmark results\n",
    "    """\n",
    "    # Create the model\n",
    "    torch.manual_seed(42)\n",
    "    model = model_factory(params)\n",
    "    \n",
    "    # Create input tensor for benchmarking\n",
    "    x = torch.randn(params.batch_size, params.in_features, \n",
    "                  dtype=params.dtype, device=params.device, requires_grad=True)\n",
    "    \n",
    "    return benchmark_model(model, x, model_name, params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import torch.nn as nn\n",
    "    from panther.nn import SKLinear, SKLinear_triton\n",
    "    \n",
    "    # Parameter combinations to test\n",
    "    ratios = [(1, 128), (128, 1), (1, 1), (2, 1), (1, 2)]\n",
    "    base_sizes = [256, 512, 1024, 8192, 16384]\n",
    "    num_terms_options = [1, 2, 3]\n",
    "    low_rank_options = [16, 32, 64, 128]\n",
    "    \n",
    "    # Define model factories\n",
    "    def create_sklinear_triton(p):\n",
    "        return SKLinear_triton(p.in_features, p.out_features, \n",
    "                             p.num_terms, p.low_rank, \n",
    "                             dtype=p.dtype, device=p.device)\n",
    "    \n",
    "    models_to_benchmark = [\n",
    "        (create_sklinear_triton, \"SKLinear_triton\")\n",
    "    ]\n",
    "    \n",
    "    # Prepare data structure to store all results\n",
    "    results_data = []\n",
    "    \n",
    "    # Iterate through all parameter combinations\n",
    "    total_combinations = len(ratios) * len(base_sizes) * len(num_terms_options) * len(low_rank_options)\n",
    "    current_combo = 0\n",
    "    \n",
    "    for ratio, base_size in itertools.product(ratios, base_sizes):\n",
    "        ratio_in, ratio_out = ratio\n",
    "        \n",
    "        # Calculate actual dimensions based on ratio and base size\n",
    "        if ratio_in == 1:\n",
    "            in_features = base_size\n",
    "            out_features = base_size * ratio_out\n",
    "        else:\n",
    "            out_features = base_size\n",
    "            in_features = base_size * ratio_in\n",
    "        \n",
    "        for num_terms, low_rank in itertools.product(num_terms_options, low_rank_options):\n",
    "            current_combo += 1\n",
    "            print(f\"\\n\\n{'='*20} COMBINATION {current_combo}/{total_combinations} {'='*20}\")\n",
    "            print(f\"In features: {in_features}, Out features: {out_features}, Ratio: {ratio_in}:{ratio_out}\")\n",
    "            print(f\"Base size: {base_size}, Num terms: {num_terms}, Low rank: {low_rank}\")\n",
    "            \n",
    "            # Check if parameters are valid\n",
    "            is_valid = is_valid_params(in_features, out_features, num_terms, low_rank)\n",
    "            \n",
    "            if not is_valid:\n",
    "                print(f\"INVALID COMBINATION: 2 * {num_terms} * {low_rank} * ({out_features} + {in_features}) >= {out_features} * {in_features}\")\n",
    "                print(\"Skipping benchmarks for this invalid combination\")\n",
    "                \n",
    "                # Add invalid entry to results data\n",
    "                for model_name in [m[1] for m in models_to_benchmark]:\n",
    "                    results_data.append({\n",
    "                        'model': model_name,\n",
    "                        'in_features': in_features,\n",
    "                        'out_features': out_features,\n",
    "                        'ratio': f\"{ratio_in}:{ratio_out}\",\n",
    "                        'base_size': base_size,\n",
    "                        'num_terms': num_terms,\n",
    "                        'low_rank': low_rank,\n",
    "                        'forward_mean_ms': float('nan'),\n",
    "                        'forward_std_ms': float('nan'),\n",
    "                        'backward_mean_ms': float('nan'),\n",
    "                        'backward_std_ms': float('nan'),\n",
    "                        'is_valid': False,\n",
    "                        'error': \"Invalid parameter combination\"\n",
    "                    })\n",
    "                continue\n",
    "            \n",
    "            # Create parameter object for this combination\n",
    "            params = BenchmarkParams(\n",
    "                in_features=in_features,\n",
    "                out_features=out_features,\n",
    "                num_terms=num_terms,\n",
    "                low_rank=low_rank\n",
    "            )\n",
    "            \n",
    "            all_results = {}\n",
    "            for model_factory, model_name in models_to_benchmark:\n",
    "                print(f\"\\n{'='*20} Benchmarking {model_name} {'='*20}\")\n",
    "                try:\n",
    "                    results = benchmark_model_factory(model_factory, model_name, params)\n",
    "                    all_results[model_name] = results\n",
    "                    \n",
    "                    # Add result to our data collection\n",
    "                    results_data.append({\n",
    "                        'model': model_name,\n",
    "                        'in_features': in_features,\n",
    "                        'out_features': out_features,\n",
    "                        'ratio': f\"{ratio_in}:{ratio_out}\",\n",
    "                        'base_size': base_size,\n",
    "                        'num_terms': num_terms,\n",
    "                        'low_rank': low_rank,\n",
    "                        'forward_mean_ms': results['forward']['mean'],\n",
    "                        'forward_std_ms': results['forward']['std'],\n",
    "                        'backward_mean_ms': results['backward']['mean'],\n",
    "                        'backward_std_ms': results['backward']['std'],\n",
    "                        'is_valid': True\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error benchmarking {model_name}: {e}\")\n",
    "                    # Add error entry to data\n",
    "                    results_data.append({\n",
    "                        'model': model_name,\n",
    "                        'in_features': in_features,\n",
    "                        'out_features': out_features,\n",
    "                        'ratio': f\"{ratio_in}:{ratio_out}\",\n",
    "                        'base_size': base_size,\n",
    "                        'num_terms': num_terms,\n",
    "                        'low_rank': low_rank,\n",
    "                        'forward_mean_ms': float('nan'),\n",
    "                        'forward_std_ms': float('nan'),\n",
    "                        'backward_mean_ms': float('nan'),\n",
    "                        'backward_std_ms': float('nan'),\n",
    "                        'is_valid': True,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "            \n",
    "            # Print comparative summary for this combination\n",
    "            if all_results:\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(f\"{'='*20} SUMMARY FOR CURRENT COMBINATION {'='*20}\")\n",
    "                print(\"=\"*60)\n",
    "                print(f\"{'Model':<20} {'Forward (ms)':<25} {'Backward (ms)':<25}\")\n",
    "                print(\"-\"*60)\n",
    "                \n",
    "                for model_name, results in all_results.items():\n",
    "                    fwd = f\"{results['forward']['mean']:.3f} ± {results['forward']['std']:.3f}\"\n",
    "                    bwd = f\"{results['backward']['mean']:.3f} ± {results['backward']['std']:.3f}\"\n",
    "                    print(f\"{model_name:<20} {fwd:<25} {bwd:<25}\")\n",
    "    \n",
    "    # Create a DataFrame with all results\n",
    "    df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_file = \"benchmark_results.csv\"\n",
    "    df.to_csv(results_file, index=False)\n",
    "    print(f\"\\nAll benchmark results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "941faff5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:14:35.009259Z",
     "iopub.status.busy": "2025-05-05T00:14:35.008579Z",
     "iopub.status.idle": "2025-05-05T00:14:35.012438Z",
     "shell.execute_reply": "2025-05-05T00:14:35.011816Z"
    },
    "papermill": {
     "duration": 0.050493,
     "end_time": "2025-05-05T00:14:35.013707",
     "exception": false,
     "start_time": "2025-05-05T00:14:34.963214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/kaggle/working/panther/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f21d3c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:14:35.104885Z",
     "iopub.status.busy": "2025-05-05T00:14:35.104578Z",
     "iopub.status.idle": "2025-05-05T00:14:35.230305Z",
     "shell.execute_reply": "2025-05-05T00:14:35.229359Z"
    },
    "papermill": {
     "duration": 0.17324,
     "end_time": "2025-05-05T00:14:35.231842",
     "exception": false,
     "start_time": "2025-05-05T00:14:35.058602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/panther\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b1ae0ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:14:35.322625Z",
     "iopub.status.busy": "2025-05-05T00:14:35.321795Z",
     "iopub.status.idle": "2025-05-05T00:14:35.447960Z",
     "shell.execute_reply": "2025-05-05T00:14:35.446948Z"
    },
    "papermill": {
     "duration": 0.173086,
     "end_time": "2025-05-05T00:14:35.449470",
     "exception": false,
     "start_time": "2025-05-05T00:14:35.276384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export TORCH_USE_CUDA_DSA=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89819981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:14:35.542446Z",
     "iopub.status.busy": "2025-05-05T00:14:35.541651Z",
     "iopub.status.idle": "2025-05-05T00:14:35.667766Z",
     "shell.execute_reply": "2025-05-05T00:14:35.666577Z"
    },
    "papermill": {
     "duration": 0.173113,
     "end_time": "2025-05-05T00:14:35.669369",
     "exception": false,
     "start_time": "2025-05-05T00:14:35.496256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaabb976",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:14:35.760759Z",
     "iopub.status.busy": "2025-05-05T00:14:35.760435Z",
     "iopub.status.idle": "2025-05-05T00:51:09.240630Z",
     "shell.execute_reply": "2025-05-05T00:51:09.239736Z"
    },
    "papermill": {
     "duration": 2193.528275,
     "end_time": "2025-05-05T00:51:09.242475",
     "exception": false,
     "start_time": "2025-05-05T00:14:35.714200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "==================== COMBINATION 1/300 ====================\r\n",
      "In features: 256, Out features: 32768, Ratio: 1:128\r\n",
      "Base size: 256, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "\r\n",
      "=== SKLinear_triton FORWARD PASS BENCHMARK ===\r\n",
      "SKLinear_triton forward: 0.000 ± 0.000 ms\r\n",
      "SKLinear_triton backward: 2.245 ± 0.028 ms\r\n",
      "\r\n",
      "============================================================\r\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\r\n",
      "============================================================\r\n",
      "Model                Forward (ms)              Backward (ms)            \r\n",
      "------------------------------------------------------------\r\n",
      "SKLinear_triton      0.000 ± 0.000             2.245 ± 0.028            \r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 2/300 ====================\r\n",
      "In features: 256, Out features: 32768, Ratio: 1:128\r\n",
      "Base size: 256, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "\r\n",
      "=== SKLinear_triton FORWARD PASS BENCHMARK ===\r\n",
      "SKLinear_triton forward: 0.000 ± 0.000 ms\r\n",
      "SKLinear_triton backward: 2.335 ± 0.042 ms\r\n",
      "\r\n",
      "============================================================\r\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\r\n",
      "============================================================\r\n",
      "Model                Forward (ms)              Backward (ms)            \r\n",
      "------------------------------------------------------------\r\n",
      "SKLinear_triton      0.000 ± 0.000             2.335 ± 0.042            \r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 3/300 ====================\r\n",
      "In features: 256, Out features: 32768, Ratio: 1:128\r\n",
      "Base size: 256, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "\r\n",
      "=== SKLinear_triton FORWARD PASS BENCHMARK ===\r\n",
      "SKLinear_triton forward: 0.000 ± 0.000 ms\r\n",
      "SKLinear_triton backward: 2.888 ± 0.781 ms\r\n",
      "\r\n",
      "============================================================\r\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\r\n",
      "============================================================\r\n",
      "Model                Forward (ms)              Backward (ms)            \r\n",
      "------------------------------------------------------------\r\n",
      "SKLinear_triton      0.000 ± 0.000             2.888 ± 0.781            \r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 4/300 ====================\r\n",
      "In features: 256, Out features: 32768, Ratio: 1:128\r\n",
      "Base size: 256, Num terms: 1, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 1 * 128 * (32768 + 256) >= 32768 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 5/300 ====================\r\n",
      "In features: 256, Out features: 32768, Ratio: 1:128\r\n",
      "Base size: 256, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "\r\n",
      "=== SKLinear_triton FORWARD PASS BENCHMARK ===\r\n",
      "SKLinear_triton forward: 0.000 ± 0.000 ms\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 6/300 ====================\r\n",
      "In features: 256, Out features: 32768, Ratio: 1:128\r\n",
      "Base size: 256, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 7/300 ====================\r\n",
      "In features: 256, Out features: 32768, Ratio: 1:128\r\n",
      "Base size: 256, Num terms: 2, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 2 * 64 * (32768 + 256) >= 32768 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 8/300 ====================\r\n",
      "In features: 256, Out features: 32768, Ratio: 1:128\r\n",
      "Base size: 256, Num terms: 2, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 2 * 128 * (32768 + 256) >= 32768 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 9/300 ====================\r\n",
      "In features: 256, Out features: 32768, Ratio: 1:128\r\n",
      "Base size: 256, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 10/300 ====================\r\n",
      "In features: 256, Out features: 32768, Ratio: 1:128\r\n",
      "Base size: 256, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 11/300 ====================\r\n",
      "In features: 256, Out features: 32768, Ratio: 1:128\r\n",
      "Base size: 256, Num terms: 3, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 3 * 64 * (32768 + 256) >= 32768 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 12/300 ====================\r\n",
      "In features: 256, Out features: 32768, Ratio: 1:128\r\n",
      "Base size: 256, Num terms: 3, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 3 * 128 * (32768 + 256) >= 32768 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 13/300 ====================\r\n",
      "In features: 512, Out features: 65536, Ratio: 1:128\r\n",
      "Base size: 512, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 14/300 ====================\r\n",
      "In features: 512, Out features: 65536, Ratio: 1:128\r\n",
      "Base size: 512, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 15/300 ====================\r\n",
      "In features: 512, Out features: 65536, Ratio: 1:128\r\n",
      "Base size: 512, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 16/300 ====================\r\n",
      "In features: 512, Out features: 65536, Ratio: 1:128\r\n",
      "Base size: 512, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 17/300 ====================\r\n",
      "In features: 512, Out features: 65536, Ratio: 1:128\r\n",
      "Base size: 512, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 18/300 ====================\r\n",
      "In features: 512, Out features: 65536, Ratio: 1:128\r\n",
      "Base size: 512, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 19/300 ====================\r\n",
      "In features: 512, Out features: 65536, Ratio: 1:128\r\n",
      "Base size: 512, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 20/300 ====================\r\n",
      "In features: 512, Out features: 65536, Ratio: 1:128\r\n",
      "Base size: 512, Num terms: 2, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 2 * 128 * (65536 + 512) >= 65536 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 21/300 ====================\r\n",
      "In features: 512, Out features: 65536, Ratio: 1:128\r\n",
      "Base size: 512, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 22/300 ====================\r\n",
      "In features: 512, Out features: 65536, Ratio: 1:128\r\n",
      "Base size: 512, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 23/300 ====================\r\n",
      "In features: 512, Out features: 65536, Ratio: 1:128\r\n",
      "Base size: 512, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 24/300 ====================\r\n",
      "In features: 512, Out features: 65536, Ratio: 1:128\r\n",
      "Base size: 512, Num terms: 3, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 3 * 128 * (65536 + 512) >= 65536 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 25/300 ====================\r\n",
      "In features: 1024, Out features: 131072, Ratio: 1:128\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 26/300 ====================\r\n",
      "In features: 1024, Out features: 131072, Ratio: 1:128\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 27/300 ====================\r\n",
      "In features: 1024, Out features: 131072, Ratio: 1:128\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 28/300 ====================\r\n",
      "In features: 1024, Out features: 131072, Ratio: 1:128\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 29/300 ====================\r\n",
      "In features: 1024, Out features: 131072, Ratio: 1:128\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 30/300 ====================\r\n",
      "In features: 1024, Out features: 131072, Ratio: 1:128\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 31/300 ====================\r\n",
      "In features: 1024, Out features: 131072, Ratio: 1:128\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 32/300 ====================\r\n",
      "In features: 1024, Out features: 131072, Ratio: 1:128\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 33/300 ====================\r\n",
      "In features: 1024, Out features: 131072, Ratio: 1:128\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 34/300 ====================\r\n",
      "In features: 1024, Out features: 131072, Ratio: 1:128\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 35/300 ====================\r\n",
      "In features: 1024, Out features: 131072, Ratio: 1:128\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 36/300 ====================\r\n",
      "In features: 1024, Out features: 131072, Ratio: 1:128\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 37/300 ====================\r\n",
      "In features: 8192, Out features: 1048576, Ratio: 1:128\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 38/300 ====================\r\n",
      "In features: 8192, Out features: 1048576, Ratio: 1:128\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 39/300 ====================\r\n",
      "In features: 8192, Out features: 1048576, Ratio: 1:128\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 40/300 ====================\r\n",
      "In features: 8192, Out features: 1048576, Ratio: 1:128\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 41/300 ====================\r\n",
      "In features: 8192, Out features: 1048576, Ratio: 1:128\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 42/300 ====================\r\n",
      "In features: 8192, Out features: 1048576, Ratio: 1:128\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 43/300 ====================\r\n",
      "In features: 8192, Out features: 1048576, Ratio: 1:128\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 44/300 ====================\r\n",
      "In features: 8192, Out features: 1048576, Ratio: 1:128\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 45/300 ====================\r\n",
      "In features: 8192, Out features: 1048576, Ratio: 1:128\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 46/300 ====================\r\n",
      "In features: 8192, Out features: 1048576, Ratio: 1:128\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 47/300 ====================\r\n",
      "In features: 8192, Out features: 1048576, Ratio: 1:128\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 48/300 ====================\r\n",
      "In features: 8192, Out features: 1048576, Ratio: 1:128\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 49/300 ====================\r\n",
      "In features: 16384, Out features: 2097152, Ratio: 1:128\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 50/300 ====================\r\n",
      "In features: 16384, Out features: 2097152, Ratio: 1:128\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 51/300 ====================\r\n",
      "In features: 16384, Out features: 2097152, Ratio: 1:128\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 52/300 ====================\r\n",
      "In features: 16384, Out features: 2097152, Ratio: 1:128\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 53/300 ====================\r\n",
      "In features: 16384, Out features: 2097152, Ratio: 1:128\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 54/300 ====================\r\n",
      "In features: 16384, Out features: 2097152, Ratio: 1:128\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 55/300 ====================\r\n",
      "In features: 16384, Out features: 2097152, Ratio: 1:128\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 56/300 ====================\r\n",
      "In features: 16384, Out features: 2097152, Ratio: 1:128\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 57/300 ====================\r\n",
      "In features: 16384, Out features: 2097152, Ratio: 1:128\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 58/300 ====================\r\n",
      "In features: 16384, Out features: 2097152, Ratio: 1:128\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 59/300 ====================\r\n",
      "In features: 16384, Out features: 2097152, Ratio: 1:128\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 60/300 ====================\r\n",
      "In features: 16384, Out features: 2097152, Ratio: 1:128\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 61/300 ====================\r\n",
      "In features: 32768, Out features: 256, Ratio: 128:1\r\n",
      "Base size: 256, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 62/300 ====================\r\n",
      "In features: 32768, Out features: 256, Ratio: 128:1\r\n",
      "Base size: 256, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 63/300 ====================\r\n",
      "In features: 32768, Out features: 256, Ratio: 128:1\r\n",
      "Base size: 256, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 64/300 ====================\r\n",
      "In features: 32768, Out features: 256, Ratio: 128:1\r\n",
      "Base size: 256, Num terms: 1, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 1 * 128 * (256 + 32768) >= 256 * 32768\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 65/300 ====================\r\n",
      "In features: 32768, Out features: 256, Ratio: 128:1\r\n",
      "Base size: 256, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 66/300 ====================\r\n",
      "In features: 32768, Out features: 256, Ratio: 128:1\r\n",
      "Base size: 256, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 67/300 ====================\r\n",
      "In features: 32768, Out features: 256, Ratio: 128:1\r\n",
      "Base size: 256, Num terms: 2, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 2 * 64 * (256 + 32768) >= 256 * 32768\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 68/300 ====================\r\n",
      "In features: 32768, Out features: 256, Ratio: 128:1\r\n",
      "Base size: 256, Num terms: 2, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 2 * 128 * (256 + 32768) >= 256 * 32768\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 69/300 ====================\r\n",
      "In features: 32768, Out features: 256, Ratio: 128:1\r\n",
      "Base size: 256, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 70/300 ====================\r\n",
      "In features: 32768, Out features: 256, Ratio: 128:1\r\n",
      "Base size: 256, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 71/300 ====================\r\n",
      "In features: 32768, Out features: 256, Ratio: 128:1\r\n",
      "Base size: 256, Num terms: 3, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 3 * 64 * (256 + 32768) >= 256 * 32768\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 72/300 ====================\r\n",
      "In features: 32768, Out features: 256, Ratio: 128:1\r\n",
      "Base size: 256, Num terms: 3, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 3 * 128 * (256 + 32768) >= 256 * 32768\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 73/300 ====================\r\n",
      "In features: 65536, Out features: 512, Ratio: 128:1\r\n",
      "Base size: 512, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 74/300 ====================\r\n",
      "In features: 65536, Out features: 512, Ratio: 128:1\r\n",
      "Base size: 512, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 75/300 ====================\r\n",
      "In features: 65536, Out features: 512, Ratio: 128:1\r\n",
      "Base size: 512, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 76/300 ====================\r\n",
      "In features: 65536, Out features: 512, Ratio: 128:1\r\n",
      "Base size: 512, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 77/300 ====================\r\n",
      "In features: 65536, Out features: 512, Ratio: 128:1\r\n",
      "Base size: 512, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 78/300 ====================\r\n",
      "In features: 65536, Out features: 512, Ratio: 128:1\r\n",
      "Base size: 512, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 79/300 ====================\r\n",
      "In features: 65536, Out features: 512, Ratio: 128:1\r\n",
      "Base size: 512, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 80/300 ====================\r\n",
      "In features: 65536, Out features: 512, Ratio: 128:1\r\n",
      "Base size: 512, Num terms: 2, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 2 * 128 * (512 + 65536) >= 512 * 65536\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 81/300 ====================\r\n",
      "In features: 65536, Out features: 512, Ratio: 128:1\r\n",
      "Base size: 512, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 82/300 ====================\r\n",
      "In features: 65536, Out features: 512, Ratio: 128:1\r\n",
      "Base size: 512, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 83/300 ====================\r\n",
      "In features: 65536, Out features: 512, Ratio: 128:1\r\n",
      "Base size: 512, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 84/300 ====================\r\n",
      "In features: 65536, Out features: 512, Ratio: 128:1\r\n",
      "Base size: 512, Num terms: 3, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 3 * 128 * (512 + 65536) >= 512 * 65536\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 85/300 ====================\r\n",
      "In features: 131072, Out features: 1024, Ratio: 128:1\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 86/300 ====================\r\n",
      "In features: 131072, Out features: 1024, Ratio: 128:1\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 87/300 ====================\r\n",
      "In features: 131072, Out features: 1024, Ratio: 128:1\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 88/300 ====================\r\n",
      "In features: 131072, Out features: 1024, Ratio: 128:1\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 89/300 ====================\r\n",
      "In features: 131072, Out features: 1024, Ratio: 128:1\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 90/300 ====================\r\n",
      "In features: 131072, Out features: 1024, Ratio: 128:1\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 91/300 ====================\r\n",
      "In features: 131072, Out features: 1024, Ratio: 128:1\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 92/300 ====================\r\n",
      "In features: 131072, Out features: 1024, Ratio: 128:1\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 93/300 ====================\r\n",
      "In features: 131072, Out features: 1024, Ratio: 128:1\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 94/300 ====================\r\n",
      "In features: 131072, Out features: 1024, Ratio: 128:1\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 95/300 ====================\r\n",
      "In features: 131072, Out features: 1024, Ratio: 128:1\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 96/300 ====================\r\n",
      "In features: 131072, Out features: 1024, Ratio: 128:1\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 97/300 ====================\r\n",
      "In features: 1048576, Out features: 8192, Ratio: 128:1\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 98/300 ====================\r\n",
      "In features: 1048576, Out features: 8192, Ratio: 128:1\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 99/300 ====================\r\n",
      "In features: 1048576, Out features: 8192, Ratio: 128:1\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 100/300 ====================\r\n",
      "In features: 1048576, Out features: 8192, Ratio: 128:1\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 101/300 ====================\r\n",
      "In features: 1048576, Out features: 8192, Ratio: 128:1\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 102/300 ====================\r\n",
      "In features: 1048576, Out features: 8192, Ratio: 128:1\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 103/300 ====================\r\n",
      "In features: 1048576, Out features: 8192, Ratio: 128:1\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 104/300 ====================\r\n",
      "In features: 1048576, Out features: 8192, Ratio: 128:1\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 105/300 ====================\r\n",
      "In features: 1048576, Out features: 8192, Ratio: 128:1\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 106/300 ====================\r\n",
      "In features: 1048576, Out features: 8192, Ratio: 128:1\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 107/300 ====================\r\n",
      "In features: 1048576, Out features: 8192, Ratio: 128:1\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 108/300 ====================\r\n",
      "In features: 1048576, Out features: 8192, Ratio: 128:1\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 109/300 ====================\r\n",
      "In features: 2097152, Out features: 16384, Ratio: 128:1\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 110/300 ====================\r\n",
      "In features: 2097152, Out features: 16384, Ratio: 128:1\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 111/300 ====================\r\n",
      "In features: 2097152, Out features: 16384, Ratio: 128:1\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 112/300 ====================\r\n",
      "In features: 2097152, Out features: 16384, Ratio: 128:1\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 113/300 ====================\r\n",
      "In features: 2097152, Out features: 16384, Ratio: 128:1\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 114/300 ====================\r\n",
      "In features: 2097152, Out features: 16384, Ratio: 128:1\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 115/300 ====================\r\n",
      "In features: 2097152, Out features: 16384, Ratio: 128:1\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 116/300 ====================\r\n",
      "In features: 2097152, Out features: 16384, Ratio: 128:1\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 117/300 ====================\r\n",
      "In features: 2097152, Out features: 16384, Ratio: 128:1\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 118/300 ====================\r\n",
      "In features: 2097152, Out features: 16384, Ratio: 128:1\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 119/300 ====================\r\n",
      "In features: 2097152, Out features: 16384, Ratio: 128:1\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 120/300 ====================\r\n",
      "In features: 2097152, Out features: 16384, Ratio: 128:1\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 121/300 ====================\r\n",
      "In features: 256, Out features: 256, Ratio: 1:1\r\n",
      "Base size: 256, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 122/300 ====================\r\n",
      "In features: 256, Out features: 256, Ratio: 1:1\r\n",
      "Base size: 256, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 123/300 ====================\r\n",
      "In features: 256, Out features: 256, Ratio: 1:1\r\n",
      "Base size: 256, Num terms: 1, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 1 * 64 * (256 + 256) >= 256 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 124/300 ====================\r\n",
      "In features: 256, Out features: 256, Ratio: 1:1\r\n",
      "Base size: 256, Num terms: 1, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 1 * 128 * (256 + 256) >= 256 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 125/300 ====================\r\n",
      "In features: 256, Out features: 256, Ratio: 1:1\r\n",
      "Base size: 256, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 126/300 ====================\r\n",
      "In features: 256, Out features: 256, Ratio: 1:1\r\n",
      "Base size: 256, Num terms: 2, Low rank: 32\r\n",
      "INVALID COMBINATION: 2 * 2 * 32 * (256 + 256) >= 256 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 127/300 ====================\r\n",
      "In features: 256, Out features: 256, Ratio: 1:1\r\n",
      "Base size: 256, Num terms: 2, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 2 * 64 * (256 + 256) >= 256 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 128/300 ====================\r\n",
      "In features: 256, Out features: 256, Ratio: 1:1\r\n",
      "Base size: 256, Num terms: 2, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 2 * 128 * (256 + 256) >= 256 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 129/300 ====================\r\n",
      "In features: 256, Out features: 256, Ratio: 1:1\r\n",
      "Base size: 256, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 130/300 ====================\r\n",
      "In features: 256, Out features: 256, Ratio: 1:1\r\n",
      "Base size: 256, Num terms: 3, Low rank: 32\r\n",
      "INVALID COMBINATION: 2 * 3 * 32 * (256 + 256) >= 256 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 131/300 ====================\r\n",
      "In features: 256, Out features: 256, Ratio: 1:1\r\n",
      "Base size: 256, Num terms: 3, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 3 * 64 * (256 + 256) >= 256 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 132/300 ====================\r\n",
      "In features: 256, Out features: 256, Ratio: 1:1\r\n",
      "Base size: 256, Num terms: 3, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 3 * 128 * (256 + 256) >= 256 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 133/300 ====================\r\n",
      "In features: 512, Out features: 512, Ratio: 1:1\r\n",
      "Base size: 512, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 134/300 ====================\r\n",
      "In features: 512, Out features: 512, Ratio: 1:1\r\n",
      "Base size: 512, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 135/300 ====================\r\n",
      "In features: 512, Out features: 512, Ratio: 1:1\r\n",
      "Base size: 512, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 136/300 ====================\r\n",
      "In features: 512, Out features: 512, Ratio: 1:1\r\n",
      "Base size: 512, Num terms: 1, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 1 * 128 * (512 + 512) >= 512 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 137/300 ====================\r\n",
      "In features: 512, Out features: 512, Ratio: 1:1\r\n",
      "Base size: 512, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 138/300 ====================\r\n",
      "In features: 512, Out features: 512, Ratio: 1:1\r\n",
      "Base size: 512, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 139/300 ====================\r\n",
      "In features: 512, Out features: 512, Ratio: 1:1\r\n",
      "Base size: 512, Num terms: 2, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 2 * 64 * (512 + 512) >= 512 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 140/300 ====================\r\n",
      "In features: 512, Out features: 512, Ratio: 1:1\r\n",
      "Base size: 512, Num terms: 2, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 2 * 128 * (512 + 512) >= 512 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 141/300 ====================\r\n",
      "In features: 512, Out features: 512, Ratio: 1:1\r\n",
      "Base size: 512, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 142/300 ====================\r\n",
      "In features: 512, Out features: 512, Ratio: 1:1\r\n",
      "Base size: 512, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 143/300 ====================\r\n",
      "In features: 512, Out features: 512, Ratio: 1:1\r\n",
      "Base size: 512, Num terms: 3, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 3 * 64 * (512 + 512) >= 512 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 144/300 ====================\r\n",
      "In features: 512, Out features: 512, Ratio: 1:1\r\n",
      "Base size: 512, Num terms: 3, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 3 * 128 * (512 + 512) >= 512 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 145/300 ====================\r\n",
      "In features: 1024, Out features: 1024, Ratio: 1:1\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 146/300 ====================\r\n",
      "In features: 1024, Out features: 1024, Ratio: 1:1\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 147/300 ====================\r\n",
      "In features: 1024, Out features: 1024, Ratio: 1:1\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 148/300 ====================\r\n",
      "In features: 1024, Out features: 1024, Ratio: 1:1\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 149/300 ====================\r\n",
      "In features: 1024, Out features: 1024, Ratio: 1:1\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 150/300 ====================\r\n",
      "In features: 1024, Out features: 1024, Ratio: 1:1\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 151/300 ====================\r\n",
      "In features: 1024, Out features: 1024, Ratio: 1:1\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 152/300 ====================\r\n",
      "In features: 1024, Out features: 1024, Ratio: 1:1\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 2 * 128 * (1024 + 1024) >= 1024 * 1024\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 153/300 ====================\r\n",
      "In features: 1024, Out features: 1024, Ratio: 1:1\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 154/300 ====================\r\n",
      "In features: 1024, Out features: 1024, Ratio: 1:1\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 155/300 ====================\r\n",
      "In features: 1024, Out features: 1024, Ratio: 1:1\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 156/300 ====================\r\n",
      "In features: 1024, Out features: 1024, Ratio: 1:1\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 3 * 128 * (1024 + 1024) >= 1024 * 1024\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 157/300 ====================\r\n",
      "In features: 8192, Out features: 8192, Ratio: 1:1\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 158/300 ====================\r\n",
      "In features: 8192, Out features: 8192, Ratio: 1:1\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 159/300 ====================\r\n",
      "In features: 8192, Out features: 8192, Ratio: 1:1\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 160/300 ====================\r\n",
      "In features: 8192, Out features: 8192, Ratio: 1:1\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 161/300 ====================\r\n",
      "In features: 8192, Out features: 8192, Ratio: 1:1\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 162/300 ====================\r\n",
      "In features: 8192, Out features: 8192, Ratio: 1:1\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 163/300 ====================\r\n",
      "In features: 8192, Out features: 8192, Ratio: 1:1\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 164/300 ====================\r\n",
      "In features: 8192, Out features: 8192, Ratio: 1:1\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 165/300 ====================\r\n",
      "In features: 8192, Out features: 8192, Ratio: 1:1\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 166/300 ====================\r\n",
      "In features: 8192, Out features: 8192, Ratio: 1:1\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 167/300 ====================\r\n",
      "In features: 8192, Out features: 8192, Ratio: 1:1\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 168/300 ====================\r\n",
      "In features: 8192, Out features: 8192, Ratio: 1:1\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 169/300 ====================\r\n",
      "In features: 16384, Out features: 16384, Ratio: 1:1\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 170/300 ====================\r\n",
      "In features: 16384, Out features: 16384, Ratio: 1:1\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 171/300 ====================\r\n",
      "In features: 16384, Out features: 16384, Ratio: 1:1\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 172/300 ====================\r\n",
      "In features: 16384, Out features: 16384, Ratio: 1:1\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 173/300 ====================\r\n",
      "In features: 16384, Out features: 16384, Ratio: 1:1\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 174/300 ====================\r\n",
      "In features: 16384, Out features: 16384, Ratio: 1:1\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 175/300 ====================\r\n",
      "In features: 16384, Out features: 16384, Ratio: 1:1\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 176/300 ====================\r\n",
      "In features: 16384, Out features: 16384, Ratio: 1:1\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 177/300 ====================\r\n",
      "In features: 16384, Out features: 16384, Ratio: 1:1\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 178/300 ====================\r\n",
      "In features: 16384, Out features: 16384, Ratio: 1:1\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 179/300 ====================\r\n",
      "In features: 16384, Out features: 16384, Ratio: 1:1\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 180/300 ====================\r\n",
      "In features: 16384, Out features: 16384, Ratio: 1:1\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 181/300 ====================\r\n",
      "In features: 512, Out features: 256, Ratio: 2:1\r\n",
      "Base size: 256, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 182/300 ====================\r\n",
      "In features: 512, Out features: 256, Ratio: 2:1\r\n",
      "Base size: 256, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 183/300 ====================\r\n",
      "In features: 512, Out features: 256, Ratio: 2:1\r\n",
      "Base size: 256, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 184/300 ====================\r\n",
      "In features: 512, Out features: 256, Ratio: 2:1\r\n",
      "Base size: 256, Num terms: 1, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 1 * 128 * (256 + 512) >= 256 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 185/300 ====================\r\n",
      "In features: 512, Out features: 256, Ratio: 2:1\r\n",
      "Base size: 256, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 186/300 ====================\r\n",
      "In features: 512, Out features: 256, Ratio: 2:1\r\n",
      "Base size: 256, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 187/300 ====================\r\n",
      "In features: 512, Out features: 256, Ratio: 2:1\r\n",
      "Base size: 256, Num terms: 2, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 2 * 64 * (256 + 512) >= 256 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 188/300 ====================\r\n",
      "In features: 512, Out features: 256, Ratio: 2:1\r\n",
      "Base size: 256, Num terms: 2, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 2 * 128 * (256 + 512) >= 256 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 189/300 ====================\r\n",
      "In features: 512, Out features: 256, Ratio: 2:1\r\n",
      "Base size: 256, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 190/300 ====================\r\n",
      "In features: 512, Out features: 256, Ratio: 2:1\r\n",
      "Base size: 256, Num terms: 3, Low rank: 32\r\n",
      "INVALID COMBINATION: 2 * 3 * 32 * (256 + 512) >= 256 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 191/300 ====================\r\n",
      "In features: 512, Out features: 256, Ratio: 2:1\r\n",
      "Base size: 256, Num terms: 3, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 3 * 64 * (256 + 512) >= 256 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 192/300 ====================\r\n",
      "In features: 512, Out features: 256, Ratio: 2:1\r\n",
      "Base size: 256, Num terms: 3, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 3 * 128 * (256 + 512) >= 256 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 193/300 ====================\r\n",
      "In features: 1024, Out features: 512, Ratio: 2:1\r\n",
      "Base size: 512, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 194/300 ====================\r\n",
      "In features: 1024, Out features: 512, Ratio: 2:1\r\n",
      "Base size: 512, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 195/300 ====================\r\n",
      "In features: 1024, Out features: 512, Ratio: 2:1\r\n",
      "Base size: 512, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 196/300 ====================\r\n",
      "In features: 1024, Out features: 512, Ratio: 2:1\r\n",
      "Base size: 512, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 197/300 ====================\r\n",
      "In features: 1024, Out features: 512, Ratio: 2:1\r\n",
      "Base size: 512, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 198/300 ====================\r\n",
      "In features: 1024, Out features: 512, Ratio: 2:1\r\n",
      "Base size: 512, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 199/300 ====================\r\n",
      "In features: 1024, Out features: 512, Ratio: 2:1\r\n",
      "Base size: 512, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 200/300 ====================\r\n",
      "In features: 1024, Out features: 512, Ratio: 2:1\r\n",
      "Base size: 512, Num terms: 2, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 2 * 128 * (512 + 1024) >= 512 * 1024\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 201/300 ====================\r\n",
      "In features: 1024, Out features: 512, Ratio: 2:1\r\n",
      "Base size: 512, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 202/300 ====================\r\n",
      "In features: 1024, Out features: 512, Ratio: 2:1\r\n",
      "Base size: 512, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 203/300 ====================\r\n",
      "In features: 1024, Out features: 512, Ratio: 2:1\r\n",
      "Base size: 512, Num terms: 3, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 3 * 64 * (512 + 1024) >= 512 * 1024\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 204/300 ====================\r\n",
      "In features: 1024, Out features: 512, Ratio: 2:1\r\n",
      "Base size: 512, Num terms: 3, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 3 * 128 * (512 + 1024) >= 512 * 1024\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 205/300 ====================\r\n",
      "In features: 2048, Out features: 1024, Ratio: 2:1\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 206/300 ====================\r\n",
      "In features: 2048, Out features: 1024, Ratio: 2:1\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 207/300 ====================\r\n",
      "In features: 2048, Out features: 1024, Ratio: 2:1\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 208/300 ====================\r\n",
      "In features: 2048, Out features: 1024, Ratio: 2:1\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 209/300 ====================\r\n",
      "In features: 2048, Out features: 1024, Ratio: 2:1\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 210/300 ====================\r\n",
      "In features: 2048, Out features: 1024, Ratio: 2:1\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 211/300 ====================\r\n",
      "In features: 2048, Out features: 1024, Ratio: 2:1\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 212/300 ====================\r\n",
      "In features: 2048, Out features: 1024, Ratio: 2:1\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 213/300 ====================\r\n",
      "In features: 2048, Out features: 1024, Ratio: 2:1\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 214/300 ====================\r\n",
      "In features: 2048, Out features: 1024, Ratio: 2:1\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 215/300 ====================\r\n",
      "In features: 2048, Out features: 1024, Ratio: 2:1\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 216/300 ====================\r\n",
      "In features: 2048, Out features: 1024, Ratio: 2:1\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 3 * 128 * (1024 + 2048) >= 1024 * 2048\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 217/300 ====================\r\n",
      "In features: 16384, Out features: 8192, Ratio: 2:1\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 218/300 ====================\r\n",
      "In features: 16384, Out features: 8192, Ratio: 2:1\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 219/300 ====================\r\n",
      "In features: 16384, Out features: 8192, Ratio: 2:1\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 220/300 ====================\r\n",
      "In features: 16384, Out features: 8192, Ratio: 2:1\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 221/300 ====================\r\n",
      "In features: 16384, Out features: 8192, Ratio: 2:1\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 222/300 ====================\r\n",
      "In features: 16384, Out features: 8192, Ratio: 2:1\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 223/300 ====================\r\n",
      "In features: 16384, Out features: 8192, Ratio: 2:1\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 224/300 ====================\r\n",
      "In features: 16384, Out features: 8192, Ratio: 2:1\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 225/300 ====================\r\n",
      "In features: 16384, Out features: 8192, Ratio: 2:1\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 226/300 ====================\r\n",
      "In features: 16384, Out features: 8192, Ratio: 2:1\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 227/300 ====================\r\n",
      "In features: 16384, Out features: 8192, Ratio: 2:1\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 228/300 ====================\r\n",
      "In features: 16384, Out features: 8192, Ratio: 2:1\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 229/300 ====================\r\n",
      "In features: 32768, Out features: 16384, Ratio: 2:1\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 230/300 ====================\r\n",
      "In features: 32768, Out features: 16384, Ratio: 2:1\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 231/300 ====================\r\n",
      "In features: 32768, Out features: 16384, Ratio: 2:1\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 232/300 ====================\r\n",
      "In features: 32768, Out features: 16384, Ratio: 2:1\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 233/300 ====================\r\n",
      "In features: 32768, Out features: 16384, Ratio: 2:1\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 234/300 ====================\r\n",
      "In features: 32768, Out features: 16384, Ratio: 2:1\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 235/300 ====================\r\n",
      "In features: 32768, Out features: 16384, Ratio: 2:1\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 236/300 ====================\r\n",
      "In features: 32768, Out features: 16384, Ratio: 2:1\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 237/300 ====================\r\n",
      "In features: 32768, Out features: 16384, Ratio: 2:1\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 238/300 ====================\r\n",
      "In features: 32768, Out features: 16384, Ratio: 2:1\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 239/300 ====================\r\n",
      "In features: 32768, Out features: 16384, Ratio: 2:1\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 240/300 ====================\r\n",
      "In features: 32768, Out features: 16384, Ratio: 2:1\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 241/300 ====================\r\n",
      "In features: 256, Out features: 512, Ratio: 1:2\r\n",
      "Base size: 256, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 242/300 ====================\r\n",
      "In features: 256, Out features: 512, Ratio: 1:2\r\n",
      "Base size: 256, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 243/300 ====================\r\n",
      "In features: 256, Out features: 512, Ratio: 1:2\r\n",
      "Base size: 256, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 244/300 ====================\r\n",
      "In features: 256, Out features: 512, Ratio: 1:2\r\n",
      "Base size: 256, Num terms: 1, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 1 * 128 * (512 + 256) >= 512 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 245/300 ====================\r\n",
      "In features: 256, Out features: 512, Ratio: 1:2\r\n",
      "Base size: 256, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 246/300 ====================\r\n",
      "In features: 256, Out features: 512, Ratio: 1:2\r\n",
      "Base size: 256, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 247/300 ====================\r\n",
      "In features: 256, Out features: 512, Ratio: 1:2\r\n",
      "Base size: 256, Num terms: 2, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 2 * 64 * (512 + 256) >= 512 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 248/300 ====================\r\n",
      "In features: 256, Out features: 512, Ratio: 1:2\r\n",
      "Base size: 256, Num terms: 2, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 2 * 128 * (512 + 256) >= 512 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 249/300 ====================\r\n",
      "In features: 256, Out features: 512, Ratio: 1:2\r\n",
      "Base size: 256, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 250/300 ====================\r\n",
      "In features: 256, Out features: 512, Ratio: 1:2\r\n",
      "Base size: 256, Num terms: 3, Low rank: 32\r\n",
      "INVALID COMBINATION: 2 * 3 * 32 * (512 + 256) >= 512 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 251/300 ====================\r\n",
      "In features: 256, Out features: 512, Ratio: 1:2\r\n",
      "Base size: 256, Num terms: 3, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 3 * 64 * (512 + 256) >= 512 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 252/300 ====================\r\n",
      "In features: 256, Out features: 512, Ratio: 1:2\r\n",
      "Base size: 256, Num terms: 3, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 3 * 128 * (512 + 256) >= 512 * 256\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 253/300 ====================\r\n",
      "In features: 512, Out features: 1024, Ratio: 1:2\r\n",
      "Base size: 512, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 254/300 ====================\r\n",
      "In features: 512, Out features: 1024, Ratio: 1:2\r\n",
      "Base size: 512, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 255/300 ====================\r\n",
      "In features: 512, Out features: 1024, Ratio: 1:2\r\n",
      "Base size: 512, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 256/300 ====================\r\n",
      "In features: 512, Out features: 1024, Ratio: 1:2\r\n",
      "Base size: 512, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 257/300 ====================\r\n",
      "In features: 512, Out features: 1024, Ratio: 1:2\r\n",
      "Base size: 512, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 258/300 ====================\r\n",
      "In features: 512, Out features: 1024, Ratio: 1:2\r\n",
      "Base size: 512, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 259/300 ====================\r\n",
      "In features: 512, Out features: 1024, Ratio: 1:2\r\n",
      "Base size: 512, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 260/300 ====================\r\n",
      "In features: 512, Out features: 1024, Ratio: 1:2\r\n",
      "Base size: 512, Num terms: 2, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 2 * 128 * (1024 + 512) >= 1024 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 261/300 ====================\r\n",
      "In features: 512, Out features: 1024, Ratio: 1:2\r\n",
      "Base size: 512, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 262/300 ====================\r\n",
      "In features: 512, Out features: 1024, Ratio: 1:2\r\n",
      "Base size: 512, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 263/300 ====================\r\n",
      "In features: 512, Out features: 1024, Ratio: 1:2\r\n",
      "Base size: 512, Num terms: 3, Low rank: 64\r\n",
      "INVALID COMBINATION: 2 * 3 * 64 * (1024 + 512) >= 1024 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 264/300 ====================\r\n",
      "In features: 512, Out features: 1024, Ratio: 1:2\r\n",
      "Base size: 512, Num terms: 3, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 3 * 128 * (1024 + 512) >= 1024 * 512\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 265/300 ====================\r\n",
      "In features: 1024, Out features: 2048, Ratio: 1:2\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 266/300 ====================\r\n",
      "In features: 1024, Out features: 2048, Ratio: 1:2\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 267/300 ====================\r\n",
      "In features: 1024, Out features: 2048, Ratio: 1:2\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 268/300 ====================\r\n",
      "In features: 1024, Out features: 2048, Ratio: 1:2\r\n",
      "Base size: 1024, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 269/300 ====================\r\n",
      "In features: 1024, Out features: 2048, Ratio: 1:2\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 270/300 ====================\r\n",
      "In features: 1024, Out features: 2048, Ratio: 1:2\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 271/300 ====================\r\n",
      "In features: 1024, Out features: 2048, Ratio: 1:2\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 272/300 ====================\r\n",
      "In features: 1024, Out features: 2048, Ratio: 1:2\r\n",
      "Base size: 1024, Num terms: 2, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 273/300 ====================\r\n",
      "In features: 1024, Out features: 2048, Ratio: 1:2\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 274/300 ====================\r\n",
      "In features: 1024, Out features: 2048, Ratio: 1:2\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 275/300 ====================\r\n",
      "In features: 1024, Out features: 2048, Ratio: 1:2\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 276/300 ====================\r\n",
      "In features: 1024, Out features: 2048, Ratio: 1:2\r\n",
      "Base size: 1024, Num terms: 3, Low rank: 128\r\n",
      "INVALID COMBINATION: 2 * 3 * 128 * (2048 + 1024) >= 2048 * 1024\r\n",
      "Skipping benchmarks for this invalid combination\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 277/300 ====================\r\n",
      "In features: 8192, Out features: 16384, Ratio: 1:2\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 278/300 ====================\r\n",
      "In features: 8192, Out features: 16384, Ratio: 1:2\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 279/300 ====================\r\n",
      "In features: 8192, Out features: 16384, Ratio: 1:2\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 280/300 ====================\r\n",
      "In features: 8192, Out features: 16384, Ratio: 1:2\r\n",
      "Base size: 8192, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 281/300 ====================\r\n",
      "In features: 8192, Out features: 16384, Ratio: 1:2\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 282/300 ====================\r\n",
      "In features: 8192, Out features: 16384, Ratio: 1:2\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 283/300 ====================\r\n",
      "In features: 8192, Out features: 16384, Ratio: 1:2\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 284/300 ====================\r\n",
      "In features: 8192, Out features: 16384, Ratio: 1:2\r\n",
      "Base size: 8192, Num terms: 2, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 285/300 ====================\r\n",
      "In features: 8192, Out features: 16384, Ratio: 1:2\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 286/300 ====================\r\n",
      "In features: 8192, Out features: 16384, Ratio: 1:2\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 287/300 ====================\r\n",
      "In features: 8192, Out features: 16384, Ratio: 1:2\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 288/300 ====================\r\n",
      "In features: 8192, Out features: 16384, Ratio: 1:2\r\n",
      "Base size: 8192, Num terms: 3, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 289/300 ====================\r\n",
      "In features: 16384, Out features: 32768, Ratio: 1:2\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 290/300 ====================\r\n",
      "In features: 16384, Out features: 32768, Ratio: 1:2\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 291/300 ====================\r\n",
      "In features: 16384, Out features: 32768, Ratio: 1:2\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 292/300 ====================\r\n",
      "In features: 16384, Out features: 32768, Ratio: 1:2\r\n",
      "Base size: 16384, Num terms: 1, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 293/300 ====================\r\n",
      "In features: 16384, Out features: 32768, Ratio: 1:2\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 294/300 ====================\r\n",
      "In features: 16384, Out features: 32768, Ratio: 1:2\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 295/300 ====================\r\n",
      "In features: 16384, Out features: 32768, Ratio: 1:2\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 296/300 ====================\r\n",
      "In features: 16384, Out features: 32768, Ratio: 1:2\r\n",
      "Base size: 16384, Num terms: 2, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 297/300 ====================\r\n",
      "In features: 16384, Out features: 32768, Ratio: 1:2\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 16\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 298/300 ====================\r\n",
      "In features: 16384, Out features: 32768, Ratio: 1:2\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 32\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 299/300 ====================\r\n",
      "In features: 16384, Out features: 32768, Ratio: 1:2\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 64\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "==================== COMBINATION 300/300 ====================\r\n",
      "In features: 16384, Out features: 32768, Ratio: 1:2\r\n",
      "Base size: 16384, Num terms: 3, Low rank: 128\r\n",
      "\r\n",
      "==================== Benchmarking SKLinear_triton ====================\r\n",
      "Error benchmarking SKLinear_triton: CUDA error: an illegal memory access was encountered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n",
      "\r\n",
      "\r\n",
      "All benchmark results saved to benchmark_results.csv\r\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=/kaggle/working/panther python /kaggle/working/panther/tests/run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cb4b28d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T00:51:09.344736Z",
     "iopub.status.busy": "2025-05-05T00:51:09.344405Z",
     "iopub.status.idle": "2025-05-05T00:51:09.348379Z",
     "shell.execute_reply": "2025-05-05T00:51:09.347763Z"
    },
    "papermill": {
     "duration": 0.05616,
     "end_time": "2025-05-05T00:51:09.349645",
     "exception": false,
     "start_time": "2025-05-05T00:51:09.293485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !CUDA_LAUNCH_BLOCKING=1 PYTHONPATH=/kaggle/working/panther python /kaggle/working/panther/tests/run.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2352.137609,
   "end_time": "2025-05-05T00:51:10.221123",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-05T00:11:58.083514",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
