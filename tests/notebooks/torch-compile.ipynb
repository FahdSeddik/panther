{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Enable error suppression to fall back to eager mode if compilation fails\n",
    "import torch._dynamo\n",
    "import torch.nn as nn\n",
    "\n",
    "from panther.nn import SKLinear\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_features: int, out_features: int, num_terms: int, low_rank: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layer = SKLinear(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            num_terms=num_terms,\n",
    "            low_rank=low_rank,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_compile(\n",
    "    model: nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    "    backend: Optional[str] = None,\n",
    "    mode: Optional[str] = None,\n",
    "    fullgraph: bool = True,\n",
    "    dynamic: bool = False,\n",
    "    options: Optional[Dict[str, Any]] = None,\n",
    "    num_runs: int = 50,  # Reduced for faster benchmarking\n",
    "    warmup: int = 5,  # Reduced for faster benchmarking\n",
    "    compile_name: str = \"Unknown\",\n",
    ") -> Dict[str, Union[float, str]]:\n",
    "    \"\"\"\n",
    "    Benchmark a model with torch.compile using different backends and parameters.\n",
    "\n",
    "    Args:\n",
    "        model: The model to benchmark\n",
    "        input_tensor: Input tensor for the model\n",
    "        backend: The backend to use for torch.compile\n",
    "        mode: The mode to use for torch.compile\n",
    "        fullgraph: Whether to use fullgraph mode\n",
    "        dynamic: Whether to use dynamic shapes\n",
    "        options: Additional options for torch.compile\n",
    "        num_runs: Number of runs for benchmarking\n",
    "        warmup: Number of warmup runs\n",
    "        compile_name: Name for this compilation configuration\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with benchmark results\n",
    "    \"\"\"\n",
    "    print(f\"Compiling model with {compile_name} configuration...\")\n",
    "\n",
    "    # Create a copy of the model for compilation\n",
    "    try:\n",
    "        compiled_model = torch.compile(\n",
    "            model,\n",
    "            backend=backend,\n",
    "            mode=mode,\n",
    "            fullgraph=fullgraph,\n",
    "            dynamic=dynamic,\n",
    "            options=options,\n",
    "        )\n",
    "        print(f\"Successfully compiled model with {compile_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Compilation failed for {compile_name}, falling back to eager mode: {e}\")\n",
    "        compiled_model = model  # Fall back to the original model\n",
    "\n",
    "    # Warmup\n",
    "    print(f\"Running warmup for {compile_name}...\")\n",
    "    for _ in range(warmup):\n",
    "        try:\n",
    "            _ = compiled_model(input_tensor)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during warmup for {compile_name}: {e}\")\n",
    "            print(\"Falling back to original model\")\n",
    "            compiled_model = model\n",
    "            # Re-run warmup with original model\n",
    "            for _ in range(warmup):\n",
    "                _ = compiled_model(input_tensor)\n",
    "            break\n",
    "\n",
    "    # Benchmark forward pass\n",
    "    print(f\"Benchmarking forward pass for {compile_name}...\")\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(num_runs):\n",
    "        _ = compiled_model(input_tensor)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    forward_time = (time.perf_counter() - start_time) / num_runs\n",
    "\n",
    "    # Benchmark backward pass\n",
    "    print(f\"Benchmarking backward pass for {compile_name}...\")\n",
    "    criterion = nn.MSELoss()\n",
    "    target = torch.randn_like(compiled_model(input_tensor))\n",
    "\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(num_runs):\n",
    "        output = compiled_model(input_tensor)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    backward_time = (time.perf_counter() - start_time) / num_runs\n",
    "\n",
    "    print(f\"Completed benchmarking for {compile_name}\")\n",
    "    return {\n",
    "        \"name\": compile_name,\n",
    "        \"backend\": str(backend) if backend else \"default\",\n",
    "        \"mode\": str(mode) if mode else \"default\",\n",
    "        \"fullgraph\": str(fullgraph),\n",
    "        \"dynamic\": str(dynamic),\n",
    "        \"forward_time\": forward_time,\n",
    "        \"backward_time\": backward_time,\n",
    "        \"total_time\": forward_time + backward_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_baseline(\n",
    "    model: nn.Module, input_tensor: torch.Tensor, num_runs: int = 50, warmup: int = 5\n",
    ") -> Dict[str, Union[float, str]]:\n",
    "    \"\"\"Benchmark the baseline model without compilation\"\"\"\n",
    "    print(\"Running baseline benchmark (no compilation)...\")\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = model(input_tensor)\n",
    "\n",
    "    # Benchmark forward pass\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(num_runs):\n",
    "        _ = model(input_tensor)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    forward_time = (time.perf_counter() - start_time) / num_runs\n",
    "\n",
    "    # Benchmark backward pass\n",
    "    criterion = nn.MSELoss()\n",
    "    target = torch.randn_like(model(input_tensor))\n",
    "\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(num_runs):\n",
    "        output = model(input_tensor)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    backward_time = (time.perf_counter() - start_time) / num_runs\n",
    "\n",
    "    print(\"Completed baseline benchmark\")\n",
    "    return {\n",
    "        \"name\": \"Baseline (No Compile)\",\n",
    "        \"backend\": \"None\",\n",
    "        \"mode\": \"None\",\n",
    "        \"fullgraph\": \"N/A\",\n",
    "        \"dynamic\": \"N/A\",\n",
    "        \"forward_time\": forward_time,\n",
    "        \"backward_time\": backward_time,\n",
    "        \"total_time\": forward_time + backward_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results: List[Dict[str, Union[float, str]]]):\n",
    "    \"\"\"Plot the benchmark results for all backends together\"\"\"\n",
    "    try:\n",
    "        # Sort results by total time\n",
    "        results = sorted(\n",
    "            results,\n",
    "            key=lambda x: float(x[\"total_time\"])\n",
    "            if isinstance(x[\"total_time\"], (int, float))\n",
    "            else 0,\n",
    "        )\n",
    "\n",
    "        # Extract data for plotting\n",
    "        names = [result[\"name\"] for result in results]\n",
    "        forward_times = [\n",
    "            float(result[\"forward_time\"]) * 1000 for result in results\n",
    "        ]  # Convert to ms\n",
    "        backward_times = [\n",
    "            float(result[\"backward_time\"]) * 1000 for result in results\n",
    "        ]  # Convert to ms\n",
    "\n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        # Create stacked bar chart\n",
    "        bar_width = 0.6\n",
    "        x = np.arange(len(names))\n",
    "\n",
    "        ax.bar(x, forward_times, bar_width, label=\"Forward Pass\", color=\"skyblue\")\n",
    "        ax.bar(\n",
    "            x,\n",
    "            backward_times,\n",
    "            bar_width,\n",
    "            bottom=forward_times,\n",
    "            label=\"Backward Pass\",\n",
    "            color=\"salmon\",\n",
    "        )\n",
    "\n",
    "        # Add labels and title\n",
    "        ax.set_xlabel(\"Compilation Configuration\")\n",
    "        ax.set_ylabel(\"Time (ms)\")\n",
    "        ax.set_title(\"SKLinear Performance with Different torch.compile Configurations\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(names, rotation=45, ha=\"right\")\n",
    "        ax.legend()\n",
    "\n",
    "        # Add speedup annotations\n",
    "        baseline_idx = next(\n",
    "            (\n",
    "                i\n",
    "                for i, result in enumerate(results)\n",
    "                if result[\"name\"] == \"Baseline (No Compile)\"\n",
    "            ),\n",
    "            0,\n",
    "        )\n",
    "        baseline_time = float(results[baseline_idx][\"total_time\"])\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            # Fix the sum() issue - we're just adding two floats\n",
    "            total_time_ms = forward_times[i] + backward_times[i]\n",
    "            speedup = baseline_time / float(result[\"total_time\"])\n",
    "            ax.text(\n",
    "                i,\n",
    "                total_time_ms + 0.5,\n",
    "                f\"{speedup:.2f}x\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "            )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"sklinear_compile_benchmark_all.png\")\n",
    "        print(\"Saved plot to sklinear_compile_benchmark_all.png\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting results: {e}\")\n",
    "        print(\"Skipping plot generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_by_backend(\n",
    "    results: List[Dict[str, Union[float, str]]],\n",
    "    baseline_result: Dict[str, Union[float, str]],\n",
    "):\n",
    "    \"\"\"Plot the benchmark results separately for each backend\"\"\"\n",
    "    try:\n",
    "        # Group results by backend\n",
    "        backends = set(\n",
    "            result[\"backend\"] for result in results if result[\"backend\"] != \"None\"\n",
    "        )\n",
    "\n",
    "        # Get baseline time for speedup calculations\n",
    "        baseline_time = float(baseline_result[\"total_time\"])\n",
    "\n",
    "        for backend in backends:\n",
    "            # Filter results for this backend and add baseline\n",
    "            backend_results = [baseline_result] + [\n",
    "                r for r in results if r[\"backend\"] == backend\n",
    "            ]\n",
    "\n",
    "            # Sort results by total time\n",
    "            backend_results = sorted(\n",
    "                backend_results,\n",
    "                key=lambda x: float(x[\"total_time\"])\n",
    "                if isinstance(x[\"total_time\"], (int, float))\n",
    "                else 0,\n",
    "            )\n",
    "\n",
    "            # Extract data for plotting\n",
    "            names = [result[\"name\"] for result in backend_results]\n",
    "            forward_times = [\n",
    "                float(result[\"forward_time\"]) * 1000 for result in backend_results\n",
    "            ]  # Convert to ms\n",
    "            backward_times = [\n",
    "                float(result[\"backward_time\"]) * 1000 for result in backend_results\n",
    "            ]  # Convert to ms\n",
    "\n",
    "            # Create figure\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "            # Create stacked bar chart\n",
    "            bar_width = 0.6\n",
    "            x = np.arange(len(names))\n",
    "\n",
    "            ax.bar(x, forward_times, bar_width, label=\"Forward Pass\", color=\"skyblue\")\n",
    "            ax.bar(\n",
    "                x,\n",
    "                backward_times,\n",
    "                bar_width,\n",
    "                bottom=forward_times,\n",
    "                label=\"Backward Pass\",\n",
    "                color=\"salmon\",\n",
    "            )\n",
    "\n",
    "            # Add labels and title\n",
    "            ax.set_xlabel(\"Compilation Configuration\")\n",
    "            ax.set_ylabel(\"Time (ms)\")\n",
    "            ax.set_title(f\"SKLinear Performance with {backend.capitalize()} Backend\")\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(names, rotation=45, ha=\"right\")\n",
    "            ax.legend()\n",
    "\n",
    "            # Add speedup annotations\n",
    "            for i, result in enumerate(backend_results):\n",
    "                total_time_ms = forward_times[i] + backward_times[i]\n",
    "                speedup = baseline_time / float(result[\"total_time\"])\n",
    "                ax.text(\n",
    "                    i,\n",
    "                    total_time_ms + 0.5,\n",
    "                    f\"{speedup:.2f}x\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                )\n",
    "\n",
    "            plt.tight_layout()\n",
    "            filename = f\"sklinear_compile_benchmark_{backend}.png\"\n",
    "            plt.savefig(filename)\n",
    "            print(f\"Saved plot to {filename}\")\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting results by backend: {e}\")\n",
    "        print(\"Skipping plot generation by backend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_triton_compatibility():\n",
    "    \"\"\"\n",
    "    Check if Triton is available and compatible with PyTorch Inductor.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (is_available, message)\n",
    "            is_available (bool): True if Triton is available and compatible\n",
    "            message (str): Descriptive message about Triton status\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import triton\n",
    "\n",
    "        triton_version = triton.__version__\n",
    "        print(f\"Triton version: {triton_version}\")\n",
    "\n",
    "        # Simple test to verify Triton works with PyTorch\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                # Try a minimal test with inductor\n",
    "                x = torch.randn(10, 10, device=\"cuda\")\n",
    "\n",
    "                def simple_fn(x):\n",
    "                    return x + x\n",
    "\n",
    "                test_model = torch.compile(simple_fn, backend=\"inductor\")\n",
    "                test_model(x)  # Run once to trigger compilation\n",
    "                return (\n",
    "                    True,\n",
    "                    f\"Triton {triton_version} is available and compatible with PyTorch Inductor\",\n",
    "                )\n",
    "            except Exception as e:\n",
    "                return (\n",
    "                    False,\n",
    "                    f\"Triton {triton_version} is installed but not compatible with PyTorch Inductor: {str(e)}\",\n",
    "                )\n",
    "        else:\n",
    "            return False, \"CUDA is not available, Inductor backend requires CUDA\"\n",
    "    except ImportError:\n",
    "        return False, \"Triton is not installed\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error checking Triton: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmarks():\n",
    "    \"\"\"Run all benchmarks\"\"\"\n",
    "    # Set up parameters\n",
    "    in_features = 1024\n",
    "    out_features = 512\n",
    "    num_terms = 4\n",
    "    low_rank = 32\n",
    "    batch_size = 64\n",
    "\n",
    "    # Reduce number of runs for faster benchmarking\n",
    "    num_runs = 30\n",
    "    warmup = 3\n",
    "\n",
    "    # Create model and input tensor\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Running benchmarks on device: {device}\")\n",
    "    model = BenchmarkModel(in_features, out_features, num_terms, low_rank).to(device)\n",
    "    input_tensor = torch.randn(batch_size, in_features, device=device)\n",
    "\n",
    "    # Check if Triton is available and compatible with PyTorch Inductor\n",
    "    inductor_available, triton_message = check_triton_compatibility()\n",
    "    print(triton_message)\n",
    "\n",
    "    # Define compilation configurations to test\n",
    "    configs = [\n",
    "        # Baseline (no compilation)\n",
    "        {\"name\": \"Baseline (No Compile)\"},\n",
    "    ]\n",
    "\n",
    "    # Define the backends and modes\n",
    "    backends = []\n",
    "\n",
    "    # Add the working backends\n",
    "    backends.extend([\"eager\", \"aot_eager\", \"cudagraphs\"])\n",
    "\n",
    "    # Only add inductor if compatible\n",
    "    if inductor_available:\n",
    "        backends.append(\"inductor\")\n",
    "\n",
    "    # Add other backends that don't depend on Triton\n",
    "    backends.extend([\"onnxrt\", \"openxla\", \"tvm\"])\n",
    "\n",
    "    modes = [\"default\", \"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"]\n",
    "\n",
    "    # Add all combinations of backends and modes\n",
    "    for backend in backends:\n",
    "        # Add default configuration for each backend\n",
    "        configs.append(\n",
    "            {\n",
    "                \"backend\": backend,\n",
    "                \"fullgraph\": True,\n",
    "                \"dynamic\": False,\n",
    "                \"name\": f\"{backend.capitalize()} (default)\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Add all modes for each backend\n",
    "        for mode in modes:\n",
    "            configs.append(\n",
    "                {\n",
    "                    \"backend\": backend,\n",
    "                    \"mode\": mode,\n",
    "                    \"fullgraph\": True,\n",
    "                    \"dynamic\": False,\n",
    "                    \"name\": f\"{backend.capitalize()} ({mode})\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Run benchmarks\n",
    "    results = []\n",
    "\n",
    "    # First run baseline without compilation\n",
    "    print(\"Running baseline benchmark...\")\n",
    "    baseline_result = benchmark_baseline(\n",
    "        model, input_tensor, num_runs=num_runs, warmup=warmup\n",
    "    )\n",
    "    results.append(baseline_result)\n",
    "\n",
    "    # Then run all compilation configurations\n",
    "    for config in configs:\n",
    "        if config.get(\"name\") == \"Baseline (No Compile)\":\n",
    "            continue  # Skip baseline as we already ran it\n",
    "\n",
    "        # Skip inductor configurations if not available\n",
    "        backend = config.get(\"backend\")\n",
    "        if backend == \"inductor\" and not inductor_available:\n",
    "            print(f\"Skipping {config.get('name')} - Inductor not compatible\")\n",
    "            continue\n",
    "\n",
    "        name = config.pop(\"name\")\n",
    "        print(f\"Running benchmark for {name}...\")\n",
    "        try:\n",
    "            result = benchmark_compile(\n",
    "                model=model,\n",
    "                input_tensor=input_tensor,\n",
    "                compile_name=name,\n",
    "                num_runs=num_runs,\n",
    "                warmup=warmup,\n",
    "                **config,\n",
    "            )\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error benchmarking {name}: {e}\")\n",
    "            # Add a placeholder result for failed benchmarks\n",
    "            results.append(\n",
    "                {\n",
    "                    \"name\": f\"{name} (Failed)\",\n",
    "                    \"backend\": str(config.get(\"backend\", \"unknown\")),\n",
    "                    \"mode\": str(config.get(\"mode\", \"default\")),\n",
    "                    \"fullgraph\": str(config.get(\"fullgraph\", True)),\n",
    "                    \"dynamic\": str(config.get(\"dynamic\", False)),\n",
    "                    \"forward_time\": 0.0,\n",
    "                    \"backward_time\": 0.0,\n",
    "                    \"total_time\": 0.0,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Convert results to DataFrame for easy viewing\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\nBenchmark Results:\")\n",
    "    print(df)\n",
    "\n",
    "    # Plot all results together\n",
    "    try:\n",
    "        plot_results(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting all results: {e}\")\n",
    "        print(\"Results DataFrame:\")\n",
    "        print(df)\n",
    "\n",
    "    # Plot results by backend\n",
    "    try:\n",
    "        plot_results_by_backend(results, baseline_result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting results by backend: {e}\")\n",
    "        print(\"Results DataFrame:\")\n",
    "        print(df)\n",
    "\n",
    "    # Save results to CSV\n",
    "    try:\n",
    "        df.to_csv(\"sklinear_compile_benchmark_results.csv\", index=False)\n",
    "        print(\"Saved results to sklinear_compile_benchmark_results.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results to CSV: {e}\")\n",
    "\n",
    "    # Save results to CSV by backend\n",
    "    try:\n",
    "        for backend in set(\n",
    "            result[\"backend\"] for result in results if result[\"backend\"] != \"None\"\n",
    "        ):\n",
    "            backend_df = pd.DataFrame(\n",
    "                [baseline_result] + [r for r in results if r[\"backend\"] == backend]\n",
    "            )\n",
    "            backend_df.to_csv(\n",
    "                f\"sklinear_compile_benchmark_results_{backend}.csv\", index=False\n",
    "            )\n",
    "            print(f\"Saved results to sklinear_compile_benchmark_results_{backend}.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results to CSV by backend: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Check PyTorch version\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "    results_df = run_benchmarks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
