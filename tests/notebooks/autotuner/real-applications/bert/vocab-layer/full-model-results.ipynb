{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11628705,"sourceType":"datasetVersion","datasetId":7295817}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"github_repos_wildcard\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:17:29.818683Z","iopub.execute_input":"2025-05-06T15:17:29.818947Z","iopub.status.idle":"2025-05-06T15:17:29.930810Z","shell.execute_reply.started":"2025-05-06T15:17:29.818922Z","shell.execute_reply":"2025-05-06T15:17:29.929948Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"repo_url = f\"https://{token}@github.com/gaserSami/panther.git\"\nbranch = \"autotuner\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:17:29.935307Z","iopub.execute_input":"2025-05-06T15:17:29.935519Z","iopub.status.idle":"2025-05-06T15:17:29.939038Z","shell.execute_reply.started":"2025-05-06T15:17:29.935503Z","shell.execute_reply":"2025-05-06T15:17:29.938236Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!git clone -b {branch} {repo_url}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:17:29.940001Z","iopub.execute_input":"2025-05-06T15:17:29.940285Z","iopub.status.idle":"2025-05-06T15:17:30.071154Z","shell.execute_reply.started":"2025-05-06T15:17:29.940260Z","shell.execute_reply":"2025-05-06T15:17:30.070187Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'panther' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# First uninstall existing torch, torchvision, torchaudio\n!pip uninstall -y torch torchvision torchaudio\n\n# Install the specified versions from PyTorch's official CUDA 12.4 wheels\n!pip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:17:30.072252Z","iopub.execute_input":"2025-05-06T15:17:30.072601Z","iopub.status.idle":"2025-05-06T15:18:00.564693Z","shell.execute_reply.started":"2025-05-06T15:17:30.072569Z","shell.execute_reply":"2025-05-06T15:18:00.563480Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.6.0+cu124\nUninstalling torch-2.6.0+cu124:\n  Successfully uninstalled torch-2.6.0+cu124\nFound existing installation: torchvision 0.21.0+cu124\nUninstalling torchvision-0.21.0+cu124:\n  Successfully uninstalled torchvision-0.21.0+cu124\nFound existing installation: torchaudio 2.6.0+cu124\nUninstalling torchaudio-2.6.0+cu124:\n  Successfully uninstalled torchaudio-2.6.0+cu124\nLooking in indexes: https://download.pytorch.org/whl/cu124\nCollecting torch==2.6.0+cu124\n  Using cached https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (28 kB)\nCollecting torchvision==0.21.0+cu124\n  Using cached https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\nCollecting torchaudio==2.6.0+cu124\n  Using cached https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (1.13.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (11.1.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0+cu124) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0+cu124) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0+cu124) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0+cu124) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.21.0+cu124) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.21.0+cu124) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.21.0+cu124) (2024.2.0)\nUsing cached https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (768.5 MB)\nUsing cached https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl (7.3 MB)\nUsing cached https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\nInstalling collected packages: torch, torchaudio, torchvision\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu124 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!mv panther Panther","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:18:00.566235Z","iopub.execute_input":"2025-05-06T15:18:00.566554Z","iopub.status.idle":"2025-05-06T15:18:00.687935Z","shell.execute_reply.started":"2025-05-06T15:18:00.566523Z","shell.execute_reply":"2025-05-06T15:18:00.686850Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\n\nsrc = '/kaggle/working/Panther'\ndst = '/kaggle/working/panther'\n\n# Simple rename\nos.rename(src, dst)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:18:00.689127Z","iopub.execute_input":"2025-05-06T15:18:00.689416Z","iopub.status.idle":"2025-05-06T15:18:04.504707Z","shell.execute_reply.started":"2025-05-06T15:18:00.689374Z","shell.execute_reply":"2025-05-06T15:18:04.503799Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"%%writefile /kaggle/working/panther/pawX/setup.py\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name=\"pawX\",\n    ext_modules=[\n        CUDAExtension(\n            name=\"pawX\",\n            sources=[\n                \"skops.cpp\",\n                \"bindings.cpp\",\n                \"linear.cpp\",\n                \"linear_cuda.cu\",\n                \"cqrrpt.cpp\",\n                \"rsvd.cpp\",\n                \"attention.cpp\",\n            ],\n            # Use system includes and libraries\n            include_dirs=[\"/usr/include/x86_64-linux-gnu\"],\n            library_dirs=[],\n            libraries=[\"openblas\"],\n            extra_compile_args={\"cxx\": [\"-O2\", \"-fopenmp\"], \"nvcc\": [\"-O2\"]},\n            extra_link_args=[\"-llapacke\", \"-lopenblas\"]\n        )\n    ],\n    cmdclass={\"build_ext\": BuildExtension},\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:18:04.505814Z","iopub.execute_input":"2025-05-06T15:18:04.506147Z","iopub.status.idle":"2025-05-06T15:18:05.318897Z","shell.execute_reply.started":"2025-05-06T15:18:04.506110Z","shell.execute_reply":"2025-05-06T15:18:05.318115Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/panther/pawX/setup.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!sudo apt-get install liblapacke-dev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:18:05.319851Z","iopub.execute_input":"2025-05-06T15:18:05.320188Z","iopub.status.idle":"2025-05-06T15:18:07.647965Z","shell.execute_reply.started":"2025-05-06T15:18:05.320153Z","shell.execute_reply":"2025-05-06T15:18:07.647227Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nliblapacke-dev is already the newest version (3.10.0-2ubuntu1).\n0 upgraded, 0 newly installed, 0 to remove and 122 not upgraded.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!cd /kaggle/working/panther/pawX; python setup.py install\n!cd /kaggle/working/panther/pawX; pip install --no-build-isolation -e .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:18:07.648979Z","iopub.execute_input":"2025-05-06T15:18:07.649296Z","iopub.status.idle":"2025-05-06T15:20:20.817554Z","shell.execute_reply.started":"2025-05-06T15:18:07.649252Z","shell.execute_reply":"2025-05-06T15:20:20.816813Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` and ``easy_install``.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://github.com/pypa/setuptools/issues/917 for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nEmitting ninja build file /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/build.ninja...\nCompiling objects...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/7] c++ -MMD -MF /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/panther/pawX/linear.cpp -o /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[2/7] c++ -MMD -MF /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/panther/pawX/rsvd.cpp -o /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[3/7] c++ -MMD -MF /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/panther/pawX/cqrrpt.cpp -o /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[4/7] c++ -MMD -MF /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/panther/pawX/attention.cpp -o /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[5/7] c++ -MMD -MF /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/panther/pawX/bindings.cpp -o /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\nIn file included from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/Exceptions.h:12,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/python.h:11,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/extension.h:9,\n                 from /kaggle/working/panther/pawX/attention.h:3,\n                 from /kaggle/working/panther/pawX/bindings.cpp:1:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘class pybind11::class_<DistributionFamily>’:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2216:7:   required from ‘class pybind11::enum_<DistributionFamily>’\n/kaggle/working/panther/pawX/bindings.cpp:23:58:   required from here\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1539:7: warning: ‘pybind11::class_<DistributionFamily>’ declared with greater visibility than its base ‘pybind11::detail::generic_type’ [-Wattributes]\n 1539 | class class_ : public detail::generic_type {\n      |       ^~~~~~\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘pybind11::class_< <template-parameter-1-1>, <template-parameter-1-2> >::class_(pybind11::handle, const char*, const Extra& ...) [with Extra = {}; type_ = DistributionFamily; options = {}]’:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2232:67:   required from ‘pybind11::enum_<Type>::enum_(const pybind11::handle&, const char*, const Extra& ...) [with Extra = {}; Type = DistributionFamily]’\n/kaggle/working/panther/pawX/bindings.cpp:23:58:   required from here\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1599:28: warning: ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>’ declared with greater visibility than the type of its field ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>::<record capture>’ [-Wattributes]\n 1599 |             with_internals([&](internals &internals) {\n      |                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1600 |                 auto &instances = record.module_local ? get_local_internals().registered_types_cpp\n      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1601 |                                                       : internals.registered_types_cpp;\n      |                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1602 |                 instances[std::type_index(typeid(type_alias))]\n      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1603 |                     = instances[std::type_index(typeid(type))];\n      |                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1604 |             });\n      |             ~               \n[6/7] c++ -MMD -MF /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/panther/pawX/skops.cpp -o /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[7/7] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o.d -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/panther/pawX/linear_cuda.cu -o /kaggle/working/panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pawX-0.0.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n\u001b[0mObtaining file:///kaggle/working/panther/pawX\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nInstalling collected packages: pawX\n  Attempting uninstall: pawX\n    Found existing installation: pawX 0.0.0\n    Uninstalling pawX-0.0.0:\n      Successfully uninstalled pawX-0.0.0\n  Running setup.py develop for pawX\nSuccessfully installed pawX-0.0.0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nprint(torch.__version__)\nimport triton\nprint(triton.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:20:20.818487Z","iopub.execute_input":"2025-05-06T15:20:20.818738Z","iopub.status.idle":"2025-05-06T15:20:22.516863Z","shell.execute_reply.started":"2025-05-06T15:20:20.818704Z","shell.execute_reply":"2025-05-06T15:20:22.516099Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu124\n3.2.0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working/panther\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:20:22.519820Z","iopub.execute_input":"2025-05-06T15:20:22.520110Z","iopub.status.idle":"2025-05-06T15:20:22.523339Z","shell.execute_reply.started":"2025-05-06T15:20:22.520091Z","shell.execute_reply":"2025-05-06T15:20:22.522789Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:20:22.524182Z","iopub.execute_input":"2025-05-06T15:20:22.524830Z","iopub.status.idle":"2025-05-06T15:20:22.661157Z","shell.execute_reply.started":"2025-05-06T15:20:22.524805Z","shell.execute_reply":"2025-05-06T15:20:22.660435Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/panther\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"%%writefile /kaggle/working/panther/panther/nn/conv2d.py\nimport math\nfrom typing import Any, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.nn import functional as F\nfrom torch.nn import init\n\nfrom panther.random import scaled_sign_sketch as gen_U\n\n\ndef mode4_unfold(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"Computes mode-4 matricization (unfolding along the last dimension).\"\"\"\n    return tensor.reshape(-1, tensor.shape[-1])  # (I4, I1 * I2 * I3)\n\n\nclass SketchedConv2dFunction(Function):\n    # Note that forward, setup_context, and backward are @staticmethods\n    @staticmethod\n    def forward(\n        input: torch.Tensor,\n        S1s: torch.Tensor,\n        S2s: torch.Tensor,\n        U1s: torch.Tensor,\n        U2s: torch.Tensor,\n        stride: Tuple[int, int],\n        padding: Tuple[int, int],\n        kernelSize: Tuple[int, int],\n        inshape,\n        bias: torch.Tensor,\n    ):\n        # in_channels, height, width = input.shape\n        _, dout = U1s[0].shape\n        hout = (inshape[2] + 2 * padding[0] - kernelSize[0]) // stride[0] + 1\n        wout = (inshape[3] + 2 * padding[1] - kernelSize[1]) // stride[1] + 1\n        input.transpose_(1, 2)\n        t = (\n            torch.einsum(\"nab,lbc,lcd->nlad\", input, S1s, U1s)\n            + torch.einsum(\"nab,lbc,lcd->nlad\", input, U2s.transpose(1, 2), S2s)\n        ).mean(dim=1)\n        t = t.view(inshape[0], dout, hout, wout)\n        return t + bias.view(1, dout, 1, 1)\n\n    @staticmethod\n    # inputs is a Tuple of all of the inputs passed to forward.\n    # output is the output of the forward().\n    def setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any):\n        input, S1s, S2s, U1s, U2s, stride, padding, kernelSize, inshap, bias = inputs\n        ctx.save_for_backward(\n            input,\n            S1s,\n            S2s,\n            U1s,\n            U2s,\n            torch.tensor(stride),\n            torch.tensor(padding),\n            torch.tensor(kernelSize),\n            torch.tensor(inshap),\n            bias,\n        )\n\n    @staticmethod\n    def backward(ctx: Any, *grad_output: Any) -> Any:\n        input, S1s, S2s, U1s, U2s, stride, padding, kernelSize, inshape, bias = (\n            ctx.saved_tensors\n        )\n        input.transpose_(1, 2)\n        num_terms, _, __ = S2s.shape\n        hout = grad_output[0].shape[2]\n        wout = grad_output[0].shape[3]\n        g_bias = grad_output[0].sum(dim=(0, 2, 3))\n        grad_output = grad_output[0].view(\n            grad_output[0].shape[0],\n            hout * wout,\n            grad_output[0].shape[1],\n        )\n        grad_output /= 2 * num_terms\n        g_S1s = torch.zeros_like(S1s)\n        g_S2s = torch.zeros_like(S2s)\n        g_S1s = torch.einsum(\n            \"nab,nbc,lcd->lad\", input, grad_output, U1s.transpose(1, 2)\n        )\n        g_S2s = torch.einsum(\"lab,nbc,ncd->lad\", U2s, input, grad_output)\n        gout = torch.einsum(\n            \"nab,lbc,lcd->nad\", grad_output, U1s.transpose(1, 2), S1s.transpose(1, 2)\n        ) + torch.einsum(\"nab,lbc,lcd->nad\", grad_output, S2s.transpose(1, 2), U2s)\n        fold = nn.Fold(\n            output_size=(inshape[2], inshape[3]),\n            kernel_size=(kernelSize[0], kernelSize[1]),\n            stride=stride,\n            padding=padding,\n        )\n        gout = gout.transpose(1, 2)\n        gout = fold(gout)\n\n        return (gout, g_S1s, g_S2s, None, None, None, None, None, None, g_bias)\n\n\nclass SKConv2d(nn.Module):\n    __constants__ = [\"in_features\", \"out_features\", \"num_terms\", \"low_rank\"]\n    in_features: int\n    out_features: int\n    num_terms: int\n    low_rank: int\n    S1s: torch.Tensor\n    S2s: torch.Tensor\n    U1s: torch.Tensor\n    U2s: torch.Tensor\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: Tuple = (3, 3),\n        stride: Tuple = (1, 1),\n        padding: Tuple = (1, 1),\n        num_terms: int = 6,\n        low_rank: int = 8,\n        dtype=None,\n        device=None,\n    ):\n        factory_kwargs = {\"dtype\": dtype, \"device\": device}\n        super(SKConv2d, self).__init__()\n        self.num_terms = num_terms\n        self.low_rank = low_rank\n        self.out_channels = out_channels\n        self.in_channels = in_channels\n        self.stride = stride if isinstance(stride, tuple) else (stride, stride)\n        self.padding = padding if isinstance(padding, tuple) else (padding, padding)\n        self.kernel_size = (\n            kernel_size\n            if isinstance(kernel_size, tuple)\n            else (kernel_size, kernel_size)\n        )\n        self.register_buffer(\n            \"U1s\",\n            torch.stack(\n                [\n                    gen_U(low_rank, out_channels, **factory_kwargs)\n                    for _ in range(num_terms)\n                ]\n            ),\n        )  # kxd1\n        self.register_buffer(\n            \"U2s\",\n            torch.stack(\n                [\n                    gen_U(\n                        low_rank * self.kernel_size[0] * self.kernel_size[1],\n                        in_channels * self.kernel_size[0] * self.kernel_size[1],\n                        **factory_kwargs,\n                    )\n                    for _ in range(num_terms)\n                ]\n            ),\n        )  # k h w x d2 h w\n        kernels = nn.Parameter(\n            torch.empty(\n                (in_channels, *self.kernel_size, out_channels), **factory_kwargs\n            )\n        )  # doutxdinxhxw\n        init.kaiming_uniform_(kernels, a=math.sqrt(5))\n        self.S1s = nn.Parameter(\n            torch.stack(\n                [\n                    mode4_unfold(torch.matmul(kernels, self.U1s[i].T))\n                    for i in range(num_terms)\n                ]\n            )\n        )  # d2xk\n        K_mat4 = kernels.view(\n            in_channels * self.kernel_size[0] * self.kernel_size[1], out_channels\n        )\n        self.S2s = nn.Parameter(\n            torch.stack(\n                [\n                    mode4_unfold(\n                        torch.matmul(self.U2s[i], K_mat4).view(\n                            low_rank, *self.kernel_size, out_channels\n                        )\n                    )\n                    for i in range(num_terms)\n                ]\n            )\n        )  #\n        self.bias = nn.Parameter(torch.empty(out_channels, **factory_kwargs))\n        fan_in, _ = init._calculate_fan_in_and_fan_out(kernels)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        init.uniform_(self.bias, -bound, bound)\n\n        # Register U1s and U2s as buffers since they are not learnable\n\n    def forward(self, x):\n        \"\"\"Forward pass of the SKConv2d layer.\"\"\"\n        # padd x\n        B, C, H, W = x.shape\n        if self.padding[0] > 0 or self.padding[1] > 0:\n            x = F.pad(\n                x, (self.padding[1], self.padding[1], self.padding[0], self.padding[0])\n            )\n        H_out = (x.shape[2] - self.kernel_size[0]) // self.stride[0] + 1\n        W_out = (x.shape[3] - self.kernel_size[1]) // self.stride[1] + 1\n        x_strided = x.as_strided(\n            size=(\n                x.shape[0],\n                x.shape[1],\n                H_out,\n                W_out,\n                self.kernel_size[0],\n                self.kernel_size[1],\n            ),\n            stride=(\n                x.stride(0),\n                x.stride(1),\n                x.stride(2) * self.stride[0],\n                x.stride(3) * self.stride[1],\n                x.stride(2),\n                x.stride(3),\n            ),\n        )\n        x_windows = x_strided.permute(0, 2, 3, 1, 4, 5)\n\n        x_windows = x_windows.reshape(\n            -1, self.kernel_size[0] * self.kernel_size[1] * self.in_channels\n        )\n        out1 = (\n            torch.einsum(\"nd,tdr,tro->no\", x_windows, self.S1s, self.U1s)\n            / self.num_terms\n        ) + self.bias\n        out2 = (\n            torch.einsum(\n                \"nd,tdr,tro->no\", x_windows, self.U2s.transpose(1, 2), self.S2s\n            )\n            / self.num_terms\n        )\n        return (\n            (out1 + out2 + self.bias)\n            .view(B, H_out, W_out, self.out_channels)\n            .permute(0, 3, 1, 2)\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:20:22.662153Z","iopub.execute_input":"2025-05-06T15:20:22.662350Z","iopub.status.idle":"2025-05-06T15:20:22.671493Z","shell.execute_reply.started":"2025-05-06T15:20:22.662331Z","shell.execute_reply":"2025-05-06T15:20:22.670754Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/panther/panther/nn/conv2d.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nfrom transformers import BertForMaskedLM, BertTokenizer\n\n# 1. Load pretrained tokenizer & model\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel     = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\nmodel.eval()\n\n# 2. Tokenize a sentence with a mask\ntext = \"Machine learning is the future of [MASK].\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# 3. Forward pass: yields logits over the full vocab for each position\nwith torch.no_grad():\n    outputs = model(**inputs)\nlogits = outputs.logits  # shape: (1, seq_len, vocab_size)\n\n# 4. Locate the [MASK] position\nmask_token_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\nbatch_idx, token_idx = mask_token_index\n\n# 5. Extract the logits for that position and pick the highest-scoring token\nmask_logits = logits[batch_idx, token_idx, :]\npredicted_token_id = mask_logits.argmax(dim=-1).item()\npredicted_token = tokenizer.decode([predicted_token_id])\n\nprint(f\"Filled mask: {predicted_token}\")  # e.g. “technology”","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:20:22.672268Z","iopub.execute_input":"2025-05-06T15:20:22.672517Z","iopub.status.idle":"2025-05-06T15:20:30.313751Z","shell.execute_reply.started":"2025-05-06T15:20:22.672494Z","shell.execute_reply":"2025-05-06T15:20:30.313087Z"}},"outputs":[{"name":"stderr","text":"2025-05-06 15:20:26.369298: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746544826.393841     337 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746544826.400921     337 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Filled mask: science\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!pip install botorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:20:30.314633Z","iopub.execute_input":"2025-05-06T15:20:30.315282Z","iopub.status.idle":"2025-05-06T15:20:33.477058Z","shell.execute_reply.started":"2025-05-06T15:20:30.315260Z","shell.execute_reply":"2025-05-06T15:20:33.476279Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: botorch in /usr/local/lib/python3.11/dist-packages (0.13.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from botorch) (4.13.1)\nRequirement already satisfied: pyre_extensions in /usr/local/lib/python3.11/dist-packages (from botorch) (0.0.32)\nRequirement already satisfied: gpytorch==1.14 in /usr/local/lib/python3.11/dist-packages (from botorch) (1.14)\nRequirement already satisfied: linear_operator==0.6 in /usr/local/lib/python3.11/dist-packages (from botorch) (0.6)\nRequirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from botorch) (2.6.0+cu124)\nRequirement already satisfied: pyro-ppl>=1.8.4 in /usr/local/lib/python3.11/dist-packages (from botorch) (1.9.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from botorch) (1.15.2)\nRequirement already satisfied: multipledispatch in /usr/local/lib/python3.11/dist-packages (from botorch) (1.0.0)\nRequirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from botorch) (3.6.0)\nRequirement already satisfied: jaxtyping in /usr/local/lib/python3.11/dist-packages (from gpytorch==1.14->botorch) (0.3.2)\nRequirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.11/dist-packages (from gpytorch==1.14->botorch) (1.3.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from gpytorch==1.14->botorch) (1.2.2)\nRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (3.4.0)\nRequirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (0.1.2)\nRequirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (1.13.1)\nRequirement already satisfied: typing-inspect in /usr/local/lib/python3.11/dist-packages (from pyre_extensions->botorch) (0.9.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2.4.1)\nRequirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from jaxtyping->gpytorch==1.14->botorch) (0.1.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.1->botorch) (3.0.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch==1.14->botorch) (1.4.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect->pyre_extensions->botorch) (1.0.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"%%writefile /kaggle/working/panther/panther/random.py\nimport torch\n\n# DISCLAIMER: THIS FILE NEEDS TO BE CHECKED FOR CORRECTNESS\n\n\ndef uniform_dense_sketch(m, n, device=None, dtype=None):\n    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n    return torch.empty(m, n, **factory_kwargs).uniform_(-1, 1)\n\n\ndef gaussian_dense_sketch(m, n, device=None, dtype=None):\n    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n    return torch.randn(m, n, **factory_kwargs)\n\n\ndef hadamard_sketch(m, device=None, dtype=None):\n    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n    if m & (m - 1) != 0:\n        raise ValueError(\"m must be a power of 2\")\n\n    H = torch.tensor([[1.0]])\n    while H.shape[0] < m:\n        H = torch.cat((torch.cat((H, H), dim=1), torch.cat((H, -H), dim=1)), dim=0)\n\n    return H / torch.sqrt(torch.tensor(m, **factory_kwargs))\n\n\ndef gaussian_orthonormal_sketch(m, n, device=None, dtype=None):\n    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n    return torch.qr(torch.randn(m, n, **factory_kwargs))[0]\n\n\ndef scaled_sign_sketch(m, n, device=None, dtype=None):\n    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n    return (torch.randint(0, 2, (m, n), **factory_kwargs) * 2 - 1) / torch.sqrt(\n        torch.tensor(m, **factory_kwargs)\n    )\n\n\ndef clarkson_woodruff_sketch(m, n, device=None, dtype=None):\n    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n    indices = torch.randint(0, m, (n,), **factory_kwargs)\n    signs = torch.randint(0, 2, (n,), **factory_kwargs) * 2 - 1\n    sketch = torch.zeros(m, n, **factory_kwargs)\n    sketch[indices, torch.arange(n)] = signs\n    return sketch\n\n\ndef sparse_sign_embeddings_sketch(m, n, sparsity=0.1):\n    mask = torch.rand(m, n) < sparsity\n    signs = torch.randint(0, 2, (m, n)) * 2 - 1\n    return mask.float() * signs.float()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:20:33.478078Z","iopub.execute_input":"2025-05-06T15:20:33.478324Z","iopub.status.idle":"2025-05-06T15:20:33.484519Z","shell.execute_reply.started":"2025-05-06T15:20:33.478302Z","shell.execute_reply":"2025-05-06T15:20:33.483803Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/panther/panther/random.py\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!pip install botorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:20:33.485329Z","iopub.execute_input":"2025-05-06T15:20:33.485561Z","iopub.status.idle":"2025-05-06T15:20:36.727746Z","shell.execute_reply.started":"2025-05-06T15:20:33.485545Z","shell.execute_reply":"2025-05-06T15:20:36.726892Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: botorch in /usr/local/lib/python3.11/dist-packages (0.13.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from botorch) (4.13.1)\nRequirement already satisfied: pyre_extensions in /usr/local/lib/python3.11/dist-packages (from botorch) (0.0.32)\nRequirement already satisfied: gpytorch==1.14 in /usr/local/lib/python3.11/dist-packages (from botorch) (1.14)\nRequirement already satisfied: linear_operator==0.6 in /usr/local/lib/python3.11/dist-packages (from botorch) (0.6)\nRequirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from botorch) (2.6.0+cu124)\nRequirement already satisfied: pyro-ppl>=1.8.4 in /usr/local/lib/python3.11/dist-packages (from botorch) (1.9.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from botorch) (1.15.2)\nRequirement already satisfied: multipledispatch in /usr/local/lib/python3.11/dist-packages (from botorch) (1.0.0)\nRequirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from botorch) (3.6.0)\nRequirement already satisfied: jaxtyping in /usr/local/lib/python3.11/dist-packages (from gpytorch==1.14->botorch) (0.3.2)\nRequirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.11/dist-packages (from gpytorch==1.14->botorch) (1.3.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from gpytorch==1.14->botorch) (1.2.2)\nRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (3.4.0)\nRequirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (0.1.2)\nRequirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (1.13.1)\nRequirement already satisfied: typing-inspect in /usr/local/lib/python3.11/dist-packages (from pyre_extensions->botorch) (0.9.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2.4.1)\nRequirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from jaxtyping->gpytorch==1.14->botorch) (0.1.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.1->botorch) (3.0.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch==1.14->botorch) (1.4.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect->pyre_extensions->botorch) (1.0.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Import components\nfrom panther.tuner.SkAutoTuner import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:20:36.728784Z","iopub.execute_input":"2025-05-06T15:20:36.729048Z","iopub.status.idle":"2025-05-06T15:20:37.172337Z","shell.execute_reply.started":"2025-05-06T15:20:36.729023Z","shell.execute_reply":"2025-05-06T15:20:37.171666Z"}},"outputs":[{"name":"stdout","text":"Warning: botorch is not available. Install with: pip install botorch\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ModelVisualizer.print_module_tree(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:20:37.173027Z","iopub.execute_input":"2025-05-06T15:20:37.173216Z","iopub.status.idle":"2025-05-06T15:20:37.176391Z","shell.execute_reply.started":"2025-05-06T15:20:37.173201Z","shell.execute_reply":"2025-05-06T15:20:37.175947Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# the normal without changing bert","metadata":{}},{"cell_type":"code","source":"# import os\n# import time\n# import copy\n# import torch\n# import numpy as np\n# from torch.utils.data import DataLoader, Dataset\n# from tqdm import tqdm\n# from transformers import BertForMaskedLM, BertTokenizer\n\n# # Import components\n# from panther.tuner.SkAutoTuner import (\n#     SKAutoTuner, \n#     LayerConfig, \n#     TuningConfigs,\n#     GridSearch,\n#     RandomSearch, \n#     ModelVisualizer\n# )\n\n# # Setting up device\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {device}\")\n\n# ##################################### HELPERS #######################################\n\n# def dump_tensor_info(tensor, name=\"Tensor\"):\n#     \"\"\"Print details about a tensor\"\"\"\n#     print(f\"{name}: shape={tensor.shape}, dtype={tensor.dtype}, device={tensor.device}\")\n#     print(f\"  - Values: min={tensor.min().item():.4f}, max={tensor.max().item():.4f}, mean={tensor.mean().item():.4f}\")\n#     print(f\"  - First few values: {tensor.flatten()[:5]}\")\n\n# def measure_time(func, *args, n_runs=20, warmup=5):\n#     \"\"\"Measure execution time of a function\"\"\"\n#     # Warmup\n#     for _ in range(warmup):\n#         func(*args)\n    \n#     # Timed runs\n#     torch.cuda.synchronize() if torch.cuda.is_available() else None\n#     start = time.time()\n#     for _ in range(n_runs):\n#         func(*args)\n#         torch.cuda.synchronize() if torch.cuda.is_available() else None\n#     end = time.time()\n    \n#     return (end - start) / n_runs\n\n# def measure_memory(model, input_tensor):\n#     \"\"\"Measure peak memory usage of a model during inference\"\"\"\n#     if not torch.cuda.is_available():\n#         return 0  # Cannot measure CUDA memory on CPU\n    \n#     # Clear cache\n#     torch.cuda.empty_cache()\n#     torch.cuda.reset_peak_memory_stats()\n    \n#     # Run inference\n#     with torch.no_grad():\n#         model(**input_tensor)\n    \n#     # Get peak memory\n#     return torch.cuda.max_memory_allocated() / (1024 * 1024)  # Convert to MB\n\n# class MaskedTextDataset(Dataset):\n#     \"\"\"Dataset for masked language modeling\"\"\"\n#     def __init__(self, texts, tokenizer, max_length=128):\n#         self.texts = texts\n#         self.tokenizer = tokenizer\n#         self.max_length = max_length\n        \n#     def __len__(self):\n#         return len(self.texts)\n    \n#     def __getitem__(self, idx):\n#         text = self.texts[idx]\n#         encoding = self.tokenizer(\n#             text,\n#             return_special_tokens_mask=True,\n#             max_length=self.max_length,\n#             padding=\"max_length\",\n#             truncation=True,\n#             return_tensors=\"pt\"\n#         )\n        \n#         # Create input_ids with masks\n#         input_ids = encoding.input_ids.clone().squeeze(0)\n#         special_tokens_mask = encoding.special_tokens_mask.squeeze(0).bool()\n        \n#         # Create labels (clone of input_ids)\n#         labels = input_ids.clone()\n        \n#         # Find positions eligible for masking (not special tokens)\n#         mask_positions = (~special_tokens_mask).nonzero(as_tuple=True)[0]\n        \n#         # Randomly mask 15% of eligible tokens\n#         num_to_mask = max(1, int(0.15 * len(mask_positions)))\n#         mask_indices = np.random.choice(mask_positions.tolist(), size=num_to_mask, replace=False)\n#         input_ids[mask_indices] = self.tokenizer.mask_token_id\n        \n#         # Create attention mask\n#         attention_mask = encoding.attention_mask.squeeze(0)\n        \n#         # Create return dictionary\n#         batch = {\n#             \"input_ids\": input_ids,\n#             \"attention_mask\": attention_mask,\n#             \"labels\": labels\n#         }\n        \n#         return batch\n\n# def evaluate_model(model, dataloader):\n#     \"\"\"Evaluate model accuracy on a dataset\"\"\"\n#     model.eval()\n#     total_loss = 0\n#     total_samples = 0\n    \n#     with torch.no_grad():\n#         for batch in dataloader:\n#             # Move batch to device\n#             batch = {k: v.to(device) for k, v in batch.items()}\n            \n#             # Forward pass\n#             outputs = model(**batch)\n#             loss = outputs.loss\n            \n#             # Accumulate statistics\n#             batch_size = batch[\"input_ids\"].size(0)\n#             total_loss += loss.item() * batch_size\n#             total_samples += batch_size\n    \n#     return total_loss / total_samples\n\n# def get_data():\n#     \"\"\"Prepare dataset for BERT testing\"\"\"\n#     print(\"Preparing BERT test dataset...\")\n    \n#     # Sample texts for testing\n#     texts = [\n#         \"Machine learning is the study of computer algorithms that improve automatically through experience.\",\n#         \"Deep learning is part of a broader family of machine learning methods based on artificial neural networks.\",\n#         \"Natural language processing is a subfield of linguistics, computer science, and artificial intelligence.\",\n#         \"Transformers have emerged as a powerful deep learning architecture for natural language processing tasks.\",\n#         \"BERT is a transformer-based machine learning technique for natural language processing pre-training.\"\n#     ]\n    \n#     # Add more texts to the dataset for more robust testing\n#     more_texts = [\n#         \"The transformer architecture uses self-attention mechanisms to process sequential data effectively.\",\n#         \"Pre-trained language models can be fine-tuned on specific downstream tasks with less data.\",\n#         \"Language model pre-training has resulted in significant advances in many natural language tasks.\",\n#         \"Transfer learning enables models to leverage knowledge from one domain to perform well in another.\",\n#         \"Masked language modeling is a self-supervised technique to train language models.\"\n#     ]\n#     texts.extend(more_texts)\n    \n#     # Tokenizer\n#     tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n    \n#     # Create dataset\n#     dataset = MaskedTextDataset(texts, tokenizer)\n    \n#     # Create data loader\n#     dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n    \n#     # Create a single batch for memory testing\n#     memory_batch = {k: v.to(device) for k, v in next(iter(dataloader)).items()}\n#     memory_batch = {k: v.repeat(4, 1) for k, v in memory_batch.items()}  # Make batch size 4\n    \n#     return tokenizer, dataloader, memory_batch\n\n# def fill_mask_test(model, tokenizer, text=\"The capital of France is [MASK].\"):\n#     \"\"\"Test mask filling capability\"\"\"\n#     # Replace [MASK] with actual mask token if needed\n#     if \"[MASK]\" in text:\n#         text = text.replace(\"[MASK]\", tokenizer.mask_token)\n    \n#     # Tokenize\n#     inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    \n#     # Find mask token position\n#     mask_token_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n    \n#     # Forward pass\n#     with torch.no_grad():\n#         outputs = model(**inputs)\n#     logits = outputs.logits\n    \n#     # Get predictions for mask position\n#     if len(mask_token_index[0]) > 0:\n#         batch_idx, token_idx = mask_token_index\n#         mask_logits = logits[batch_idx, token_idx, :]\n        \n#         # Get top 5 predictions\n#         topk_values, topk_indices = torch.topk(mask_logits, 5, dim=1)\n        \n#         # Convert to tokens\n#         topk_tokens = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in topk_indices[0]]\n        \n#         return topk_tokens\n#     else:\n#         return [\"No mask token found\"]\n\n# def test_bert_optimization():\n#     \"\"\"Test SKAutoTuner on BERT model's linear layers\"\"\"\n    \n#     # Load pre-trained model\n#     model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)\n#     model.eval()\n    \n#     # Create a copy of the model for reference\n#     orig_model = copy.deepcopy(model)\n    \n#     # Get data for testing\n#     tokenizer, val_loader, memory_batch = get_data()\n    \n#     print(\"\\n===== Original Model Structure =====\")\n#     ModelVisualizer.print_module_tree(model)\n    \n#     # Create an evaluation function for the model\n#     def acc_eval_func(model):\n#         \"\"\"Evaluation function based on MLM loss\"\"\"\n#         loss = evaluate_model(model, val_loader)\n#         # Convert loss to accuracy-like score (higher is better)\n#         # Original model's loss is used as reference\n#         orig_loss = evaluate_model(orig_model, val_loader)\n        \n#         # Score based on relative loss difference (higher is better)\n#         score = (orig_loss - loss) / orig_loss\n        \n#         print(f\"MLM Loss: {loss:.4f} (original: {orig_loss:.4f}, relative diff: {score:.4f})\")\n#         return score\n    \n#     # Create a separate speed evaluation function\n#     def speed_eval_func(model):\n#         \"\"\"Speed evaluation function\"\"\"\n#         def infer(model, inputs):\n#             with torch.no_grad():\n#                 return model(**inputs)\n        \n#         # Higher is better (inverse of time)\n#         throughput = 1.0 / measure_time(infer, model, memory_batch, n_runs=10)\n#         print(f\"Inference speed: {throughput:.2f} samples/sec\")\n#         return throughput\n    \n#     # Test original model performance\n#     print(\"\\nBaseline BERT model:\")\n#     baseline_loss = evaluate_model(model, val_loader)\n#     baseline_speed = speed_eval_func(orig_model)\n#     baseline_memory = measure_memory(orig_model, memory_batch)\n    \n#     print(f\"MLM Loss: {baseline_loss:.4f}\")\n#     print(f\"Baseline model memory usage: {baseline_memory:.2f} MB\")\n#     print(f\"Baseline model speed: {baseline_speed:.2f} samples/sec\")\n    \n#     # Test mask filling capability\n#     print(\"\\nTesting mask filling on original model:\")\n#     test_sentence = \"The capital of France is [MASK].\"\n#     predictions = fill_mask_test(model, tokenizer, test_sentence)\n#     print(f\"Sentence: {test_sentence}\")\n#     print(f\"Top 5 predictions: {predictions}\")\n    \n#     # Strategy 1: Optimizing decoder linear layer in BertMLMHead\n#     print(\"\\n===== Strategy 1: Optimizing decoder linear layer =====\")\n    \n#     # Create configs to tune the decoder layer in MLM head\n#     configs_strategy1 = TuningConfigs([\n#         LayerConfig(\n#             # Target the decoder linear layer in the MLM head\n#             layer_names={\"pattern\": \"cls.predictions.decoder\"},\n#             params={\n#                 \"num_terms\": [1, 2, 3],\n#                 \"low_rank\": [16, 32, 64, 128],\n#             },\n#             separate=True\n#         ),\n#     ])\n    \n#     # Calculate accuracy threshold\n#     accuracy_threshold = -0.05  # Allow 5% reduction in accuracy\n#     print(f\"Setting accuracy threshold to {accuracy_threshold:.4f}\")\n    \n#     # Create tuner focusing on the decoder linear layer\n#     tuner_strategy1 = SKAutoTuner(\n#         model=copy.deepcopy(model),\n#         configs=configs_strategy1,\n#         accuracy_eval_func=acc_eval_func,\n#         search_algorithm=GridSearch(),\n#         verbose=True,\n#         accuracy_threshold=accuracy_threshold,\n#         optmization_eval_func=speed_eval_func\n#     )\n    \n#     # Run tuning\n#     print(\"\\nRunning decoder layer tuning...\")\n#     best_params = tuner_strategy1.tune()\n#     print(f\"Best parameters: {best_params}\")\n    \n#     # Apply best parameters\n#     tuned_model_strategy1 = tuner_strategy1.apply_best_params()\n    \n#     print(\"\\n===== Tuned Model Structure (Strategy 1) =====\")\n#     ModelVisualizer.print_module_tree(tuned_model_strategy1)\n    \n#     # Test the tuned model\n#     print(\"\\nEvaluating decoder-tuned model:\")\n#     final_loss = evaluate_model(tuned_model_strategy1, val_loader)\n#     final_speed = speed_eval_func(tuned_model_strategy1)\n#     final_memory = measure_memory(tuned_model_strategy1, memory_batch)\n    \n#     print(f\"MLM Loss: {final_loss:.4f} (original: {baseline_loss:.4f})\")\n#     print(f\"Speed: {final_speed:.2f} samples/sec (original: {baseline_speed:.2f})\")\n#     print(f\"Memory: {final_memory:.2f} MB (original: {baseline_memory:.2f})\")\n    \n#     # Strategy 2: Optimizing transform dense layer in BertMLMHead\n#     print(\"\\n===== Strategy 2: Optimizing transform dense layer =====\")\n    \n#     # Create configs to tune the dense layer in MLM head's transform module\n#     configs_strategy2 = TuningConfigs([\n#         LayerConfig(\n#             # Target the dense linear layer in the transform module\n#             layer_names={\"pattern\": \"cls.predictions.transform.dense\"},\n#             params={\n#                 \"num_terms\": [1, 2],\n#                 \"low_rank\": [16, 32, 64, 128],\n#             },\n#             separate=True\n#         ),\n#     ])\n    \n#     # Create tuner focusing on the transform dense layer\n#     tuner_strategy2 = SKAutoTuner(\n#         model=copy.deepcopy(model),\n#         configs=configs_strategy2,\n#         accuracy_eval_func=acc_eval_func,\n#         search_algorithm=GridSearch(),\n#         verbose=True,\n#         accuracy_threshold=accuracy_threshold,\n#         optmization_eval_func=speed_eval_func\n#     )\n    \n#     # Run tuning\n#     print(\"\\nRunning transform dense layer tuning...\")\n#     best_params = tuner_strategy2.tune()\n#     print(f\"Best parameters: {best_params}\")\n    \n#     # Apply best parameters\n#     tuned_model_strategy2 = tuner_strategy2.apply_best_params()\n    \n#     print(\"\\n===== Tuned Model Structure (Strategy 2) =====\")\n#     ModelVisualizer.print_module_tree(tuned_model_strategy2)\n    \n#     # Test the tuned model\n#     print(\"\\nEvaluating transform-tuned model:\")\n#     final_loss = evaluate_model(tuned_model_strategy2, val_loader)\n#     final_speed = speed_eval_func(tuned_model_strategy2)\n#     final_memory = measure_memory(tuned_model_strategy2, memory_batch)\n    \n#     print(f\"MLM Loss: {final_loss:.4f} (original: {baseline_loss:.4f})\")\n#     print(f\"Speed: {final_speed:.2f} samples/sec (original: {baseline_speed:.2f})\")\n#     print(f\"Memory: {final_memory:.2f} MB (original: {baseline_memory:.2f})\")\n    \n#     # Strategy 3: Optimizing both linear layers in the MLM head\n#     print(\"\\n===== Strategy 3: Optimizing both MLM head linear layers =====\")\n    \n#     # Create configs to tune both linear layers together\n#     configs_strategy3 = TuningConfigs([\n#         LayerConfig(\n#             # Target both linear layers in the MLM head\n#             layer_names=[\n#                 \"cls.predictions.decoder\",\n#                 \"cls.predictions.transform.dense\"\n#             ],\n#             params={\n#                 \"num_terms\": [1, 2],\n#                 \"low_rank\": [16, 32, 64, 128],\n#             },\n#             separate=False  # Tune as a group\n#         ),\n#     ])\n    \n#     # Create tuner for both layers together\n#     tuner_strategy3 = SKAutoTuner(\n#         model=copy.deepcopy(model),\n#         configs=configs_strategy3,\n#         accuracy_eval_func=acc_eval_func,\n#         search_algorithm=GridSearch(),  # Use random search with limited trials\n#         verbose=True,\n#         accuracy_threshold=accuracy_threshold,\n#         optmization_eval_func=speed_eval_func\n#     )\n    \n#     # Run tuning\n#     print(\"\\nRunning combined MLM head layers tuning...\")\n#     best_params = tuner_strategy3.tune()\n#     print(f\"Best parameters: {best_params}\")\n    \n#     # Apply best parameters\n#     tuned_model_strategy3 = tuner_strategy3.apply_best_params()\n    \n#     print(\"\\n===== Tuned Model Structure (Strategy 3) =====\")\n#     ModelVisualizer.print_module_tree(tuned_model_strategy3)\n    \n#     # Test the tuned model\n#     print(\"\\nEvaluating combined-tuning model:\")\n#     final_loss = evaluate_model(tuned_model_strategy3, val_loader)\n#     final_speed = speed_eval_func(tuned_model_strategy3)\n#     final_memory = measure_memory(tuned_model_strategy3, memory_batch)\n    \n#     print(f\"MLM Loss: {final_loss:.4f} (original: {baseline_loss:.4f})\")\n#     print(f\"Speed: {final_speed:.2f} samples/sec (original: {baseline_speed:.2f})\")\n#     print(f\"Memory: {final_memory:.2f} MB (original: {baseline_memory:.2f})\")\n    \n#     # Test mask filling capability on the final tuned model\n#     print(\"\\nTesting mask filling on tuned model:\")\n#     predictions = fill_mask_test(tuned_model_strategy3, tokenizer, test_sentence)\n#     print(f\"Sentence: {test_sentence}\")\n#     print(f\"Top 5 predictions: {predictions}\")\n    \n#     # Final comparison table\n#     print(\"\\n===== Performance Comparison =====\")\n#     print(\"Model Version | MLM Loss | Speed (samples/sec) | Memory (MB)\")\n#     print(\"-\" * 65)\n#     print(f\"Original     | {baseline_loss:.4f} | {baseline_speed:.2f} | {baseline_memory:.2f}\")\n    \n#     # Get metrics for all tuned models\n#     models = [tuned_model_strategy1, tuned_model_strategy2, tuned_model_strategy3]\n#     names = [\"Decoder-Tuned\", \"Transform-Tuned\", \"Combined-Tuned\"]\n    \n#     for name, tuned_model in zip(names, models):\n#         loss = evaluate_model(tuned_model, val_loader)\n#         speed = speed_eval_func(tuned_model)\n#         memory = measure_memory(tuned_model, memory_batch)\n#         print(f\"{name:13} | {loss:.4f} | {speed:.2f} | {memory:.2f}\")\n\n# if __name__ == \"__main__\":\n#     import copy  # Used for deep copying models\n    \n#     # Run the BERT optimization test\n#     print(\"\\nRunning BERT optimization test with SKAutoTuner...\")\n#     test_bert_optimization()\n    \n#     print(\"\\nTest completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:20:37.177051Z","iopub.execute_input":"2025-05-06T15:20:37.177237Z","iopub.status.idle":"2025-05-06T15:20:37.195492Z","shell.execute_reply.started":"2025-05-06T15:20:37.177223Z","shell.execute_reply":"2025-05-06T15:20:37.194675Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import os\nimport time\nimport copy\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nfrom transformers import BertForMaskedLM, BertTokenizer\nimport random\nimport torch.nn.functional as F\n\n# Import components\nfrom panther.tuner.SkAutoTuner import (\n    SKAutoTuner, \n    LayerConfig, \n    TuningConfigs,\n    GridSearch,\n    RandomSearch, \n    ModelVisualizer\n)\n\n# Set seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    print(f\"Random seed set to {seed} for reproducibility\")\n\n# Setting up device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Call set_seed early in the script\nset_seed(42)\n\n##################################### HELPERS #######################################\n\ndef count_parameters(model):\n    \"\"\"Count trainable parameters in the model\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef model_size_info(model):\n    \"\"\"Get detailed size information about the model\"\"\"\n    total_params = count_parameters(model)\n    \n    # Get layer-wise parameter counts for important components\n    layer_params = {}\n    \n    # Check BERT layers\n    if hasattr(model, 'bert') and hasattr(model.bert, 'encoder'):\n        for i, layer in enumerate(model.bert.encoder.layer):\n            layer_params[f'bert.encoder.layer.{i}'] = sum(p.numel() for p in layer.parameters() if p.requires_grad)\n    \n    # Check MLM head\n    if hasattr(model, 'cls'):\n        if hasattr(model.cls, 'predictions'):\n            if hasattr(model.cls.predictions, 'transform'):\n                layer_params['cls.predictions.transform'] = sum(\n                    p.numel() for p in model.cls.predictions.transform.parameters() if p.requires_grad)\n            if hasattr(model.cls.predictions, 'decoder'):\n                layer_params['cls.predictions.decoder'] = sum(\n                    p.numel() for p in model.cls.predictions.decoder.parameters() if p.requires_grad)\n    \n    return {\n        \"total_params\": total_params,\n        \"total_params_millions\": total_params / 1e6,\n        \"layer_params\": layer_params\n    }\n\ndef dump_tensor_info(tensor, name=\"Tensor\"):\n    \"\"\"Print details about a tensor\"\"\"\n    print(f\"{name}: shape={tensor.shape}, dtype={tensor.dtype}, device={tensor.device}\")\n    print(f\"  - Values: min={tensor.min().item():.4f}, max={tensor.max().item():.4f}, mean={tensor.mean().item():.4f}\")\n    print(f\"  - First few values: {tensor.flatten()[:5]}\")\n\ndef measure_time_with_stats(func, *args, n_runs=20, warmup=5):\n    \"\"\"Measure execution time of a function with proper GPU synchronization and report statistics\"\"\"\n    # Clear cache first\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n    \n    # Warmup\n    for _ in range(warmup):\n        result = func(*args)\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n    \n    # Timed runs\n    times = []\n    for _ in range(n_runs):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        \n        start = time.time()\n        result = func(*args)\n        \n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        \n        end = time.time()\n        times.append(end - start)\n    \n    # Calculate statistics\n    times = np.array(times)\n    mean_time = np.mean(times)\n    std_time = np.std(times)\n    \n    return {\n        \"mean\": mean_time,\n        \"std\": std_time,\n        \"min\": np.min(times),\n        \"max\": np.max(times),\n        \"samples_per_sec\": 1.0 / mean_time,\n        \"samples_per_sec_std\": std_time / (mean_time * mean_time)\n    }\n\ndef measure_memory(model, input_tensor):\n    \"\"\"Measure peak memory usage of a model during inference\"\"\"\n    if not torch.cuda.is_available():\n        return 0  # Cannot measure CUDA memory on CPU\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    \n    # Run inference\n    with torch.no_grad():\n        model(**input_tensor)\n    \n    # Get peak memory\n    return torch.cuda.max_memory_allocated() / (1024 * 1024)  # Convert to MB\n\nclass MaskedTextDataset(Dataset):\n    \"\"\"Dataset for masked language modeling\"\"\"\n    def __init__(self, texts, tokenizer, max_length=128):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        encoding = self.tokenizer(\n            text,\n            return_special_tokens_mask=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        # Create input_ids with masks\n        input_ids = encoding.input_ids.clone().squeeze(0)\n        special_tokens_mask = encoding.special_tokens_mask.squeeze(0).bool()\n        \n        # Create labels (clone of input_ids)\n        labels = input_ids.clone()\n        \n        # Find positions eligible for masking (not special tokens)\n        mask_positions = (~special_tokens_mask).nonzero(as_tuple=True)[0]\n        \n        # Randomly mask 15% of eligible tokens\n        num_to_mask = max(1, int(0.15 * len(mask_positions)))\n        mask_indices = np.random.choice(mask_positions.tolist(), size=num_to_mask, replace=False)\n        input_ids[mask_indices] = self.tokenizer.mask_token_id\n        \n        # Create attention mask\n        attention_mask = encoding.attention_mask.squeeze(0)\n        \n        # Create return dictionary\n        batch = {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels\n        }\n        \n        return batch\n\ndef evaluate_model_with_stats(model, dataloader, tokenizer=None, n_runs=3):\n    \"\"\"Evaluate model accuracy and loss on a dataset with multiple runs for statistics\"\"\"\n    all_results = []\n    \n    for run in range(n_runs):\n        model.eval()\n        total_loss = 0\n        total_correct = 0\n        total_predictions = 0\n        total_samples = 0\n        \n        with torch.no_grad():\n            for batch in dataloader:\n                # Move batch to device\n                batch = {k: v.to(device) for k, v in batch.items()}\n                \n                # Forward pass\n                outputs = model(**batch)\n                loss = outputs.loss\n                \n                # Calculate accuracy if tokenizer is provided\n                if tokenizer is not None:\n                    predictions = outputs.logits.argmax(dim=-1)\n                    labels = batch[\"labels\"]\n                    \n                    # Only consider masked positions for accuracy calculation\n                    masked_positions = (batch[\"input_ids\"] == tokenizer.mask_token_id)\n                    if masked_positions.sum() > 0:\n                        masked_predictions = predictions[masked_positions]\n                        masked_labels = labels[masked_positions]\n                        correct = (masked_predictions == masked_labels).sum().item()\n                        total_correct += correct\n                        total_predictions += masked_positions.sum().item()\n                \n                # Accumulate loss statistics\n                batch_size = batch[\"input_ids\"].size(0)\n                total_loss += loss.item() * batch_size\n                total_samples += batch_size\n        \n        avg_loss = total_loss / total_samples\n        mlm_accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n        \n        all_results.append({\n            \"loss\": avg_loss,\n            \"accuracy\": mlm_accuracy if tokenizer is not None else None\n        })\n    \n    # Compute statistics across runs\n    losses = [res[\"loss\"] for res in all_results]\n    accuracies = [res[\"accuracy\"] for res in all_results] if tokenizer is not None else None\n    \n    results = {\n        \"loss_mean\": np.mean(losses),\n        \"loss_std\": np.std(losses),\n        \"accuracy_mean\": np.mean(accuracies) if accuracies else None,\n        \"accuracy_std\": np.std(accuracies) if accuracies else None,\n        \"runs\": all_results\n    }\n    \n    return results\n\ndef get_data():\n    \"\"\"Prepare dataset for BERT testing\"\"\"\n    print(\"Preparing BERT test dataset...\")\n    \n    # Sample texts for testing (original set)\n    texts = [\n        \"Machine learning is the study of computer algorithms that improve automatically through experience.\",\n        \"Deep learning is part of a broader family of machine learning methods based on artificial neural networks.\",\n        \"Natural language processing is a subfield of linguistics, computer science, and artificial intelligence.\",\n        \"Transformers have emerged as a powerful deep learning architecture for natural language processing tasks.\",\n        \"BERT is a transformer-based machine learning technique for natural language processing pre-training.\"\n    ]\n    \n    # Add more texts to the dataset for more robust testing\n    more_texts = [\n        \"The transformer architecture uses self-attention mechanisms to process sequential data effectively.\",\n        \"Pre-trained language models can be fine-tuned on specific downstream tasks with less data.\",\n        \"Language model pre-training has resulted in significant advances in many natural language tasks.\",\n        \"Transfer learning enables models to leverage knowledge from one domain to perform well in another.\",\n        \"Masked language modeling is a self-supervised technique to train language models.\"\n    ]\n    texts.extend(more_texts)\n    \n    # Add more realistic data for better evaluation\n    additional_texts = []\n    \n    # Use WikiText-2 chunks if available, or more synthetic examples\n    try:\n        from datasets import load_dataset\n        print(\"Loading WikiText dataset for more realistic evaluation...\")\n        wiki_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n        # Take chunks of 100-200 characters, clean them, and add to texts\n        for item in wiki_dataset:\n            text = item['text'].strip()\n            if 100 <= len(text) <= 200 and len(text.split()) > 10:\n                additional_texts.append(text)\n                if len(additional_texts) >= 90:  # Add 90 more examples to get 100 total\n                    break\n        print(f\"Added {len(additional_texts)} examples from WikiText dataset\")\n    except Exception as e:\n        # If datasets package is not available, generate synthetic text\n        print(f\"WikiText dataset not available ({str(e)}), using synthetic examples\")\n        additional_sentences = [\n            \"The transformer model has revolutionized natural language processing with its attention mechanism.\",\n            \"Neural networks can learn complex patterns from large amounts of training data.\",\n            \"Word embeddings map words to vectors in a high-dimensional space to capture semantic meaning.\",\n            \"Recurrent neural networks process sequential data by maintaining a hidden state.\",\n            \"Attention mechanisms allow models to focus on relevant parts of the input sequence.\",\n            \"Backpropagation is an algorithm for efficiently computing gradients in neural networks.\",\n            \"Gradient descent optimizes neural network parameters by iteratively updating weights.\",\n            \"Convolutional neural networks are particularly effective for image recognition tasks.\",\n            \"Regularization techniques help prevent overfitting in machine learning models.\",\n            \"Transfer learning leverages knowledge from pre-trained models to improve performance.\",\n            \"Long Short-Term Memory networks are designed to handle the vanishing gradient problem.\",\n            \"Encoder-decoder architectures are commonly used for sequence-to-sequence tasks.\",\n            \"Tokenization is the process of converting text into discrete tokens for processing.\",\n            \"Fine-tuning adapts pre-trained models to specific downstream tasks with less data.\",\n            \"The softmax function converts a vector of values into a probability distribution.\",\n            \"Cross-entropy loss is commonly used for training classification models.\",\n            \"Dropout randomly deactivates neurons during training to prevent co-adaptation.\",\n            \"Batch normalization stabilizes and accelerates training of deep neural networks.\",\n            \"Reinforcement learning trains agents through interaction with an environment.\",\n            \"Semi-supervised learning combines labeled and unlabeled data for training.\",\n            \"One-hot encoding represents categorical variables as binary vectors.\",\n            \"Dimensionality reduction techniques compress data while preserving information.\",\n            \"Ensemble methods combine multiple models to improve prediction accuracy.\",\n            \"Generative adversarial networks consist of generator and discriminator components.\",\n            \"Autoencoders learn efficient representations by reconstructing their input data.\",\n            \"The curse of dimensionality refers to challenges in high-dimensional spaces.\",\n            \"Decision trees recursively partition data based on feature values.\",\n            \"Random forests are ensembles of decision trees with randomized feature selection.\",\n            \"Support vector machines find the optimal hyperplane to separate data classes.\",\n            \"K-means clustering groups data points based on similarity metrics.\",\n            \"Principal component analysis identifies directions of maximum variance in data.\",\n            \"Bias-variance tradeoff is a fundamental concept in machine learning generalization.\",\n            \"Feature engineering transforms raw data into features suitable for models.\",\n            \"Hyperparameter tuning optimizes model configuration for best performance.\",\n            \"Cross-validation assesses model performance on different data subsets.\",\n            \"The Internet of Things connects everyday devices to the global network.\",\n            \"Cloud computing provides on-demand access to computing resources over the internet.\",\n            \"Blockchain technology enables secure, decentralized transaction records.\",\n            \"Quantum computing leverages quantum mechanics for computational tasks.\",\n            \"Edge computing processes data near its source rather than in a centralized location.\"\n        ]\n        # Take up to 90 synthetic examples\n        additional_texts.extend(additional_sentences[:90])\n    \n    texts.extend(additional_texts[:90])  # Add up to 90 more texts\n    print(f\"Created dataset with {len(texts)} examples\")\n    \n    # Tokenizer\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n    \n    # Create dataset\n    dataset = MaskedTextDataset(texts, tokenizer)\n    \n    # Create data loader with batch size that's a multiple of 16 for Tensor Core optimization\n    batch_size = 16  # Changed from 2 to 16 to enable Tensor Core optimizations\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    # Create a single batch for memory testing (also multiple of 16)\n    memory_batch = {k: v.to(device) for k, v in next(iter(dataloader)).items()}\n    # No need to expand since we already have batch size 16\n    \n    return tokenizer, dataloader, memory_batch\n\ndef get_data_varied_lengths(seq_lengths=[128, 256, 384, 512]):\n    \"\"\"Prepare datasets with varying sequence lengths for scaling tests\"\"\"\n    print(f\"Preparing BERT test datasets with varying lengths: {seq_lengths}\")\n    \n    # Sample texts (use longer texts for this test)\n    texts = []\n    \n    # Try to load more complex texts from WikiText\n    try:\n        from datasets import load_dataset\n        print(\"Loading WikiText dataset for sequence length tests...\")\n        wiki_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n        \n        # Take chunks of various lengths\n        for item in wiki_dataset:\n            text = item['text'].strip()\n            if len(text) > 50 and text not in texts:  # Ensure it's a substantive text\n                texts.append(text)\n                if len(texts) >= 100:  # Get 100 examples\n                    break\n    except Exception as e:\n        # Fallback to synthetic data\n        print(f\"WikiText dataset not available ({str(e)}), using synthetic examples\")\n        # Generate longer synthetic texts by repeating and combining existing ones\n        base_texts = [\n            \"Machine learning is the study of computer algorithms that improve automatically through experience.\",\n            \"Deep learning is part of a broader family of machine learning methods based on artificial neural networks.\",\n            \"Natural language processing is a subfield of linguistics, computer science, and artificial intelligence.\",\n            \"Transformers have emerged as a powerful deep learning architecture for natural language processing tasks.\",\n            \"BERT is a transformer-based machine learning technique for natural language processing pre-training.\"\n        ]\n        \n        # Create longer texts by combining shorter ones\n        for _ in range(100):\n            num_sentences = random.randint(3, 10)\n            combined_text = \" \".join(random.choices(base_texts, k=num_sentences))\n            texts.append(combined_text)\n    \n    print(f\"Created dataset with {len(texts)} examples\")\n    \n    # Tokenizer\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n    \n    # Create datasets and dataloaders for each sequence length\n    datasets = {}\n    dataloaders = {}\n    memory_batches = {}\n    \n    for max_length in seq_lengths:\n        # Create dataset with this specific max_length\n        dataset = MaskedTextDataset(texts, tokenizer, max_length=max_length)\n        \n        # Create data loader with batch size that's a multiple of 16 for Tensor Core optimization\n        # Use smaller batches for longer sequences to prevent OOM\n        batch_size = 16 if max_length <= 128 else 8 if max_length <= 256 else 4 if max_length <= 384 else 2\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n        \n        # Create a memory test batch\n        memory_batch = {k: v.to(device) for k, v in next(iter(dataloader)).items()}\n        \n        datasets[max_length] = dataset\n        dataloaders[max_length] = dataloader\n        memory_batches[max_length] = memory_batch\n    \n    return tokenizer, datasets, dataloaders, memory_batches\n\ndef test_sequence_scaling(orig_model, tuned_model, tokenizer):\n    \"\"\"Test how performance improvements scale with sequence length\"\"\"\n    print(\"\\n===== Testing Performance Scaling with Sequence Length =====\")\n    \n    # Get datasets with varying sequence lengths\n    tokenizer, datasets, dataloaders, memory_batches = get_data_varied_lengths()\n    \n    # Results table\n    results = []\n    \n    # Test each sequence length\n    for seq_length in sorted(dataloaders.keys()):\n        print(f\"\\nTesting with sequence length: {seq_length}\")\n        dataloader = dataloaders[seq_length]\n        memory_batch = memory_batches[seq_length]\n        \n        # Function for inference\n        def infer(model, inputs):\n            with torch.no_grad():\n                return model(**inputs)\n        \n        # Test models\n        for model_name, model in [(\"Original\", orig_model), (\"Tuned\", tuned_model)]:\n            model.eval()\n            torch.cuda.empty_cache()\n            \n            # Measure accuracy\n            print(f\"Evaluating {model_name} model accuracy...\")\n            eval_results = evaluate_model_with_stats(model, dataloader, tokenizer, n_runs=3)\n            \n            # Measure speed\n            print(f\"Measuring {model_name} model speed...\")\n            time_results = measure_time_with_stats(infer, model, memory_batch, n_runs=10, warmup=3)\n            \n            # Measure memory\n            memory_used = measure_memory(model, memory_batch)\n            \n            # Store results\n            results.append({\n                \"seq_length\": seq_length,\n                \"model\": model_name,\n                \"loss_mean\": eval_results[\"loss_mean\"],\n                \"loss_std\": eval_results[\"loss_std\"],\n                \"accuracy_mean\": eval_results[\"accuracy_mean\"],\n                \"accuracy_std\": eval_results[\"accuracy_std\"],\n                \"speed_mean\": time_results[\"samples_per_sec\"],\n                \"speed_std\": time_results[\"samples_per_sec_std\"],\n                \"memory\": memory_used\n            })\n    \n    # Print results table\n    print(\"\\n===== Sequence Length Scaling Results =====\")\n    print(\"| Seq Length | Model | MLM Loss | MLM Accuracy | Speed (samples/sec) | Memory (MB) | Speedup |\")\n    print(\"|------------|-------|----------|--------------|---------------------|-------------|---------|\")\n    \n    for seq_length in sorted(dataloaders.keys()):\n        # Extract results for this sequence length\n        orig_result = next(r for r in results if r[\"seq_length\"] == seq_length and r[\"model\"] == \"Original\")\n        tuned_result = next(r for r in results if r[\"seq_length\"] == seq_length and r[\"model\"] == \"Tuned\")\n        \n        # Calculate speedup\n        speedup = tuned_result[\"speed_mean\"] / orig_result[\"speed_mean\"]\n        \n        # Print original model results\n        print(f\"| {seq_length:10d} | Original | {orig_result['loss_mean']:.4f}±{orig_result['loss_std']:.4f} | \"\n              f\"{orig_result['accuracy_mean']:.4f}±{orig_result['accuracy_std']:.4f} | \"\n              f\"{orig_result['speed_mean']:.2f}±{orig_result['speed_std']:.2f} | \"\n              f\"{orig_result['memory']:.2f} | 1.00x |\")\n        \n        # Print tuned model results\n        print(f\"| {seq_length:10d} | Tuned | {tuned_result['loss_mean']:.4f}±{tuned_result['loss_std']:.4f} | \"\n              f\"{tuned_result['accuracy_mean']:.4f}±{tuned_result['accuracy_std']:.4f} | \"\n              f\"{tuned_result['speed_mean']:.2f}±{tuned_result['speed_std']:.2f} | \"\n              f\"{tuned_result['memory']:.2f} | {speedup:.2f}x |\")\n    \n    return results\n\ndef fill_mask_test(model, tokenizer, text=\"The capital of France is [MASK].\"):\n    \"\"\"Test mask filling capability\"\"\"\n    # Replace [MASK] with actual mask token if needed\n    if \"[MASK]\" in text:\n        text = text.replace(\"[MASK]\", tokenizer.mask_token)\n    \n    # Tokenize\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    \n    # Find mask token position\n    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n    \n    # Forward pass\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    \n    # Get predictions for mask position\n    if len(mask_token_index[0]) > 0:\n        batch_idx, token_idx = mask_token_index\n        mask_logits = logits[batch_idx, token_idx, :]\n        \n        # Get top 5 predictions\n        topk_values, topk_indices = torch.topk(mask_logits, 5, dim=1)\n        \n        # Convert to tokens\n        topk_tokens = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in topk_indices[0]]\n        \n        return topk_tokens\n    else:\n        return [\"No mask token found\"]\n\ndef test_bert_optimization():\n    \"\"\"Test SKAutoTuner on BERT model's linear layers\"\"\"\n    \n    # Set seed for reproducibility\n    set_seed(42)\n    \n    # Create reference copy before any modifications to ensure identical initial states\n    model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)\n    model.eval()\n    orig_model = copy.deepcopy(model)  # Create copy before any modifications\n    \n    # Get data for testing (do this before modifying the models)\n    tokenizer, val_loader, memory_batch = get_data()\n    \n    # Get parameter counts before optimization\n    print(\"\\n===== Model Parameter Counts Before Optimization =====\")\n    orig_params = model_size_info(model)\n    print(f\"Total parameters: {orig_params['total_params_millions']:.2f}M\")\n    print(\"Parameters by layer:\")\n    for layer_name, param_count in orig_params['layer_params'].items():\n        print(f\"  - {layer_name}: {param_count/1e6:.2f}M parameters\")\n    \n    # Apply identical vocab size modifications to both models\n    orig_out_features = model.cls.predictions.decoder.weight.size(0)\n    new_out_features = ((orig_out_features + 15) // 16) * 16\n    \n    # Store the true original forward methods before any wrapping\n    true_orig_forward = model.forward\n    true_orig_ref_forward = orig_model.forward\n    \n    # Define the post-processing function\n    def post_process_outputs(model_outputs, orig_size=orig_out_features):\n        \"\"\"Trim any padded outputs back to original vocabulary size\"\"\"\n        if hasattr(model_outputs, 'logits') and model_outputs.logits is not None:\n            if model_outputs.logits.size(-1) > orig_size:\n                # Trim to original vocabulary size\n                model_outputs.logits = model_outputs.logits[..., :orig_size]\n        return model_outputs\n    \n    # Create a wrapper factory for forward methods\n    def create_wrapped_forward(original_forward_fn):\n        def wrapped_forward(*args, **kwargs):\n            outputs = original_forward_fn(*args, **kwargs)\n            return post_process_outputs(outputs)\n        return wrapped_forward\n    \n    # Apply identical modifications to both models\n    for m in [model, orig_model]:\n        if orig_out_features != new_out_features:\n            # Create padded weights and bias\n            orig_weight = m.cls.predictions.decoder.weight\n            orig_bias = m.cls.predictions.decoder.bias\n            \n            new_weight = torch.zeros(new_out_features, orig_weight.size(1), \n                                    device=orig_weight.device, dtype=orig_weight.dtype)\n            new_bias = torch.zeros(new_out_features, \n                                  device=orig_bias.device, dtype=orig_bias.dtype)\n            \n            # Copy the original values\n            new_weight[:orig_out_features, :] = orig_weight\n            new_bias[:orig_out_features] = orig_bias\n            \n            # Replace the decoder\n            new_decoder = torch.nn.Linear(orig_weight.size(1), new_out_features, bias=True)\n            new_decoder.weight = torch.nn.Parameter(new_weight)\n            new_decoder.bias = torch.nn.Parameter(new_bias)\n            \n            m.cls.predictions.decoder = new_decoder\n            \n        # Update the config's vocab_size\n        m.config.vocab_size = new_out_features\n    \n    # Apply a single wrapping to each model\n    model.forward = create_wrapped_forward(true_orig_forward)\n    orig_model.forward = create_wrapped_forward(true_orig_ref_forward)\n    \n    # First evaluate the original model before any modifications\n    print(\"\\nBaseline BERT model (before any modifications):\")\n    baseline_results = evaluate_model_with_stats(model, val_loader, tokenizer)\n    \n    # Measure performance metrics of original model\n    def infer(model, inputs):\n        with torch.no_grad():\n            return model(**inputs)\n    \n    baseline_time_stats = measure_time_with_stats(infer, model, memory_batch, n_runs=10)\n    baseline_speed = baseline_time_stats[\"samples_per_sec\"]\n    baseline_memory = measure_memory(model, memory_batch)\n    \n    print(f\"MLM Loss: {baseline_results['loss_mean']:.4f}±{baseline_results['loss_std']:.4f}\")\n    print(f\"MLM Accuracy: {baseline_results['accuracy_mean']:.4f}±{baseline_results['accuracy_std']:.4f}\")\n    print(f\"Baseline model memory usage: {baseline_memory:.2f} MB\")\n    print(f\"Baseline model speed: {baseline_speed:.2f}±{baseline_time_stats['samples_per_sec_std']:.2f} samples/sec\")\n    \n    print(\"\\n===== Original Model Structure =====\")\n    ModelVisualizer.print_module_tree(model)\n    \n    # Create an evaluation function for the model\n    def acc_eval_func(model):\n        \"\"\"Evaluation function based on true MLM accuracy\"\"\"\n        results = evaluate_model_with_stats(model, val_loader, tokenizer)\n        print(f\"MLM Loss: {results['loss_mean']:.4f}±{results['loss_std']:.4f}, MLM Accuracy: {results['accuracy_mean']:.4f}±{results['accuracy_std']:.4f}\")\n        return results['accuracy_mean']  # Return accuracy (higher is better)\n    \n    # Create a separate speed evaluation function\n    def speed_eval_func(model):\n        \"\"\"Speed evaluation function\"\"\"\n        def infer(model, inputs):\n            with torch.no_grad():\n                return model(**inputs)\n        \n        # Higher is better (inverse of time)\n        time_stats = measure_time_with_stats(infer, model, memory_batch, n_runs=10)\n        throughput = time_stats[\"samples_per_sec\"]\n        print(f\"Inference speed: {throughput:.2f}±{time_stats['samples_per_sec_std']:.2f} samples/sec\")\n        return throughput\n    \n    # Calculate accuracy threshold\n    accuracy_threshold = -0.05  # Allow 5% reduction in accuracy\n    print(f\"Setting accuracy threshold to {accuracy_threshold:.4f}\")\n    \n    # Strategy: Optimizing both linear layers in the MLM head\n    print(\"\\n===== Optimizing both MLM head linear layers =====\")\n    \n    # Create configs to tune both linear layers together with Tensor Core friendly dimensions\n    configs = TuningConfigs([\n        LayerConfig(\n            # Target both linear layers in the MLM head\n            layer_names={\n                \"pattern\": \"cls.predictions.*\",\n                \"type\": \"Linear\",\n            },\n            params={\n                \"num_terms\": [1, 2, 3],\n                \"low_rank\": [16, 32, 64],  # All values are multiples of 16 for Tensor Core\n            },\n            separate=False  # Tune as a group\n        ),\n    ])\n    \n    # Create tuner for both layers together\n    tuner = SKAutoTuner(\n        model=copy.deepcopy(model),\n        configs=configs,\n        accuracy_eval_func=acc_eval_func,\n        search_algorithm=GridSearch(),\n        verbose=True,\n        accuracy_threshold=accuracy_threshold,\n        optmization_eval_func=speed_eval_func\n    )\n    \n    # Run tuning\n    print(\"\\nRunning combined MLM head layers tuning...\")\n    best_params = tuner.tune()\n    print(f\"Best parameters: {best_params}\")\n    \n    # Apply best parameters\n    tuned_model = tuner.apply_best_params()\n    \n    print(\"\\n===== Tuned Model Structure =====\")\n    ModelVisualizer.print_module_tree(tuned_model)\n    \n    # Test the tuned model\n    print(\"\\nEvaluating models with identical conditions:\")\n    \n    # Ensure both models are in the same state for fair comparison\n    for m in [orig_model, tuned_model]:\n        m.eval()\n        torch.cuda.empty_cache()\n\n    # Use identical test conditions\n    def test_model(model_name, model):\n        torch.cuda.empty_cache()\n        if torch.cuda.is_available():\n            torch.cuda.reset_peak_memory_stats()\n        \n        # Run standardized tests\n        results = evaluate_model_with_stats(model, val_loader, tokenizer)\n        \n        def infer(model, inputs):\n            with torch.no_grad():\n                return model(**inputs)\n        \n        time_result = measure_time_with_stats(infer, model, memory_batch, n_runs=10)\n        speed = time_result[\"samples_per_sec\"]\n        speed_std = time_result[\"samples_per_sec_std\"]\n        memory_used = measure_memory(model, memory_batch)\n        \n        return {\n            \"name\": model_name,\n            \"loss\": results[\"loss_mean\"],\n            \"loss_std\": results[\"loss_std\"],\n            \"accuracy\": results[\"accuracy_mean\"],\n            \"accuracy_std\": results[\"accuracy_std\"],\n            \"speed\": speed,\n            \"speed_std\": speed_std,\n            \"memory\": memory_used\n        }\n\n    # Test both models under identical conditions\n    baseline_results = test_model(\"Original\", orig_model)\n    tuned_results = test_model(\"Tuned\", tuned_model)\n    \n    # Extract results for the comparison table\n    baseline_loss = baseline_results[\"loss\"]\n    baseline_accuracy = baseline_results[\"accuracy\"]\n    baseline_speed = baseline_results[\"speed\"]\n    baseline_memory = baseline_results[\"memory\"]\n    \n    final_loss = tuned_results[\"loss\"]\n    final_accuracy = tuned_results[\"accuracy\"]\n    final_speed = tuned_results[\"speed\"]\n    final_memory = tuned_results[\"memory\"]\n    \n    # After optimization, get new parameter counts\n    print(\"\\n===== Model Parameter Counts After Optimization =====\")\n    tuned_params = model_size_info(tuned_model)\n    print(f\"Original model: {orig_params['total_params_millions']:.2f}M parameters\")\n    print(f\"Tuned model: {tuned_params['total_params_millions']:.2f}M parameters\")\n    print(f\"Reduction: {(1 - tuned_params['total_params_millions']/orig_params['total_params_millions'])*100:.2f}%\")\n    \n    print(\"\\nParameters by layer:\")\n    for layer_name in sorted(set(list(orig_params['layer_params'].keys()) + list(tuned_params['layer_params'].keys()))):\n        orig_count = orig_params['layer_params'].get(layer_name, 0) / 1e6\n        tuned_count = tuned_params['layer_params'].get(layer_name, 0) / 1e6\n        \n        if orig_count > 0 and tuned_count > 0:\n            reduction = (1 - tuned_count/orig_count) * 100\n            print(f\"  - {layer_name}: {orig_count:.2f}M → {tuned_count:.2f}M ({reduction:.2f}% reduction)\")\n    \n    print(f\"MLM Loss: {final_loss:.4f}±{tuned_results['loss_std']:.4f} (original: {baseline_loss:.4f}±{baseline_results['loss_std']:.4f})\")\n    print(f\"MLM Accuracy: {final_accuracy:.4f}±{tuned_results['accuracy_std']:.4f} (original: {baseline_accuracy:.4f}±{baseline_results['accuracy_std']:.4f})\")\n    print(f\"Speed: {final_speed:.2f}±{tuned_results['speed_std']:.2f} samples/sec (original: {baseline_speed:.2f}±{baseline_results['speed_std']:.2f})\")\n    print(f\"Memory: {final_memory:.2f} MB (original: {baseline_memory:.2f})\")\n    \n    # Enhanced performance comparison table\n    print(\"\\n===== Performance Comparison =====\")\n    print(\"| Model Version | MLM Loss | MLM Accuracy | Speed (samples/sec) | Memory (MB) | Speed Improvement |\")\n    print(\"|--------------|----------|--------------|---------------------|-------------|-------------------|\")\n    print(f\"| Original     | {baseline_loss:.4f}±{baseline_results['loss_std']:.4f} | {baseline_accuracy:.4f}±{baseline_results['accuracy_std']:.4f} | {baseline_speed:.2f}±{baseline_results['speed_std']:.2f} | {baseline_memory:.2f} | 1.00x |\")\n    print(f\"| Tuned        | {final_loss:.4f}±{tuned_results['loss_std']:.4f} | {final_accuracy:.4f}±{tuned_results['accuracy_std']:.4f} | {final_speed:.2f}±{tuned_results['speed_std']:.2f} | {final_memory:.2f} | {final_speed/baseline_speed:.2f}x |\")\n    \n    # Additional comparison tests with real examples\n    test_examples = [\n        \"The capital of France is [MASK].\",\n        \"Machine learning models [MASK] data to make predictions.\",\n        \"Transformers use [MASK] attention to process sequences.\",\n        \"The [MASK] language model was developed by Google researchers.\"\n    ]\n    \n    print(\"\\n===== Qualitative Comparison: Mask Filling =====\")\n    for test_sentence in test_examples:\n        print(f\"\\nSentence: {test_sentence}\")\n        \n        # Original model predictions\n        orig_predictions = fill_mask_test(orig_model, tokenizer, test_sentence)\n        print(f\"Original model predictions: {', '.join(orig_predictions)}\")\n        \n        # Tuned model predictions\n        tuned_predictions = fill_mask_test(tuned_model, tokenizer, test_sentence)\n        print(f\"Tuned model predictions:    {', '.join(tuned_predictions)}\")\n    \n    # Run sequence length scaling test\n    test_sequence_scaling(orig_model, tuned_model, tokenizer)\n    \n    return tuned_model\n\nif __name__ == \"__main__\":\n    import copy  # Used for deep copying models\n    \n    # Run the BERT optimization test\n    print(\"\\nRunning BERT optimization test with SKAutoTuner...\")\n    test_bert_optimization()\n    \n    print(\"\\nTest completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:11:34.956214Z","iopub.execute_input":"2025-05-06T16:11:34.956945Z","iopub.status.idle":"2025-05-06T16:14:01.007916Z","shell.execute_reply.started":"2025-05-06T16:11:34.956921Z","shell.execute_reply":"2025-05-06T16:14:01.007230Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nRandom seed set to 42 for reproducibility\n\nRunning BERT optimization test with SKAutoTuner...\nRandom seed set to 42 for reproducibility\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Preparing BERT test dataset...\nLoading WikiText dataset for more realistic evaluation...\nAdded 90 examples from WikiText dataset\nCreated dataset with 100 examples\n\n===== Model Parameter Counts Before Optimization =====\nTotal parameters: 109.51M\nParameters by layer:\n  - bert.encoder.layer.0: 7.09M parameters\n  - bert.encoder.layer.1: 7.09M parameters\n  - bert.encoder.layer.2: 7.09M parameters\n  - bert.encoder.layer.3: 7.09M parameters\n  - bert.encoder.layer.4: 7.09M parameters\n  - bert.encoder.layer.5: 7.09M parameters\n  - bert.encoder.layer.6: 7.09M parameters\n  - bert.encoder.layer.7: 7.09M parameters\n  - bert.encoder.layer.8: 7.09M parameters\n  - bert.encoder.layer.9: 7.09M parameters\n  - bert.encoder.layer.10: 7.09M parameters\n  - bert.encoder.layer.11: 7.09M parameters\n  - cls.predictions.transform: 0.59M parameters\n  - cls.predictions.decoder: 23.47M parameters\n\nBaseline BERT model (before any modifications):\nMLM Loss: 14.3385±0.0361\nMLM Accuracy: 0.5525±0.0211\nBaseline model memory usage: 6599.47 MB\nBaseline model speed: 6.90±0.07 samples/sec\n\n===== Original Model Structure =====\nmodel (BertForMaskedLM)/\n└─ bert (BertModel)/\n│   └─ embeddings (BertEmbeddings)/\n│   │   └─ LayerNorm (LayerNorm)\n│   │   └─ dropout (Dropout)\n│   │   └─ position_embeddings (Embedding)\n│   │   └─ token_type_embeddings (Embedding)\n│   │   └─ word_embeddings (Embedding)\n│   └─ encoder (BertEncoder)/\n│       └─ layer (ModuleList)/\n│           └─ 0 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 1 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 10 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 11 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 2 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 3 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 4 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 5 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 6 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 7 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 8 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 9 (BertLayer)/\n│               └─ attention (BertAttention)/\n│               │   └─ output (BertSelfOutput)/\n│               │   │   └─ LayerNorm (LayerNorm)\n│               │   │   └─ dense (Linear)\n│               │   │   └─ dropout (Dropout)\n│               │   └─ self (BertSdpaSelfAttention)/\n│               │       └─ dropout (Dropout)\n│               │       └─ key (Linear)\n│               │       └─ query (Linear)\n│               │       └─ value (Linear)\n│               └─ intermediate (BertIntermediate)/\n│               │   └─ dense (Linear)\n│               │   └─ intermediate_act_fn (GELUActivation)\n│               └─ output (BertOutput)/\n│                   └─ LayerNorm (LayerNorm)\n│                   └─ dense (Linear)\n│                   └─ dropout (Dropout)\n└─ cls (BertOnlyMLMHead)/\n    └─ predictions (BertLMPredictionHead)/\n        └─ decoder (Linear)\n        └─ transform (BertPredictionHeadTransform)/\n            └─ LayerNorm (LayerNorm)\n            └─ dense (Linear)\n            └─ transform_act_fn (GELUActivation)\nSetting accuracy threshold to -0.0500\n\n===== Optimizing both MLM head linear layers =====\n\nRunning combined MLM head layers tuning...\nReplaced cls.predictions.decoder with sketched version using parameters: {'num_terms': 1, 'low_rank': 16}\nReplaced cls.predictions.transform.dense with sketched version using parameters: {'num_terms': 1, 'low_rank': 16}\nMLM Loss: 14.3039±0.0049, MLM Accuracy: 0.5525±0.0115\nInference speed: 6.74±0.06 samples/sec\nTried parameters: {'num_terms': 1, 'low_rank': 16}, accuracy_score: 0.5524691358024691, speed_score: 6.740584877724463                          final score: 6.740584877724463\nReplaced cls.predictions.decoder with sketched version using parameters: {'num_terms': 1, 'low_rank': 32}\nReplaced cls.predictions.transform.dense with sketched version using parameters: {'num_terms': 1, 'low_rank': 32}\nMLM Loss: 14.2212±0.0253, MLM Accuracy: 0.5417±0.0100\nInference speed: 6.66±0.05 samples/sec\nTried parameters: {'num_terms': 1, 'low_rank': 32}, accuracy_score: 0.5416666666666666, speed_score: 6.657054792128928                          final score: 6.657054792128928\nReplaced cls.predictions.decoder with sketched version using parameters: {'num_terms': 1, 'low_rank': 64}\nReplaced cls.predictions.transform.dense with sketched version using parameters: {'num_terms': 1, 'low_rank': 64}\nMLM Loss: 14.2851±0.0178, MLM Accuracy: 0.5478±0.0240\nInference speed: 6.53±0.03 samples/sec\nTried parameters: {'num_terms': 1, 'low_rank': 64}, accuracy_score: 0.5478395061728395, speed_score: 6.528810921034198                          final score: 6.528810921034198\nReplaced cls.predictions.decoder with sketched version using parameters: {'num_terms': 2, 'low_rank': 16}\nReplaced cls.predictions.transform.dense with sketched version using parameters: {'num_terms': 2, 'low_rank': 16}\nMLM Loss: 14.3398±0.0824, MLM Accuracy: 0.5324±0.0182\nInference speed: 6.40±0.04 samples/sec\nTried parameters: {'num_terms': 2, 'low_rank': 16}, accuracy_score: 0.5324074074074073, speed_score: 6.397221909552828                          final score: 6.397221909552828\nReplaced cls.predictions.decoder with sketched version using parameters: {'num_terms': 2, 'low_rank': 32}\nReplaced cls.predictions.transform.dense with sketched version using parameters: {'num_terms': 2, 'low_rank': 32}\nMLM Loss: 14.2589±0.0086, MLM Accuracy: 0.5440±0.0065\nInference speed: 6.27±0.05 samples/sec\nTried parameters: {'num_terms': 2, 'low_rank': 32}, accuracy_score: 0.5439814814814815, speed_score: 6.267630542636748                          final score: 6.267630542636748\nReplaced cls.predictions.decoder with sketched version using parameters: {'num_terms': 2, 'low_rank': 64}\nReplaced cls.predictions.transform.dense with sketched version using parameters: {'num_terms': 2, 'low_rank': 64}\nMLM Loss: 14.2992±0.0597, MLM Accuracy: 0.5679±0.0221\nInference speed: 6.23±0.04 samples/sec\nTried parameters: {'num_terms': 2, 'low_rank': 64}, accuracy_score: 0.5679012345679012, speed_score: 6.233401835346682                          final score: 6.233401835346682\nReplaced cls.predictions.decoder with sketched version using parameters: {'num_terms': 3, 'low_rank': 16}\nReplaced cls.predictions.transform.dense with sketched version using parameters: {'num_terms': 3, 'low_rank': 16}\nMLM Loss: 14.3349±0.0395, MLM Accuracy: 0.5540±0.0225\nInference speed: 6.11±0.04 samples/sec\nTried parameters: {'num_terms': 3, 'low_rank': 16}, accuracy_score: 0.5540123456790124, speed_score: 6.105705480310527                          final score: 6.105705480310527\nReplaced cls.predictions.decoder with sketched version using parameters: {'num_terms': 3, 'low_rank': 32}\nReplaced cls.predictions.transform.dense with sketched version using parameters: {'num_terms': 3, 'low_rank': 32}\nMLM Loss: 14.3438±0.0489, MLM Accuracy: 0.5502±0.0323\nInference speed: 6.12±0.03 samples/sec\nTried parameters: {'num_terms': 3, 'low_rank': 32}, accuracy_score: 0.5501543209876543, speed_score: 6.122552233160954                          final score: 6.122552233160954\nReplaced cls.predictions.decoder with sketched version using parameters: {'num_terms': 3, 'low_rank': 64}\nReplaced cls.predictions.transform.dense with sketched version using parameters: {'num_terms': 3, 'low_rank': 64}\nMLM Loss: 14.2928±0.0196, MLM Accuracy: 0.5478±0.0137\nInference speed: 6.22±0.03 samples/sec\nTried parameters: {'num_terms': 3, 'low_rank': 64}, accuracy_score: 0.5478395061728395, speed_score: 6.215929350427629                          final score: 6.215929350427629\nBest parameters: {'cls.predictions.decoder': {'params': {'num_terms': 1, 'low_rank': 16}, 'copy_weights': True}, 'cls.predictions.transform.dense': {'params': {'num_terms': 1, 'low_rank': 16}, 'copy_weights': True}}\nReplaced cls.predictions.decoder with sketched version using parameters: {'num_terms': 1, 'low_rank': 16}\nReplaced cls.predictions.transform.dense with sketched version using parameters: {'num_terms': 1, 'low_rank': 16}\n\n===== Tuned Model Structure =====\nmodel (BertForMaskedLM)/\n└─ bert (BertModel)/\n│   └─ embeddings (BertEmbeddings)/\n│   │   └─ LayerNorm (LayerNorm)\n│   │   └─ dropout (Dropout)\n│   │   └─ position_embeddings (Embedding)\n│   │   └─ token_type_embeddings (Embedding)\n│   │   └─ word_embeddings (Embedding)\n│   └─ encoder (BertEncoder)/\n│       └─ layer (ModuleList)/\n│           └─ 0 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 1 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 10 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 11 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 2 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 3 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 4 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 5 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 6 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 7 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 8 (BertLayer)/\n│           │   └─ attention (BertAttention)/\n│           │   │   └─ output (BertSelfOutput)/\n│           │   │   │   └─ LayerNorm (LayerNorm)\n│           │   │   │   └─ dense (Linear)\n│           │   │   │   └─ dropout (Dropout)\n│           │   │   └─ self (BertSdpaSelfAttention)/\n│           │   │       └─ dropout (Dropout)\n│           │   │       └─ key (Linear)\n│           │   │       └─ query (Linear)\n│           │   │       └─ value (Linear)\n│           │   └─ intermediate (BertIntermediate)/\n│           │   │   └─ dense (Linear)\n│           │   │   └─ intermediate_act_fn (GELUActivation)\n│           │   └─ output (BertOutput)/\n│           │       └─ LayerNorm (LayerNorm)\n│           │       └─ dense (Linear)\n│           │       └─ dropout (Dropout)\n│           └─ 9 (BertLayer)/\n│               └─ attention (BertAttention)/\n│               │   └─ output (BertSelfOutput)/\n│               │   │   └─ LayerNorm (LayerNorm)\n│               │   │   └─ dense (Linear)\n│               │   │   └─ dropout (Dropout)\n│               │   └─ self (BertSdpaSelfAttention)/\n│               │       └─ dropout (Dropout)\n│               │       └─ key (Linear)\n│               │       └─ query (Linear)\n│               │       └─ value (Linear)\n│               └─ intermediate (BertIntermediate)/\n│               │   └─ dense (Linear)\n│               │   └─ intermediate_act_fn (GELUActivation)\n│               └─ output (BertOutput)/\n│                   └─ LayerNorm (LayerNorm)\n│                   └─ dense (Linear)\n│                   └─ dropout (Dropout)\n└─ cls (BertOnlyMLMHead)/\n    └─ predictions (BertLMPredictionHead)/\n        └─ decoder (SKLinear)\n        └─ transform (BertPredictionHeadTransform)/\n            └─ LayerNorm (LayerNorm)\n            └─ dense (SKLinear)\n            └─ transform_act_fn (GELUActivation)\n\nEvaluating models with identical conditions:\n\n===== Model Parameter Counts After Optimization =====\nOriginal model: 109.51M parameters\nTuned model: 109.48M parameters\nReduction: 0.03%\n\nParameters by layer:\n  - bert.encoder.layer.0: 7.09M → 7.09M (0.00% reduction)\n  - bert.encoder.layer.1: 7.09M → 7.09M (0.00% reduction)\n  - bert.encoder.layer.10: 7.09M → 7.09M (0.00% reduction)\n  - bert.encoder.layer.11: 7.09M → 7.09M (0.00% reduction)\n  - bert.encoder.layer.2: 7.09M → 7.09M (0.00% reduction)\n  - bert.encoder.layer.3: 7.09M → 7.09M (0.00% reduction)\n  - bert.encoder.layer.4: 7.09M → 7.09M (0.00% reduction)\n  - bert.encoder.layer.5: 7.09M → 7.09M (0.00% reduction)\n  - bert.encoder.layer.6: 7.09M → 7.09M (0.00% reduction)\n  - bert.encoder.layer.7: 7.09M → 7.09M (0.00% reduction)\n  - bert.encoder.layer.8: 7.09M → 7.09M (0.00% reduction)\n  - bert.encoder.layer.9: 7.09M → 7.09M (0.00% reduction)\n  - cls.predictions.decoder: 23.47M → 0.53M (97.74% reduction)\n  - cls.predictions.transform: 0.59M → 0.03M (95.46% reduction)\nMLM Loss: 14.3010±0.0435 (original: 14.3601±0.0488)\nMLM Accuracy: 0.5540±0.0324 (original: 0.5718±0.0208)\nSpeed: 6.41±0.08 samples/sec (original: 6.30±0.06)\nMemory: 7111.76 MB (original: 7111.76)\n\n===== Performance Comparison =====\n| Model Version | MLM Loss | MLM Accuracy | Speed (samples/sec) | Memory (MB) | Speed Improvement |\n|--------------|----------|--------------|---------------------|-------------|-------------------|\n| Original     | 14.3601±0.0488 | 0.5718±0.0208 | 6.30±0.06 | 7111.76 | 1.00x |\n| Tuned        | 14.3010±0.0435 | 0.5540±0.0324 | 6.41±0.08 | 7111.76 | 1.02x |\n\n===== Qualitative Comparison: Mask Filling =====\n\nSentence: The capital of France is [MASK].\nOriginal model predictions: paris, lille, lyon, marseille, tours\nTuned model predictions:    paris, lille, lyon, marseille, tours\n\nSentence: Machine learning models [MASK] data to make predictions.\nOriginal model predictions: use, process, utilize, gather, analyze\nTuned model predictions:    use, process, utilize, gather, analyze\n\nSentence: Transformers use [MASK] attention to process sequences.\nOriginal model predictions: mechanical, special, close, careful, manual\nTuned model predictions:    mechanical, special, close, careful, manual\n\nSentence: The [MASK] language model was developed by Google researchers.\nOriginal model predictions: google, natural, programming, query, human\nTuned model predictions:    google, natural, programming, query, human\n\n===== Testing Performance Scaling with Sequence Length =====\nPreparing BERT test datasets with varying lengths: [128, 256, 384, 512]\nLoading WikiText dataset for sequence length tests...\nCreated dataset with 100 examples\n\nTesting with sequence length: 128\nEvaluating Original model accuracy...\nMeasuring Original model speed...\nEvaluating Tuned model accuracy...\nMeasuring Tuned model speed...\n\nTesting with sequence length: 256\nEvaluating Original model accuracy...\nMeasuring Original model speed...\nEvaluating Tuned model accuracy...\nMeasuring Tuned model speed...\n\nTesting with sequence length: 384\nEvaluating Original model accuracy...\nMeasuring Original model speed...\nEvaluating Tuned model accuracy...\nMeasuring Tuned model speed...\n\nTesting with sequence length: 512\nEvaluating Original model accuracy...\nMeasuring Original model speed...\nEvaluating Tuned model accuracy...\nMeasuring Tuned model speed...\n\n===== Sequence Length Scaling Results =====\n| Seq Length | Model | MLM Loss | MLM Accuracy | Speed (samples/sec) | Memory (MB) | Speedup |\n|------------|-------|----------|--------------|---------------------|-------------|---------|\n|        128 | Original | 3.6636±0.0220 | 0.5926±0.0098 | 6.53±0.07 | 7111.91 | 1.00x |\n|        128 | Tuned | 3.6717±0.0077 | 0.5867±0.0070 | 6.57±0.05 | 7111.91 | 1.01x |\n|        256 | Original | 8.2475±0.0501 | 0.6119±0.0027 | 6.47±0.09 | 7111.91 | 1.00x |\n|        256 | Tuned | 8.2407±0.0311 | 0.5985±0.0081 | 6.46±0.09 | 7111.91 | 1.00x |\n|        384 | Original | 11.6019±0.0418 | 0.5953±0.0058 | 8.41±0.07 | 6990.60 | 1.00x |\n|        384 | Tuned | 11.6179±0.0308 | 0.6046±0.0091 | 8.28±0.03 | 6990.60 | 0.98x |\n|        512 | Original | 13.4997±0.0502 | 0.5988±0.0045 | 12.38±0.13 | 6871.35 | 1.00x |\n|        512 | Tuned | 13.4312±0.0358 | 0.5990±0.0069 | 12.41±0.09 | 6871.35 | 1.00x |\n\nTest completed.\n","output_type":"stream"}],"execution_count":30}]}