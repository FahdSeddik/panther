{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"github_repos_wildcard\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:39:33.258083Z","iopub.execute_input":"2025-05-26T17:39:33.258320Z","iopub.status.idle":"2025-05-26T17:39:33.348843Z","shell.execute_reply.started":"2025-05-26T17:39:33.258304Z","shell.execute_reply":"2025-05-26T17:39:33.348191Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"repo_url = f\"https://{token}@github.com/gaserSami/panther.git\"\nbranch = \"autotuner\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:39:33.350142Z","iopub.execute_input":"2025-05-26T17:39:33.350518Z","iopub.status.idle":"2025-05-26T17:39:33.353611Z","shell.execute_reply.started":"2025-05-26T17:39:33.350502Z","shell.execute_reply":"2025-05-26T17:39:33.352909Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!git clone -b {branch} {repo_url}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:39:33.354511Z","iopub.execute_input":"2025-05-26T17:39:33.354732Z","iopub.status.idle":"2025-05-26T17:39:36.836887Z","shell.execute_reply.started":"2025-05-26T17:39:33.354712Z","shell.execute_reply":"2025-05-26T17:39:36.835906Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'panther'...\nremote: Enumerating objects: 1657, done.\u001b[K\nremote: Counting objects: 100% (337/337), done.\u001b[K\nremote: Compressing objects: 100% (89/89), done.\u001b[K\nremote: Total 1657 (delta 271), reused 283 (delta 247), pack-reused 1320 (from 1)\u001b[K\nReceiving objects: 100% (1657/1657), 31.58 MiB | 19.48 MiB/s, done.\nResolving deltas: 100% (1098/1098), done.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# First uninstall existing torch, torchvision, torchaudio\n!pip uninstall -y torch torchvision torchaudio\n\n# Install the specified versions from PyTorch's official CUDA 12.4 wheels\n!pip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:39:36.839202Z","iopub.execute_input":"2025-05-26T17:39:36.839437Z","iopub.status.idle":"2025-05-26T17:41:38.459152Z","shell.execute_reply.started":"2025-05-26T17:39:36.839414Z","shell.execute_reply":"2025-05-26T17:41:38.458362Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.6.0+cu124\nUninstalling torch-2.6.0+cu124:\n  Successfully uninstalled torch-2.6.0+cu124\nFound existing installation: torchvision 0.21.0+cu124\nUninstalling torchvision-0.21.0+cu124:\n  Successfully uninstalled torchvision-0.21.0+cu124\nFound existing installation: torchaudio 2.6.0+cu124\nUninstalling torchaudio-2.6.0+cu124:\n  Successfully uninstalled torchaudio-2.6.0+cu124\nLooking in indexes: https://download.pytorch.org/whl/cu124\nCollecting torch==2.6.0+cu124\n  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (28 kB)\nCollecting torchvision==0.21.0+cu124\n  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\nCollecting torchaudio==2.6.0+cu124\n  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (1.13.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (11.1.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0+cu124) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0+cu124) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0+cu124) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0+cu124) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.21.0+cu124) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.21.0+cu124) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.21.0+cu124) (2024.2.0)\nDownloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (768.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl (7.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchaudio, torchvision\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!mv panther Panther","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:41:38.460136Z","iopub.execute_input":"2025-05-26T17:41:38.460346Z","iopub.status.idle":"2025-05-26T17:41:38.589875Z","shell.execute_reply.started":"2025-05-26T17:41:38.460324Z","shell.execute_reply":"2025-05-26T17:41:38.588890Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"%%writefile /kaggle/working/Panther/pawX/setup.py\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name=\"pawX\",\n    ext_modules=[\n        CUDAExtension(\n            name=\"pawX\",\n            sources=[\n                \"skops.cpp\",\n                \"bindings.cpp\",\n                \"linear.cpp\",\n                \"linear_cuda.cu\",\n                \"cqrrpt.cpp\",\n                \"rsvd.cpp\",\n                \"attention.cpp\",\n                \"conv2d.cpp\"\n            ],\n            # Use system includes and libraries\n            include_dirs=[\"/usr/include/x86_64-linux-gnu\"],\n            library_dirs=[],\n            libraries=[\"openblas\"],\n            extra_compile_args={\"cxx\": [\"-O2\", \"-fopenmp\"], \"nvcc\": [\"-O2\"]},\n            extra_link_args=[\"-llapacke\", \"-lopenblas\"]\n        )\n    ],\n    cmdclass={\"build_ext\": BuildExtension},\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:41:38.591099Z","iopub.execute_input":"2025-05-26T17:41:38.591800Z","iopub.status.idle":"2025-05-26T17:41:38.597666Z","shell.execute_reply.started":"2025-05-26T17:41:38.591764Z","shell.execute_reply":"2025-05-26T17:41:38.596980Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/Panther/pawX/setup.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!sudo apt-get install liblapacke-dev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:41:38.598455Z","iopub.execute_input":"2025-05-26T17:41:38.598776Z","iopub.status.idle":"2025-05-26T17:41:50.773620Z","shell.execute_reply.started":"2025-05-26T17:41:38.598755Z","shell.execute_reply":"2025-05-26T17:41:50.772708Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  liblapacke libtmglib-dev libtmglib3\nSuggested packages:\n  liblapack-doc\nThe following NEW packages will be installed:\n  liblapacke liblapacke-dev libtmglib-dev libtmglib3\n0 upgraded, 4 newly installed, 0 to remove and 87 not upgraded.\nNeed to get 1,071 kB of archives.\nAfter this operation, 12.3 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtmglib3 amd64 3.10.0-2ubuntu1 [144 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblapacke amd64 3.10.0-2ubuntu1 [435 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtmglib-dev amd64 3.10.0-2ubuntu1 [134 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblapacke-dev amd64 3.10.0-2ubuntu1 [358 kB]\nFetched 1,071 kB in 1s (1,011 kB/s)     \ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\ndebconf: falling back to frontend: Readline\nSelecting previously unselected package libtmglib3:amd64.\n(Reading database ... 129184 files and directories currently installed.)\nPreparing to unpack .../libtmglib3_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking libtmglib3:amd64 (3.10.0-2ubuntu1) ...\nSelecting previously unselected package liblapacke:amd64.\nPreparing to unpack .../liblapacke_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking liblapacke:amd64 (3.10.0-2ubuntu1) ...\nSelecting previously unselected package libtmglib-dev:amd64.\nPreparing to unpack .../libtmglib-dev_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking libtmglib-dev:amd64 (3.10.0-2ubuntu1) ...\nSelecting previously unselected package liblapacke-dev:amd64.\nPreparing to unpack .../liblapacke-dev_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking liblapacke-dev:amd64 (3.10.0-2ubuntu1) ...\nSetting up libtmglib3:amd64 (3.10.0-2ubuntu1) ...\nSetting up liblapacke:amd64 (3.10.0-2ubuntu1) ...\nSetting up libtmglib-dev:amd64 (3.10.0-2ubuntu1) ...\nSetting up liblapacke-dev:amd64 (3.10.0-2ubuntu1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!cd /kaggle/working/Panther/pawX; python setup.py install\n!cd /kaggle/working/Panther/pawX; pip install --no-build-isolation -e .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:41:50.774726Z","iopub.execute_input":"2025-05-26T17:41:50.775013Z","iopub.status.idle":"2025-05-26T17:44:17.546262Z","shell.execute_reply.started":"2025-05-26T17:41:50.774975Z","shell.execute_reply":"2025-05-26T17:44:17.545305Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` and ``easy_install``.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://github.com/pypa/setuptools/issues/917 for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nEmitting ninja build file /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/build.ninja...\nCompiling objects...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/cqrrpt.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[2/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/conv2d.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/conv2d.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/conv2d.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n/kaggle/working/Panther/pawX/conv2d.cpp: In function ‘at::Tensor sketched_conv2d_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const std::vector<long int>&, const std::vector<long int>&, const std::vector<long int>&, const std::optional<at::Tensor>&)’:\n/kaggle/working/Panther/pawX/conv2d.cpp:17:28: warning: unused variable ‘C’ [-Wunused-variable]\n   17 |     int64_t B = x.size(0), C = x.size(1), H = x.size(2), W = x.size(3);\n      |                            ^\n[3/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/linear.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[4/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/attention.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[5/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/bindings.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\nIn file included from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/Exceptions.h:12,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/python.h:11,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/extension.h:9,\n                 from /kaggle/working/Panther/pawX/attention.h:3,\n                 from /kaggle/working/Panther/pawX/bindings.cpp:1:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘class pybind11::class_<DistributionFamily>’:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2216:7:   required from ‘class pybind11::enum_<DistributionFamily>’\n/kaggle/working/Panther/pawX/bindings.cpp:26:60:   required from here\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1539:7: warning: ‘pybind11::class_<DistributionFamily>’ declared with greater visibility than its base ‘pybind11::detail::generic_type’ [-Wattributes]\n 1539 | class class_ : public detail::generic_type {\n      |       ^~~~~~\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘pybind11::class_< <template-parameter-1-1>, <template-parameter-1-2> >::class_(pybind11::handle, const char*, const Extra& ...) [with Extra = {}; type_ = DistributionFamily; options = {}]’:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2232:67:   required from ‘pybind11::enum_<Type>::enum_(const pybind11::handle&, const char*, const Extra& ...) [with Extra = {}; Type = DistributionFamily]’\n/kaggle/working/Panther/pawX/bindings.cpp:26:60:   required from here\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1599:28: warning: ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>’ declared with greater visibility than the type of its field ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>::<record capture>’ [-Wattributes]\n 1599 |             with_internals([&](internals &internals) {\n      |                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1600 |                 auto &instances = record.module_local ? get_local_internals().registered_types_cpp\n      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1601 |                                                       : internals.registered_types_cpp;\n      |                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1602 |                 instances[std::type_index(typeid(type_alias))]\n      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1603 |                     = instances[std::type_index(typeid(type))];\n      |                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1604 |             });\n      |             ~               \n[6/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/skops.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[7/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/rsvd.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[8/8] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o.d -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/linear_cuda.cu -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pawX-0.0.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n\u001b[0mObtaining file:///kaggle/working/Panther/pawX\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nInstalling collected packages: pawX\n  Attempting uninstall: pawX\n    Found existing installation: pawX 0.0.0\n    Uninstalling pawX-0.0.0:\n      Successfully uninstalled pawX-0.0.0\n  Running setup.py develop for pawX\nSuccessfully installed pawX-0.0.0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\nprint(torch.__version__)\nimport triton\nprint(triton.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:44:17.547444Z","iopub.execute_input":"2025-05-26T17:44:17.547784Z","iopub.status.idle":"2025-05-26T17:44:17.990987Z","shell.execute_reply.started":"2025-05-26T17:44:17.547748Z","shell.execute_reply":"2025-05-26T17:44:17.990280Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu124\n3.2.0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working/Panther\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:44:17.993027Z","iopub.execute_input":"2025-05-26T17:44:17.993271Z","iopub.status.idle":"2025-05-26T17:44:17.996617Z","shell.execute_reply.started":"2025-05-26T17:44:17.993254Z","shell.execute_reply":"2025-05-26T17:44:17.995874Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!pip install botorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:44:17.997355Z","iopub.execute_input":"2025-05-26T17:44:17.997647Z","iopub.status.idle":"2025-05-26T17:44:23.540375Z","shell.execute_reply.started":"2025-05-26T17:44:17.997624Z","shell.execute_reply":"2025-05-26T17:44:23.539665Z"}},"outputs":[{"name":"stdout","text":"Collecting botorch\n  Downloading botorch-0.14.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from botorch) (4.13.2)\nCollecting pyre_extensions (from botorch)\n  Downloading pyre_extensions-0.0.32-py3-none-any.whl.metadata (4.0 kB)\nCollecting gpytorch==1.14 (from botorch)\n  Downloading gpytorch-1.14-py3-none-any.whl.metadata (8.0 kB)\nCollecting linear_operator==0.6 (from botorch)\n  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from botorch) (2.6.0+cu124)\nCollecting pyro-ppl>=1.8.4 (from botorch)\n  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from botorch) (1.15.2)\nRequirement already satisfied: multipledispatch in /usr/local/lib/python3.11/dist-packages (from botorch) (1.0.0)\nRequirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from botorch) (3.6.0)\nCollecting jaxtyping (from gpytorch==1.14->botorch)\n  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.11/dist-packages (from gpytorch==1.14->botorch) (1.3.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from gpytorch==1.14->botorch) (1.2.2)\nRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (3.4.0)\nCollecting pyro-api>=0.1.1 (from pyro-ppl>=1.8.4->botorch)\n  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (1.13.1)\nRequirement already satisfied: typing-inspect in /usr/local/lib/python3.11/dist-packages (from pyre_extensions->botorch) (0.9.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2.4.1)\nCollecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch==1.14->botorch)\n  Downloading wadler_lindig-0.1.6-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.1->botorch) (3.0.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch==1.14->botorch) (1.5.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect->pyre_extensions->botorch) (1.1.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\nDownloading botorch-0.14.0-py3-none-any.whl (738 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m738.3/738.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading gpytorch-1.14-py3-none-any.whl (277 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.7/277.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading linear_operator-0.6-py3-none-any.whl (176 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyre_extensions-0.0.32-py3-none-any.whl (12 kB)\nDownloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\nDownloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wadler_lindig-0.1.6-py3-none-any.whl (20 kB)\nInstalling collected packages: pyro-api, wadler-lindig, pyre_extensions, jaxtyping, linear_operator, pyro-ppl, gpytorch, botorch\nSuccessfully installed botorch-0.14.0 gpytorch-1.14 jaxtyping-0.3.2 linear_operator-0.6 pyre_extensions-0.0.32 pyro-api-0.1.2 pyro-ppl-1.9.1 wadler-lindig-0.1.6\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os\nimport time\nimport copy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n\n# Import components from the SKAutoTuner module\nfrom panther.utils.SkAutoTuner import (\n    SKAutoTuner, \n    LayerConfig, \n    TuningConfigs, \n    GridSearch,\n    RandomSearch, \n    ModelVisualizer\n)\nfrom panther.nn import SKLinear, SKLinear_triton, RandMultiHeadAttention\n\n# Setting up device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n##################################### CUSTOM MODEL #######################################\n\nclass ConvBlock(nn.Module):\n    \"\"\"A simple convolutional block with batch normalization and ReLU\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        return self.relu(self.bn(self.conv(x)))\n\nclass FCBlock(nn.Module):\n    \"\"\"A fully connected block with dropout and ReLU\"\"\"\n    def __init__(self, in_features, out_features, dropout=0.2):\n        super(FCBlock, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.dropout = nn.Dropout(dropout)\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        return self.relu(self.dropout(self.linear(x)))\n\nclass AttentionBlock(nn.Module):\n    \"\"\"An attention block using MultiheadAttention\"\"\"\n    def __init__(self, embed_dim, num_heads):\n        super(AttentionBlock, self).__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim * 4),\n            nn.ReLU(),\n            nn.Linear(embed_dim * 4, embed_dim)\n        )\n    \n    def forward(self, x):\n        # Layer normalization and attention\n        normalized = self.norm1(x)\n        attention_output, _ = self.mha(normalized, normalized, normalized)\n        x = x + attention_output\n        \n        # Layer normalization and feed-forward network\n        normalized = self.norm2(x)\n        ffn_output = self.ffn(normalized)\n        \n        return x + ffn_output\n\nclass MixedModel(nn.Module):\n    \"\"\"A model with mixed layer types (Conv2D, Linear, MultiheadAttention) in a nested structure\"\"\"\n    def __init__(self, num_classes=10):\n        super(MixedModel, self).__init__()\n        \n        # Convolutional feature extractor for images\n        self.feature_extractor = nn.Sequential(\n            ConvBlock(3, 32),                # 32x32x32\n            nn.MaxPool2d(2, 2),              # 32x16x16\n            ConvBlock(32, 64),               # 64x16x16\n            nn.MaxPool2d(2, 2),              # 64x8x8\n            ConvBlock(64, 128),              # 128x8x8\n            nn.MaxPool2d(2, 2),              # 128x4x4\n        )\n        \n        # Fully connected classifier\n        self.classifier = nn.Sequential(\n            FCBlock(128 * 4 * 4, 512),\n            FCBlock(512, 256),\n            nn.Linear(256, num_classes)\n        )\n        \n        # Attention branch (processes flattened features from a different perspective)\n        self.attention_branch = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128 * 4 * 4, 256),\n            nn.LayerNorm(256)\n        )\n        self.attention_blocks = nn.ModuleList([\n            AttentionBlock(256, 8),\n            AttentionBlock(256, 8)\n        ])\n        self.attention_output = nn.Linear(256, num_classes)\n        \n        # Final combination\n        self.alpha = nn.Parameter(torch.tensor(0.5))\n    \n    def forward(self, x):\n        # Extract features\n        features = self.feature_extractor(x)\n        \n        # Process through classifier branch\n        flattened = features.view(features.size(0), -1)\n        classifier_output = self.classifier(flattened)\n        \n        # Process through attention branch\n        attention_input = self.attention_branch(features)\n        \n        # Apply attention blocks sequentially\n        for block in self.attention_blocks:\n            attention_input = block(attention_input)\n        \n        attention_output = self.attention_output(attention_input)\n        \n        # Combined output\n        combined_output = self.alpha * classifier_output + (1 - self.alpha) * attention_output\n        \n        return combined_output\n\n##################################### TEST HELPERS #######################################\n\ndef generate_dummy_data(batch_size=32, input_shape=(3, 32, 32), num_classes=10):\n    \"\"\"Generate dummy data for testing\"\"\"\n    inputs = torch.randn(batch_size, *input_shape)\n    labels = torch.randint(0, num_classes, (batch_size,))\n    return inputs, labels\n\ndef create_data_loaders(batch_size=32, num_samples=1000, input_shape=(3, 32, 32), num_classes=10):\n    \"\"\"Create data loaders for training and validation\"\"\"\n    # Generate random data\n    inputs, labels = generate_dummy_data(num_samples, input_shape, num_classes)\n    dataset = TensorDataset(inputs, labels)\n    \n    # Split into training and validation\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    return train_loader, val_loader\n\ndef measure_accuracy(model, data_loader, device):\n    \"\"\"Measure the accuracy of a model on a dataset\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    return correct / total\n\ndef measure_time(func, *args, n_runs=10, warmup=2):\n    \"\"\"Measure execution time of a function\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        func(*args)\n    \n    # Timed runs\n    torch.cuda.synchronize() if torch.cuda.is_available() else None\n    start = time.time()\n    for _ in range(n_runs):\n        func(*args)\n        torch.cuda.synchronize() if torch.cuda.is_available() else None\n    end = time.time()\n    \n    return (end - start) / n_runs\n\ndef measure_memory(model, input_tensor):\n    \"\"\"Measure peak memory usage of a model during inference\"\"\"\n    if not torch.cuda.is_available():\n        return 0  # Cannot measure CUDA memory on CPU\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    \n    # Run inference\n    with torch.no_grad():\n        model(input_tensor)\n    \n    # Get peak memory\n    return torch.cuda.max_memory_allocated() / (1024 * 1024)  # Convert to MB\n\n##################################### TEST FUNCTIONS #######################################\n\ndef test_skautotuner_visualization():\n    \"\"\"Test SKAutoTuner's visualization functionality\"\"\"\n    print(\"\\n===== Testing SKAutoTuner Visualization =====\")\n    \n    # Create data and model\n    train_loader, val_loader = create_data_loaders()\n    sample_batch, _ = next(iter(val_loader))\n    sample_batch = sample_batch.to(device)\n    \n    model = MixedModel().to(device)\n\n    # Create evaluation functions\n    def accuracy_eval_func(model):\n        return measure_accuracy(model, val_loader, device)\n    \n    def speed_eval_func(model):\n        def inference(model, x):\n            with torch.no_grad():\n                return model(x)\n        \n        return 1.0 / measure_time(inference, model, sample_batch)\n    \n    # Create tuning configuration focusing on linear layers in the classifier\n    config = TuningConfigs([\n        LayerConfig(\n            layer_names={\"pattern\": \"classifier.*linear\"},\n            params={\n                \"num_terms\": [1, 2],\n                \"low_rank\": [8, 16],\n            },\n            separate=True\n        )\n    ])\n    \n    # Create tuner with limited search space for quick testing\n    tuner = SKAutoTuner(\n        model=copy.deepcopy(model),\n        configs=config,\n        accuracy_eval_func=accuracy_eval_func,\n        search_algorithm=GridSearch(),\n        verbose=True\n    )\n    \n    # Run a simple tuning process\n    print(\"Running tuning...\")\n    tuner.tune()\n    \n    # Test visualization function (save to file and show)\n    save_path = \"tuning_visualization.png\"\n    print(f\"Generating visualization at {save_path}...\")\n    \n    # Test with and without saving\n    tuner.visualize_tuning_results(save_path=save_path, show_plot=False)\n    tuner.visualize_tuning_results(show_plot=False)\n    \n    # Test with custom figure size\n    tuner.visualize_tuning_results(figsize=(10, 8), show_plot=False)\n    \n    if os.path.exists(save_path):\n        print(f\"Visualization saved successfully to {save_path}\")\n    else:\n        print(\"WARNING: Visualization file not created\")\n    \n    return tuner\n\ndef test_skautotuner_results():\n    \"\"\"Test SKAutoTuner's result retrieval functions\"\"\"\n    print(\"\\n===== Testing SKAutoTuner Results Retrieval =====\")\n    \n    # Create data and model\n    train_loader, val_loader = create_data_loaders()\n    \n    model = MixedModel().to(device)\n    \n    # Create evaluation function\n    def accuracy_eval_func(model):\n        return measure_accuracy(model, val_loader, device)\n    \n    # Create tuning configuration with different layer types\n    config = TuningConfigs([\n        # Test Conv2d layers\n        LayerConfig(\n            layer_names={\"pattern\": \"feature_extractor.*conv\"},\n            params={\n                \"num_terms\": [1, 2],\n                \"low_rank\": [8, 16],\n            },\n            separate=False\n        ),\n        # Test Linear layers\n        LayerConfig(\n            layer_names={\"pattern\": \"classifier.*linear\"},\n            params={\n                \"num_terms\": [1, 2],\n                \"low_rank\": [8, 16],\n            },\n            separate=True\n        ),\n        # Test MultiheadAttention layers\n        LayerConfig(\n            layer_names={\"pattern\": \"attention_blocks.*mha\", \"type\" : \"MultiheadAttention\"},\n            params={\n                \"num_random_features\": [16, 32],\n                \"kernel_fn\": [\"softmax\", \"relu\"],\n            },\n            separate=True\n        )\n    ])\n    \n    # Create tuner\n    tuner = SKAutoTuner(\n        model=copy.deepcopy(model),\n        configs=config,\n        accuracy_eval_func=accuracy_eval_func,\n        search_algorithm=RandomSearch(max_trials=2),  # Using a small number of trials for quick testing\n        verbose=True\n    )\n    \n    # Run tuning\n    print(\"Running tuning...\")\n    best_params = tuner.tune()\n    \n    # Test get_best_params\n    print(\"\\nTesting get_best_params:\")\n    retrieved_best_params = tuner.get_best_params()\n    \n    # Check if the returned parameters match the ones from tune()\n    for layer_name in best_params:\n        if layer_name in retrieved_best_params:\n            print(f\"Layer: {layer_name}\")\n            print(f\"  Parameters consistent: {best_params[layer_name]['params'] == retrieved_best_params[layer_name]['params']}\")\n        else:\n            print(f\"WARNING: Layer {layer_name} not found in retrieved_best_params\")\n    \n    # Test get_results_dataframe\n    print(\"\\nTesting get_results_dataframe:\")\n    results_df = tuner.get_results_dataframe()\n    \n    # Print summary of results\n    print(f\"DataFrame shape: {results_df.shape}\")\n    print(\"Columns in DataFrame:\")\n    for col in results_df.columns:\n        print(f\"  - {col}\")\n    \n    # Check if layer names are in the DataFrame\n    layers_in_df = results_df['layer_name'].unique()\n    print(f\"Unique layer names in results: {len(layers_in_df)}\")\n    \n    # Try to find results for each layer group in the configs\n    for config_group in config.configs:\n        for layer_name in config_group.layer_names:\n            found = any(layer_name in df_layer_name for df_layer_name in layers_in_df)\n            if found:\n                print(f\"Results found for layer: {layer_name}\")\n            else:\n                print(f\"WARNING: No results found for layer: {layer_name}\")\n    \n    return tuner, best_params, results_df\n\ndef test_skautotuner_apply_best_params():\n    \"\"\"Test SKAutoTuner's apply_best_params function\"\"\"\n    print(\"\\n===== Testing SKAutoTuner apply_best_params =====\")\n    \n    # Create data and model\n    train_loader, val_loader = create_data_loaders()\n    sample_batch, _ = next(iter(val_loader))\n    sample_batch = sample_batch.to(device)\n    \n    orig_model = MixedModel().to(device)\n    print(\"visualize the model before doing anything\")\n    ModelVisualizer.print_module_tree(orig_model)\n    \n    # Measure original model characteristics\n    orig_accuracy = measure_accuracy(orig_model, val_loader, device)\n    \n    def inference(model, x):\n        with torch.no_grad():\n            return model(x)\n    \n    orig_speed = 1.0 / measure_time(inference, orig_model, sample_batch)\n    orig_memory = measure_memory(orig_model, sample_batch)\n    \n    print(f\"Original model accuracy: {orig_accuracy:.4f}\")\n    print(f\"Original model speed: {orig_speed:.2f} samples/sec\")\n    print(f\"Original model memory: {orig_memory:.2f} MB\")\n    \n    # Create evaluation function with threshold\n    def accuracy_eval_func(model):\n        return measure_accuracy(model, val_loader, device)\n    \n    def speed_eval_func(model):\n        return 1.0 / measure_time(inference, model, sample_batch)\n    \n    # Create tuning configuration focusing on computationally intensive parts\n    config = TuningConfigs([\n        # Tune the final layers in each branch of the network\n        LayerConfig(\n            layer_names=[\n                \"classifier.2\",  # Final linear in classifier\n                \"attention_output\"  # Final linear in attention branch\n            ],\n            params={\n                \"num_terms\": [1, 2, 3],\n                \"low_rank\": [8, 16, 32],\n            },\n            separate=True\n        ),\n        # Tune the attention blocks\n        LayerConfig(\n            layer_names={\"pattern\": \"attention_blocks.*mha\", \"type\" : \"MultiheadAttention\"},\n            params={\n                \"num_random_features\": [16, 32, 64],\n                \"kernel_fn\": [\"softmax\"],\n            },\n            separate=False  # Tune as a group\n        )\n    ])\n    \n    # Create tuner with accuracy threshold\n    accuracy_threshold = orig_accuracy - 0.05  # Allow 5% accuracy drop\n    tuner = SKAutoTuner(\n        model=copy.deepcopy(orig_model),\n        configs=config,\n        accuracy_eval_func=accuracy_eval_func,\n        accuracy_threshold=accuracy_threshold,\n        optmization_eval_func=speed_eval_func,\n        search_algorithm=RandomSearch(max_trials=3),  # Using a small number of trials for quick testing\n        verbose=True\n    )\n    \n    # Run tuning\n    print(\"Running tuning...\")\n    best_params = tuner.tune()\n    print(\"Best parameters:\")\n    for layer, params in best_params.items():\n        print(f\"  {layer}: {params}\")\n    \n    # Apply best parameters\n    print(\"Applying best parameters...\")\n    tuned_model = tuner.apply_best_params()\n    \n    # Print model structure after tuning\n    print(\"\\nTuned model structure:\")\n    ModelVisualizer.print_module_tree(tuned_model)\n    \n    # Evaluate the tuned model\n    tuned_accuracy = measure_accuracy(tuned_model, val_loader, device)\n    tuned_speed = 1.0 / measure_time(inference, tuned_model, sample_batch)\n    tuned_memory = measure_memory(tuned_model, sample_batch)\n    \n    print(f\"Tuned model accuracy: {tuned_accuracy:.4f} (diff: {tuned_accuracy - orig_accuracy:.4f})\")\n    print(f\"Tuned model speed: {tuned_speed:.2f} samples/sec (diff: {tuned_speed - orig_speed:.2f})\")\n    print(f\"Tuned model memory: {tuned_memory:.2f} MB (diff: {tuned_memory - orig_memory:.2f} MB)\")\n    \n    # Check if the accuracy threshold was respected\n    if tuned_accuracy >= accuracy_threshold:\n        print(\"✅ Tuned model meets accuracy threshold\")\n    else:\n        print(\"❌ Tuned model fails to meet accuracy threshold\")\n    \n    return tuner, tuned_model\n\ndef test_skautotuner_combined_scenario():\n    \"\"\"Test a combined real-world scenario with the SKAutoTuner\"\"\"\n    print(\"\\n===== Testing SKAutoTuner in a Combined Scenario =====\")\n    \n    # Create data and model\n    train_loader, val_loader = create_data_loaders(batch_size=64, num_samples=2000)\n    sample_batch, _ = next(iter(val_loader))\n    sample_batch = sample_batch.to(device)\n    \n    orig_model = MixedModel().to(device)\n    print(\"\\nOriginal model structure:\")\n    ModelVisualizer.print_module_tree(orig_model)\n    \n    # Measure original model characteristics\n    orig_accuracy = measure_accuracy(orig_model, val_loader, device)\n    \n    def inference(model, x):\n        with torch.no_grad():\n            return model(x)\n    \n    orig_speed = 1.0 / measure_time(inference, orig_model, sample_batch)\n    orig_memory = measure_memory(orig_model, sample_batch)\n    \n    print(f\"Original model accuracy: {orig_accuracy:.4f}\")\n    print(f\"Original model speed: {orig_speed:.2f} samples/sec\")\n    print(f\"Original model memory: {orig_memory:.2f} MB\")\n    \n    # Define evaluation functions\n    def accuracy_eval_func(model):\n        return measure_accuracy(model, val_loader, device)\n    \n    def speed_eval_func(model):\n        return 1.0 / measure_time(inference, model, sample_batch)\n    \n    def memory_eval_func(model):\n        mem_usage = measure_memory(model, sample_batch)\n        orig_mem = measure_memory(orig_model, sample_batch)\n        \n        # Return a score that's higher when memory reduction is greater\n        # Normalize by original memory to get a relative improvement\n        score = (orig_mem - mem_usage) / max(orig_mem, 1e-8)\n        \n        print(f\"Memory evaluation: {mem_usage:.2f} MB (original: {orig_mem:.2f} MB, reduction: {score:.2f})\")\n        return score\n    \n    # Strategy 1: Focus on speed optimization\n    print(\"\\n----- Strategy 1: Speed Optimization -----\")\n    \n    config_speed = TuningConfigs([\n        # Tune convolutional layers\n        LayerConfig(\n            layer_names={\"pattern\": \"feature_extractor.*conv\"},\n            params={\n                \"num_terms\": [1, 2],\n                \"low_rank\": [16, 32],\n            },\n            separate=False  # Tune as a group\n        ),\n        # Tune linear layers in classifier\n        LayerConfig(\n            layer_names={\"pattern\": \"classifier.*linear\"},\n            params={\n                \"num_terms\": [1, 2],\n                \"low_rank\": [16, 32],\n            },\n            separate=True  # Tune individually\n        )\n    ])\n    \n    # Create tuner for speed\n    accuracy_threshold = orig_accuracy - 0.05  # Allow 5% accuracy drop\n    tuner_speed = SKAutoTuner(\n        model=copy.deepcopy(orig_model),\n        configs=config_speed,\n        accuracy_eval_func=accuracy_eval_func,\n        accuracy_threshold=accuracy_threshold,\n        optmization_eval_func=speed_eval_func,\n        search_algorithm=RandomSearch(max_trials=2),  # Using a small number of trials for quick testing\n        verbose=True\n    )\n    \n    # Run tuning\n    print(\"Running speed-focused tuning...\")\n    best_params_speed = tuner_speed.tune()\n    tuned_model_speed = tuner_speed.apply_best_params()\n    \n    # Evaluate speed-tuned model\n    speed_accuracy = measure_accuracy(tuned_model_speed, val_loader, device)\n    speed_speed = 1.0 / measure_time(inference, tuned_model_speed, sample_batch)\n    speed_memory = measure_memory(tuned_model_speed, sample_batch)\n    \n    print(f\"Speed-tuned model accuracy: {speed_accuracy:.4f} (diff: {speed_accuracy - orig_accuracy:.4f})\")\n    print(f\"Speed-tuned model speed: {speed_speed:.2f} samples/sec (diff: {speed_speed - orig_speed:.2f})\")\n    print(f\"Speed-tuned model memory: {speed_memory:.2f} MB (diff: {speed_memory - orig_memory:.2f} MB)\")\n    \n    # Strategy 2: Focus on memory optimization\n    print(\"\\n----- Strategy 2: Memory Optimization -----\")\n    \n    config_memory = TuningConfigs([\n        # Tune attention blocks (memory-intensive)\n        LayerConfig(\n            layer_names={\"pattern\": \"attention_blocks.*mha\", \"type\" : \"MultiheadAttention\"},\n            params={\n                \"num_random_features\": [16, 32],\n                \"kernel_fn\": [\"relu\", \"softmax\"],  # Different kernel functions affect memory usage\n            },\n            separate=False  # Tune as a group\n        ),\n        # Tune the attention output layer\n        LayerConfig(\n            layer_names={\"pattern\": \"attention_output\"},\n            params={\n                \"num_terms\": [1],\n                \"low_rank\": [8, 16, 32],  # Lower rank = lower memory\n            },\n            separate=True\n        )\n    ])\n    \n    # Create tuner for memory\n    tuner_memory = SKAutoTuner(\n        model=copy.deepcopy(orig_model),\n        configs=config_memory,\n        accuracy_eval_func=accuracy_eval_func,\n        accuracy_threshold=accuracy_threshold,\n        optmization_eval_func=memory_eval_func,\n        search_algorithm=GridSearch(),\n        verbose=True\n    )\n    \n    # Run tuning\n    print(\"Running memory-focused tuning...\")\n    best_params_memory = tuner_memory.tune()\n    tuned_model_memory = tuner_memory.apply_best_params()\n    \n    # Evaluate memory-tuned model\n    memory_accuracy = measure_accuracy(tuned_model_memory, val_loader, device)\n    memory_speed = 1.0 / measure_time(inference, tuned_model_memory, sample_batch)\n    memory_memory = measure_memory(tuned_model_memory, sample_batch)\n    \n    print(f\"Memory-tuned model accuracy: {memory_accuracy:.4f} (diff: {memory_accuracy - orig_accuracy:.4f})\")\n    print(f\"Memory-tuned model speed: {memory_speed:.2f} samples/sec (diff: {memory_speed - orig_speed:.2f})\")\n    print(f\"Memory-tuned model memory: {memory_memory:.2f} MB (diff: {memory_memory - orig_memory:.2f} MB)\")\n    \n    # Get and visualize results\n    speed_results_df = tuner_speed.get_results_dataframe()\n    memory_results_df = tuner_memory.get_results_dataframe()\n    \n    print(\"\\nSpeed tuning results summary:\")\n    print(f\"Total trials: {len(speed_results_df)}\")\n    print(f\"Average speed score: {speed_results_df['score'].mean():.4f}\")\n    \n    print(\"\\nMemory tuning results summary:\")\n    print(f\"Total trials: {len(memory_results_df)}\")\n    print(f\"Average memory score: {memory_results_df['score'].mean():.4f}\")\n    \n    # Visualize results\n    print(\"\\nGenerating visualizations...\")\n    tuner_speed.visualize_tuning_results(save_path=\"speed_tuning_results.png\", show_plot=False)\n    tuner_memory.visualize_tuning_results(save_path=\"memory_tuning_results.png\", show_plot=False)\n    \n    # Final comparison\n    print(\"\\n----- Final Comparison -----\")\n    print(f\"Original model - Accuracy: {orig_accuracy:.4f}, Speed: {orig_speed:.2f} samples/sec, Memory: {orig_memory:.2f} MB\")\n    print(f\"Speed-tuned  - Accuracy: {speed_accuracy:.4f}, Speed: {speed_speed:.2f} samples/sec, Memory: {speed_memory:.2f} MB\")\n    print(f\"Memory-tuned - Accuracy: {memory_accuracy:.4f}, Speed: {memory_speed:.2f} samples/sec, Memory: {memory_memory:.2f} MB\")\n    \n    return tuner_speed, tuner_memory, tuned_model_speed, tuned_model_memory\n\nif __name__ == \"__main__\":\n    import sys\n    \n    # Default behavior: run all tests\n    run_visualization = True\n    run_results = True\n    run_apply = True\n    run_combined = True\n    \n    print(\"SKAutoTuner Test Suite\")\n    print(\"=====================\")\n    print(f\"PyTorch version: {torch.__version__}\")\n    print(f\"Device: {device}\")\n    print(f\"Tests to run: Visualization={run_visualization}, Results={run_results}, Apply={run_apply}, Combined={run_combined}\")\n    print(f\"model visualization:\")\n    ModelVisualizer.print_module_tree(MixedModel())\n\n    # Run tests\n    vis_tuner = None\n    results_tuner = None\n    apply_tuner = None\n    combined_tuners = None\n    \n    try:\n        if run_visualization:\n            vis_tuner = test_skautotuner_visualization()\n        \n        if run_results:\n            results_tuner, best_params, results_df = test_skautotuner_results()\n        \n        if run_apply:\n            apply_tuner, tuned_model = test_skautotuner_apply_best_params()\n        \n        if run_combined:\n            combined_tuners = test_skautotuner_combined_scenario()\n        \n        print(\"\\nAll tests completed successfully!\")\n    \n    except Exception as e:\n        print(f\"\\nTest failed with error: {str(e)}\")\n        import traceback\n        traceback.print_exc()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:44:23.541698Z","iopub.execute_input":"2025-05-26T17:44:23.541988Z","execution_failed":"2025-05-26T17:44:28.340Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nSKAutoTuner Test Suite\n=====================\nPyTorch version: 2.6.0+cu124\nDevice: cuda\nTests to run: Visualization=True, Results=True, Apply=True, Combined=True\nmodel visualization:\nmodel (MixedModel)/\n└─ attention_blocks (ModuleList)/\n│   ├─ 0 (AttentionBlock)/\n│   │   ├─ ffn (Sequential)/\n│   │   │   ├─ 0 (Linear)\n│   │   │   ├─ 1 (ReLU)\n│   │   │   ├─ 2 (Linear)\n│   │   ├─ mha (MultiheadAttention)/\n│   │   │   ├─ out_proj (NonDynamicallyQuantizableLinear)\n│   │   ├─ norm1 (LayerNorm)\n│   │   ├─ norm2 (LayerNorm)\n│   ├─ 1 (AttentionBlock)/\n│       └─ ffn (Sequential)/\n│       │   ├─ 0 (Linear)\n│       │   ├─ 1 (ReLU)\n│       │   ├─ 2 (Linear)\n│       └─ mha (MultiheadAttention)/\n│       │   ├─ out_proj (NonDynamicallyQuantizableLinear)\n│       └─ norm1 (LayerNorm)\n│       └─ norm2 (LayerNorm)\n└─ attention_branch (Sequential)/\n│   ├─ 0 (Flatten)\n│   ├─ 1 (Linear)\n│   ├─ 2 (LayerNorm)\n└─ attention_output (Linear)\n└─ classifier (Sequential)/\n│   ├─ 0 (FCBlock)/\n│   │   ├─ dropout (Dropout)\n│   │   ├─ linear (Linear)\n│   │   ├─ relu (ReLU)\n│   ├─ 1 (FCBlock)/\n│   │   ├─ dropout (Dropout)\n│   │   ├─ linear (Linear)\n│   │   ├─ relu (ReLU)\n│   ├─ 2 (Linear)\n└─ feature_extractor (Sequential)/\n    └─ 0 (ConvBlock)/\n    │   ├─ bn (BatchNorm2d)\n    │   ├─ conv (Conv2d)\n    │   ├─ relu (ReLU)\n    └─ 1 (MaxPool2d)\n    └─ 2 (ConvBlock)/\n    │   ├─ bn (BatchNorm2d)\n    │   ├─ conv (Conv2d)\n    │   ├─ relu (ReLU)\n    └─ 3 (MaxPool2d)\n    └─ 4 (ConvBlock)/\n    │   ├─ bn (BatchNorm2d)\n    │   ├─ conv (Conv2d)\n    │   ├─ relu (ReLU)\n    └─ 5 (MaxPool2d)\n\n===== Testing SKAutoTuner Visualization =====\n","output_type":"stream"}],"execution_count":null}]}