{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-27T17:44:34.589994Z",
     "iopub.status.busy": "2025-05-27T17:44:34.589260Z",
     "iopub.status.idle": "2025-05-27T17:44:34.745825Z",
     "shell.execute_reply": "2025-05-27T17:44:34.745319Z",
     "shell.execute_reply.started": "2025-05-27T17:44:34.589968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "token = user_secrets.get_secret(\"github_repos_wildcard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:44:34.746938Z",
     "iopub.status.busy": "2025-05-27T17:44:34.746750Z",
     "iopub.status.idle": "2025-05-27T17:44:34.750314Z",
     "shell.execute_reply": "2025-05-27T17:44:34.749787Z",
     "shell.execute_reply.started": "2025-05-27T17:44:34.746923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "repo_url = f\"https://{token}@github.com/gaserSami/panther.git\"\n",
    "branch = \"autotuner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:44:34.751112Z",
     "iopub.status.busy": "2025-05-27T17:44:34.750926Z",
     "iopub.status.idle": "2025-05-27T17:44:38.529596Z",
     "shell.execute_reply": "2025-05-27T17:44:38.528737Z",
     "shell.execute_reply.started": "2025-05-27T17:44:34.751092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Cloning into 'panther'...\nremote: Enumerating objects: 1876, done.\u001b[K\nremote: Counting objects: 100% (379/379), done.\u001b[K\nremote: Compressing objects: 100% (54/54), done.\u001b[K\nremote: Total 1876 (delta 342), reused 336 (delta 325), pack-reused 1497 (from 1)\u001b[K\nReceiving objects: 100% (1876/1876), 33.63 MiB | 22.19 MiB/s, done.\nResolving deltas: 100% (1229/1229), done.\n"
    }
   ],
   "source": [
    "!git clone -b {branch} {repo_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:44:38.531624Z",
     "iopub.status.busy": "2025-05-27T17:44:38.531397Z",
     "iopub.status.idle": "2025-05-27T17:47:18.542748Z",
     "shell.execute_reply": "2025-05-27T17:47:18.542039Z",
     "shell.execute_reply.started": "2025-05-27T17:44:38.531602Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Found existing installation: torch 2.6.0+cu124\nUninstalling torch-2.6.0+cu124:\n  Successfully uninstalled torch-2.6.0+cu124\nFound existing installation: torchvision 0.21.0+cu124\nUninstalling torchvision-0.21.0+cu124:\n  Successfully uninstalled torchvision-0.21.0+cu124\nFound existing installation: torchaudio 2.6.0+cu124\nUninstalling torchaudio-2.6.0+cu124:\n  Successfully uninstalled torchaudio-2.6.0+cu124\nLooking in indexes: https://download.pytorch.org/whl/cu124\nCollecting torch==2.6.0+cu124\n  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (28 kB)\nCollecting torchvision==0.21.0+cu124\n  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\nCollecting torchaudio==2.6.0+cu124\n  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (1.13.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (11.1.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0+cu124) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0+cu124) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0+cu124) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0+cu124) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.21.0+cu124) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.21.0+cu124) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.21.0+cu124) (2024.2.0)\nDownloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (768.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl (7.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchaudio, torchvision\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124\n"
    }
   ],
   "source": [
    "# First uninstall existing torch, torchvision, torchaudio\n",
    "!pip uninstall -y torch torchvision torchaudio\n",
    "\n",
    "# Install the specified versions from PyTorch's official CUDA 12.4 wheels\n",
    "!pip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:47:18.544040Z",
     "iopub.status.busy": "2025-05-27T17:47:18.543727Z",
     "iopub.status.idle": "2025-05-27T17:47:18.675785Z",
     "shell.execute_reply": "2025-05-27T17:47:18.674864Z",
     "shell.execute_reply.started": "2025-05-27T17:47:18.544007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mv panther Panther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:47:18.677219Z",
     "iopub.status.busy": "2025-05-27T17:47:18.676909Z",
     "iopub.status.idle": "2025-05-27T17:47:18.797512Z",
     "shell.execute_reply": "2025-05-27T17:47:18.796913Z",
     "shell.execute_reply.started": "2025-05-27T17:47:18.677182Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Panther\n"
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:47:18.798786Z",
     "iopub.status.busy": "2025-05-27T17:47:18.798527Z",
     "iopub.status.idle": "2025-05-27T17:47:18.804664Z",
     "shell.execute_reply": "2025-05-27T17:47:18.803976Z",
     "shell.execute_reply.started": "2025-05-27T17:47:18.798756Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Overwriting /kaggle/working/Panther/pawX/setup.py\n"
    }
   ],
   "source": "%%writefile /kaggle/working/Panther/pawX/setup.py\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name=\"pawX\",\n    ext_modules=[\n        CUDAExtension(\n            name=\"pawX\",\n            sources=[\n                \"skops.cpp\",\n                \"bindings.cpp\",\n                \"linear.cpp\",\n                \"linear_cuda.cu\",\n                \"cqrrpt.cpp\",\n                \"rsvd.cpp\",\n                \"attention.cpp\",\n                \"conv2d.cpp\"\n            ],\n            # Use system includes and libraries\n            include_dirs=[\"/usr/include/x86_64-linux-gnu\"],\n            library_dirs=[],\n            libraries=[\"openblas\"],\n            extra_compile_args={\"cxx\": [\"-O2\", \"-fopenmp\"], \"nvcc\": [\"-O2\"]},\n            extra_link_args=[\"-llapacke\", \"-lopenblas\"]\n        )\n    ],\n    cmdclass={\"build_ext\": BuildExtension},\n)"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:47:18.805703Z",
     "iopub.status.busy": "2025-05-27T17:47:18.805456Z",
     "iopub.status.idle": "2025-05-27T17:47:32.281721Z",
     "shell.execute_reply": "2025-05-27T17:47:32.280915Z",
     "shell.execute_reply.started": "2025-05-27T17:47:18.805672Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  liblapacke libtmglib-dev libtmglib3\nSuggested packages:\n  liblapack-doc\nThe following NEW packages will be installed:\n  liblapacke liblapacke-dev libtmglib-dev libtmglib3\n0 upgraded, 4 newly installed, 0 to remove and 87 not upgraded.\nNeed to get 1,071 kB of archives.\nAfter this operation, 12.3 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtmglib3 amd64 3.10.0-2ubuntu1 [144 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblapacke amd64 3.10.0-2ubuntu1 [435 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtmglib-dev amd64 3.10.0-2ubuntu1 [134 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblapacke-dev amd64 3.10.0-2ubuntu1 [358 kB]\nFetched 1,071 kB in 1s (1,014 kB/s)     \ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\ndebconf: falling back to frontend: Readline\nSelecting previously unselected package libtmglib3:amd64.\n(Reading database ... 129184 files and directories currently installed.)\nPreparing to unpack .../libtmglib3_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking libtmglib3:amd64 (3.10.0-2ubuntu1) ...\nSelecting previously unselected package liblapacke:amd64.\nPreparing to unpack .../liblapacke_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking liblapacke:amd64 (3.10.0-2ubuntu1) ...\nSelecting previously unselected package libtmglib-dev:amd64.\nPreparing to unpack .../libtmglib-dev_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking libtmglib-dev:amd64 (3.10.0-2ubuntu1) ...\nSelecting previously unselected package liblapacke-dev:amd64.\nPreparing to unpack .../liblapacke-dev_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking liblapacke-dev:amd64 (3.10.0-2ubuntu1) ...\nSetting up libtmglib3:amd64 (3.10.0-2ubuntu1) ...\nSetting up liblapacke:amd64 (3.10.0-2ubuntu1) ...\nSetting up libtmglib-dev:amd64 (3.10.0-2ubuntu1) ...\nSetting up liblapacke-dev:amd64 (3.10.0-2ubuntu1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n"
    }
   ],
   "source": [
    "!sudo apt-get install liblapacke-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:47:32.282983Z",
     "iopub.status.busy": "2025-05-27T17:47:32.282704Z",
     "iopub.status.idle": "2025-05-27T17:50:21.585520Z",
     "shell.execute_reply": "2025-05-27T17:50:21.584438Z",
     "shell.execute_reply.started": "2025-05-27T17:47:32.282952Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` and ``easy_install``.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://github.com/pypa/setuptools/issues/917 for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nEmitting ninja build file /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/build.ninja...\nCompiling objects...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/conv2d.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/conv2d.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/conv2d.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n/kaggle/working/Panther/pawX/conv2d.cpp: In function ‘at::Tensor sketched_conv2d_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const std::vector<long int>&, const std::vector<long int>&, const std::vector<long int>&, const std::optional<at::Tensor>&)’:\n/kaggle/working/Panther/pawX/conv2d.cpp:16:28: warning: unused variable ‘C’ [-Wunused-variable]\n   16 |     int64_t B = x.size(0), C = x.size(1), H = x.size(2), W = x.size(3);\n      |                            ^\n[2/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/linear.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[3/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/cqrrpt.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[4/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/attention.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n/kaggle/working/Panther/pawX/attention.cpp: In function ‘std::vector<at::Tensor> causal_numerator_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&)’:\n/kaggle/working/Panther/pawX/attention.cpp:103:9: warning: unused variable ‘B’ [-Wunused-variable]\n  103 |     int B = qs.size(0);\n      |         ^\n/kaggle/working/Panther/pawX/attention.cpp:105:9: warning: unused variable ‘H’ [-Wunused-variable]\n  105 |     int H = qs.size(2);\n      |         ^\n/kaggle/working/Panther/pawX/attention.cpp:106:9: warning: unused variable ‘M’ [-Wunused-variable]\n  106 |     int M = qs.size(3);\n      |         ^\n[5/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/bindings.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\nIn file included from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/Exceptions.h:12,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/python.h:11,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/extension.h:9,\n                 from /kaggle/working/Panther/pawX/attention.h:3,\n                 from /kaggle/working/Panther/pawX/bindings.cpp:1:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘class pybind11::class_<DistributionFamily>’:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2216:7:   required from ‘class pybind11::enum_<DistributionFamily>’\n/kaggle/working/Panther/pawX/bindings.cpp:24:58:   required from here\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1539:7: warning: ‘pybind11::class_<DistributionFamily>’ declared with greater visibility than its base ‘pybind11::detail::generic_type’ [-Wattributes]\n 1539 | class class_ : public detail::generic_type {\n      |       ^~~~~~\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘class pybind11::class_<Axis>’:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2216:7:   required from ‘class pybind11::enum_<Axis>’\n/kaggle/working/Panther/pawX/bindings.cpp:28:30:   required from here\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1539:7: warning: ‘pybind11::class_<Axis>’ declared with greater visibility than its base ‘pybind11::detail::generic_type’ [-Wattributes]\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘pybind11::class_< <template-parameter-1-1>, <template-parameter-1-2> >::class_(pybind11::handle, const char*, const Extra& ...) [with Extra = {}; type_ = DistributionFamily; options = {}]’:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2232:67:   required from ‘pybind11::enum_<Type>::enum_(const pybind11::handle&, const char*, const Extra& ...) [with Extra = {}; Type = DistributionFamily]’\n/kaggle/working/Panther/pawX/bindings.cpp:24:58:   required from here\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1599:28: warning: ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>’ declared with greater visibility than the type of its field ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>::<record capture>’ [-Wattributes]\n 1599 |             with_internals([&](internals &internals) {\n      |                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1600 |                 auto &instances = record.module_local ? get_local_internals().registered_types_cpp\n      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1601 |                                                       : internals.registered_types_cpp;\n      |                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1602 |                 instances[std::type_index(typeid(type_alias))]\n      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1603 |                     = instances[std::type_index(typeid(type))];\n      |                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1604 |             });\n      |             ~               \n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘pybind11::class_< <template-parameter-1-1>, <template-parameter-1-2> >::class_(pybind11::handle, const char*, const Extra& ...) [with Extra = {}; type_ = Axis; options = {}]’:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2232:67:   required from ‘pybind11::enum_<Type>::enum_(const pybind11::handle&, const char*, const Extra& ...) [with Extra = {}; Type = Axis]’\n/kaggle/working/Panther/pawX/bindings.cpp:28:30:   required from here\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1599:28: warning: ‘pybind11::class_<Axis>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>’ declared with greater visibility than the type of its field ‘pybind11::class_<Axis>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>::<record capture>’ [-Wattributes]\n[6/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/rsvd.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n/kaggle/working/Panther/pawX/rsvd.cpp: In function ‘at::Tensor powerSketch(const at::Tensor&, int64_t, int64_t, int64_t)’:\n/kaggle/working/Panther/pawX/rsvd.cpp:13:10: warning: unused variable ‘m’ [-Wunused-variable]\n   13 |     auto m = A.size(0);\n      |          ^\n/kaggle/working/Panther/pawX/rsvd.cpp: In function ‘std::tuple<at::Tensor, at::Tensor> blockedQB(const at::Tensor&, int64_t, int64_t, double)’:\n/kaggle/working/Panther/pawX/rsvd.cpp:50:10: warning: unused variable ‘n’ [-Wunused-variable]\n   50 |     auto n = A.size(1);\n      |          ^\n[7/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/skops.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[8/8] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o.d -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/linear_cuda.cu -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pawX-0.0.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n\u001b[0mObtaining file:///kaggle/working/Panther/pawX\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nInstalling collected packages: pawX\n  Attempting uninstall: pawX\n    Found existing installation: pawX 0.0.0\n    Uninstalling pawX-0.0.0:\n      Successfully uninstalled pawX-0.0.0\n  Running setup.py develop for pawX\nSuccessfully installed pawX-0.0.0\n"
    }
   ],
   "source": [
    "!cd /kaggle/working/Panther/pawX; python setup.py install\n",
    "!cd /kaggle/working/Panther/pawX; pip install --no-build-isolation -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:50:21.588306Z",
     "iopub.status.busy": "2025-05-27T17:50:21.588031Z",
     "iopub.status.idle": "2025-05-27T17:50:24.006149Z",
     "shell.execute_reply": "2025-05-27T17:50:24.005371Z",
     "shell.execute_reply.started": "2025-05-27T17:50:21.588284Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2.6.0+cu124\n3.2.0\n"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "import triton\n",
    "\n",
    "print(triton.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:50:24.007104Z",
     "iopub.status.busy": "2025-05-27T17:50:24.006804Z",
     "iopub.status.idle": "2025-05-27T17:50:24.010583Z",
     "shell.execute_reply": "2025-05-27T17:50:24.009888Z",
     "shell.execute_reply.started": "2025-05-27T17:50:24.007087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/kaggle/working/Panther\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:50:24.011590Z",
     "iopub.status.busy": "2025-05-27T17:50:24.011339Z",
     "iopub.status.idle": "2025-05-27T17:50:24.159323Z",
     "shell.execute_reply": "2025-05-27T17:50:24.158699Z",
     "shell.execute_reply.started": "2025-05-27T17:50:24.011569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "/kaggle/working/Panther\n"
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:50:24.160384Z",
     "iopub.status.busy": "2025-05-27T17:50:24.160209Z",
     "iopub.status.idle": "2025-05-27T17:50:30.410280Z",
     "shell.execute_reply": "2025-05-27T17:50:30.409567Z",
     "shell.execute_reply.started": "2025-05-27T17:50:24.160366Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Collecting botorch\n  Downloading botorch-0.14.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from botorch) (4.13.2)\nCollecting pyre_extensions (from botorch)\n  Downloading pyre_extensions-0.0.32-py3-none-any.whl.metadata (4.0 kB)\nCollecting gpytorch==1.14 (from botorch)\n  Downloading gpytorch-1.14-py3-none-any.whl.metadata (8.0 kB)\nCollecting linear_operator==0.6 (from botorch)\n  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from botorch) (2.6.0+cu124)\nCollecting pyro-ppl>=1.8.4 (from botorch)\n  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from botorch) (1.15.2)\nRequirement already satisfied: multipledispatch in /usr/local/lib/python3.11/dist-packages (from botorch) (1.0.0)\nRequirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from botorch) (3.6.0)\nCollecting jaxtyping (from gpytorch==1.14->botorch)\n  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.11/dist-packages (from gpytorch==1.14->botorch) (1.3.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from gpytorch==1.14->botorch) (1.2.2)\nRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (3.4.0)\nCollecting pyro-api>=0.1.1 (from pyro-ppl>=1.8.4->botorch)\n  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (1.13.1)\nRequirement already satisfied: typing-inspect in /usr/local/lib/python3.11/dist-packages (from pyre_extensions->botorch) (0.9.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2.4.1)\nCollecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch==1.14->botorch)\n  Downloading wadler_lindig-0.1.6-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.1->botorch) (3.0.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch==1.14->botorch) (1.5.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect->pyre_extensions->botorch) (1.1.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\nDownloading botorch-0.14.0-py3-none-any.whl (738 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m738.3/738.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading gpytorch-1.14-py3-none-any.whl (277 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.7/277.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hDownloading linear_operator-0.6-py3-none-any.whl (176 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyre_extensions-0.0.32-py3-none-any.whl (12 kB)\nDownloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\nDownloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wadler_lindig-0.1.6-py3-none-any.whl (20 kB)\nInstalling collected packages: pyro-api, wadler-lindig, pyre_extensions, jaxtyping, linear_operator, pyro-ppl, gpytorch, botorch\nSuccessfully installed botorch-0.14.0 gpytorch-1.14 jaxtyping-0.3.2 linear_operator-0.6 pyre_extensions-0.0.32 pyro-api-0.1.2 pyro-ppl-1.9.1 wadler-lindig-0.1.6\n"
    }
   ],
   "source": [
    "!pip install botorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T18:40:17.628676Z",
     "iopub.status.busy": "2025-05-27T18:40:17.628393Z",
     "iopub.status.idle": "2025-05-27T18:41:56.285976Z",
     "shell.execute_reply": "2025-05-27T18:41:56.285337Z",
     "shell.execute_reply.started": "2025-05-27T18:40:17.628659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "SKAutoTuner Example Script\n==========================\n\n--- Original Model Summary ---\n================================================================================\nModel Summary (Total trainable parameters: 538,999,808)\nSketched layers: 0\n--------------------------------------------------------------------------------\nLayer Name                               Layer Type                Parameters      Sketched\n--------------------------------------------------------------------------------\nconv1                                    Conv2d                    28,672          \nrelu1                                    ReLU                      0               \npool1                                    MaxPool2d                 0               \nflatten                                  Flatten                   0               \nfc1                                      Linear                    536,872,960     \nrelu2                                    ReLU                      0               \nfc2                                      Linear                    2,098,176       \n================================================================================\n\n--- Initializing SKAutoTuner for Tuning ---\n\n--- Starting Tuning Process (tune) ---\nTuning layer: conv1\nTrying parameters: {'num_terms': 10, 'low_rank': 5}\nReplaced conv1 with sketched version using parameters: {'num_terms': 10, 'low_rank': 5}\nrun: 1/1 - accuracy_score: 0.666888437030501, speed_score: 0.0007421272323112936, final score: 0.0007421272323112936\nTried parameters: {'num_terms': 10, 'low_rank': 5}, accuracy_score: 0.666888437030501, speed_score: 0.0007421272323112936, final score: 0.0007421272323112936\nTrying parameters: {'num_terms': 10, 'low_rank': 8}\nReplaced conv1 with sketched version using parameters: {'num_terms': 10, 'low_rank': 8}\nrun: 1/1 - accuracy_score: 0.6651590880588061, speed_score: 0.0007421272323112936, final score: 0.0007421272323112936\nTried parameters: {'num_terms': 10, 'low_rank': 8}, accuracy_score: 0.6651590880588061, speed_score: 0.0007421272323112936, final score: 0.0007421272323112936\nTrying parameters: {'num_terms': 20, 'low_rank': 5}\nReplaced conv1 with sketched version using parameters: {'num_terms': 20, 'low_rank': 5}\nrun: 1/1 - accuracy_score: 0.6784114316166169, speed_score: 0.0007421272323112936, final score: 0.0007421272323112936\nTried parameters: {'num_terms': 20, 'low_rank': 5}, accuracy_score: 0.6784114316166169, speed_score: 0.0007421272323112936, final score: 0.0007421272323112936\nTrying parameters: {'num_terms': 20, 'low_rank': 8}\nReplaced conv1 with sketched version using parameters: {'num_terms': 20, 'low_rank': 8}\nrun: 1/1 - accuracy_score: 0.6751783350058592, speed_score: 0.0007421272323112936, final score: 0.0007421272323112936\nTried parameters: {'num_terms': 20, 'low_rank': 8}, accuracy_score: 0.6751783350058592, speed_score: 0.0007421272323112936, final score: 0.0007421272323112936\n  Best parameters for conv1: {'num_terms': 10, 'low_rank': 5}, score: 0.0007421272323112936\nTuning layer: fc1\nTrying parameters: {'num_terms': 15, 'low_rank': 6}\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/kaggle/working/Panther/panther/utils/SkAutoTuner/SKAutoTuner.py:238: UserWarning: Tensor Core not utilized. Ensure 'in_features', 'out_features', and 'low_rank' are multiples of 16 (current: 262144, 2048, 6).\n  sketched_layer = sketched_class(\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Replaced fc1 with sketched version using parameters: {'num_terms': 15, 'low_rank': 6}\nrun: 1/1 - accuracy_score: 0.6602254944273722, speed_score: 0.1863196649227146, final score: 0.1863196649227146\nTried parameters: {'num_terms': 15, 'low_rank': 6}, accuracy_score: 0.6602254944273722, speed_score: 0.1863196649227146, final score: 0.1863196649227146\nTrying parameters: {'num_terms': 15, 'low_rank': 10}\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/kaggle/working/Panther/panther/utils/SkAutoTuner/SKAutoTuner.py:238: UserWarning: Tensor Core not utilized. Ensure 'in_features', 'out_features', and 'low_rank' are multiples of 16 (current: 262144, 2048, 10).\n  sketched_layer = sketched_class(\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Replaced fc1 with sketched version using parameters: {'num_terms': 15, 'low_rank': 10}\nrun: 1/1 - accuracy_score: 0.6480986827490083, speed_score: None, final score: -inf\nTried parameters: {'num_terms': 15, 'low_rank': 10}, accuracy_score: -inf, speed_score: -inf, final score: -inf\nTrying parameters: {'num_terms': 25, 'low_rank': 6}\nReplaced fc1 with sketched version using parameters: {'num_terms': 25, 'low_rank': 6}\nrun: 1/1 - accuracy_score: 0.6856759717806954, speed_score: 0.1863196649227146, final score: 0.1863196649227146\nTried parameters: {'num_terms': 25, 'low_rank': 6}, accuracy_score: 0.6856759717806954, speed_score: 0.1863196649227146, final score: 0.1863196649227146\nTrying parameters: {'num_terms': 25, 'low_rank': 10}\nReplaced fc1 with sketched version using parameters: {'num_terms': 25, 'low_rank': 10}\nrun: 1/1 - accuracy_score: 0.6660662545215785, speed_score: 0.1863196649227146, final score: 0.1863196649227146\nTried parameters: {'num_terms': 25, 'low_rank': 10}, accuracy_score: 0.6660662545215785, speed_score: 0.1863196649227146, final score: 0.1863196649227146\n  Best parameters for fc1: {'num_terms': 15, 'low_rank': 6}, score: 0.1863196649227146\nTuning finished.\n\n--- Best Parameters Found (get_best_params) ---\nLayer: conv1, Best Params: {'num_terms': 10, 'low_rank': 5}\nLayer: fc1, Best Params: {'num_terms': 15, 'low_rank': 6}\n\n--- Tuning Results DataFrame (get_results_dataframe) ---\n  layer_name  num_terms  low_rank     score\n0      conv1         10         5  0.000742\n1      conv1         10         8  0.000742\n2      conv1         20         5  0.000742\n3      conv1         20         8  0.000742\n4        fc1         15         6  0.186320\n5        fc1         15        10      -inf\n6        fc1         25         6  0.186320\n7        fc1         25        10  0.186320\n\n--- Applying Best Parameters to the Model (apply_best_params) ---\nreplaced conv1 with SKConv2d\nreplaced fc1 with SKLinear\nBest parameters applied. Model summary after tuning:\n================================================================================\nModel Summary (Total trainable parameters: 25,881,228)\nSketched layers: 2\n--------------------------------------------------------------------------------\nLayer Name                               Layer Type                Parameters      Sketched\n--------------------------------------------------------------------------------\nconv1                                    SKConv2d                  3,724           ✓\nrelu1                                    ReLU                      0               \npool1                                    MaxPool2d                 0               \nflatten                                  Flatten                   0               \nfc1                                      SKLinear                  23,779,328      ✓\nrelu2                                    ReLU                      0               \nfc2                                      Linear                    2,098,176       \n================================================================================\n\n--- Visualizing Tuning Results (visualize_tuning_results) ---\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/kaggle/working/Panther/panther/utils/SkAutoTuner/SKAutoTuner.py:769: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n/kaggle/working/Panther/panther/utils/SkAutoTuner/SKAutoTuner.py:773: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Visualization saved to tuning_visualizations/tuning_visualization.png\nTuning visualization saved to tuning_visualizations/tuning_visualization.png\nTo view the plot, open the saved image file.\n\n--- Explicit call to get_model_summary (on tuned model) ---\nTotal parameters in tuned model: 25881228\nNumber of sketched layers: 2\n\n--- Saving Tuning Results (save_tuning_results) ---\nTuning results saved to tuning_results_data/tuning_session.pkl\nTuning results (including best_params) saved to tuning_results_data/tuning_session.pkl\n\n--- Loading Tuning Results (load_tuning_results) ---\nSuccessfully loaded tuning results from tuning_results_data/tuning_session.pkl into a new tuner instance.\nBest parameters from loaded results:\n  Layer: conv1, Best Params: {'num_terms': 10, 'low_rank': 5}\n  Layer: fc1, Best Params: {'num_terms': 15, 'low_rank': 6}\n\n--- Exporting Tuned Model State (export_model) ---\nModel exported to exported_models/tuned_simple_model.pth\nTuned model state_dict exported to exported_models/tuned_simple_model.pth\n\n--- Visualizing Parameter Distribution of Tuned Model (visualize_parameter_distribution) ---\nParameter distribution visualization saved to tuning_visualizations/tuned_model_parameter_distribution.png\nParameter distribution visualization saved to tuning_visualizations/tuned_model_parameter_distribution.png\n\n--- Getting Inference Benchmark (get_inference_benchmark) ---\n\nBenchmarking original model:\n  Original Model Benchmark (avg time): 0.094066 s, FPS: 10.63\n\nBenchmarking tuned model (from main 'tuner' instance):\n  Tuned Model Benchmark (avg time): 0.019018 s, FPS: 52.58\n  Potential speedup: 79.78% (based on mean inference time)\n\n--- Demonstrating Replace Without Tuning (replace_without_tuning) ---\nModel summary before replace_without_tuning:\n================================================================================\nModel Summary (Total trainable parameters: 538,999,808)\nSketched layers: 0\n--------------------------------------------------------------------------------\nLayer Name                               Layer Type                Parameters      Sketched\n--------------------------------------------------------------------------------\nconv1                                    Conv2d                    28,672          \nrelu1                                    ReLU                      0               \npool1                                    MaxPool2d                 0               \nflatten                                  Flatten                   0               \nfc1                                      Linear                    536,872,960     \nrelu2                                    ReLU                      0               \nfc2                                      Linear                    2,098,176       \n================================================================================\nReplaced conv1 with sketched version using parameters: {'num_terms': 10, 'low_rank': 5}\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/kaggle/working/Panther/panther/utils/SkAutoTuner/SKAutoTuner.py:238: UserWarning: Tensor Core not utilized. Ensure 'in_features', 'out_features', and 'low_rank' are multiples of 16 (current: 262144, 2048, 6).\n  sketched_layer = sketched_class(\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Replaced fc1 with sketched version using parameters: {'num_terms': 15, 'low_rank': 6}\n\nModel summary after replace_without_tuning:\n================================================================================\nModel Summary (Total trainable parameters: 25,881,228)\nSketched layers: 2\n--------------------------------------------------------------------------------\nLayer Name                               Layer Type                Parameters      Sketched\n--------------------------------------------------------------------------------\nconv1                                    SKConv2d                  3,724           ✓\nrelu1                                    ReLU                      0               \npool1                                    MaxPool2d                 0               \nflatten                                  Flatten                   0               \nfc1                                      SKLinear                  23,779,328      ✓\nrelu2                                    ReLU                      0               \nfc2                                      Linear                    2,098,176       \n================================================================================\n\n--- Additional SKAutoTuner features demonstrated ---\n\n- Model Comparison (print_comparison_summary - uses compare_models internally):\n================================================================================\nModel Comparison Summary\n--------------------------------------------------------------------------------\nOriginal model parameters: 538,999,808\nTuned model parameters:    25,881,228\nParameter reduction:       513,118,580 (95.20%)\n--------------------------------------------------------------------------------\nModified Layers:\nLayer Name                               Original Type        Tuned Type           Param Reduction\n--------------------------------------------------------------------------------\nfc1                                      Linear               SKLinear             513,093,632 (95.57%)\nconv1                                    Conv2d               SKConv2d             24,948 (87.01%)\n================================================================================\n\n- Explicit call to compare_models to get the raw dictionary:\n  Parameter reduction from compare_models dict: 95.20%\n\n--- SKAutoTuner Example Script Finished ---\nNote: This script uses dummy evaluation functions.\nIn a real application, provide actual model evaluation logic.\nEnsure 'matplotlib' and 'pandas' are installed to see visualizations and dataframes.\n"
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from panther.utils import *\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "# 1. Define a simple PyTorch model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=1024, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Calculate flattened size: 1024 channels, image H/2, W/2. Assume 32x32 input -> 16x16 after pool.\n",
    "        # (1024 * 16 * 16)\n",
    "        self.fc1 = nn.Linear(1024 * 16 * 16, 2048)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(2048, 1024)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu2(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 2. Define evaluation functions\n",
    "def dummy_accuracy_eval_func(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    A dummy accuracy evaluation function.\n",
    "    In a real scenario, this would evaluate the model on a validation dataset.\n",
    "    This function gives slightly higher accuracy if layers are sketched.\n",
    "    \"\"\"\n",
    "    base_accuracy = 0.6\n",
    "    sketched_bonus = 0.0\n",
    "    num_sketched = 0\n",
    "    for module in model.modules():\n",
    "        if \"SK\" in type(module).__name__:  # Check if it's a sketched layer\n",
    "            sketched_bonus += 0.05\n",
    "            num_sketched += 1\n",
    "            # Example: Favor specific sketch parameters for variety in results\n",
    "            if hasattr(module, \"num_terms\") and hasattr(module, \"low_rank\"):\n",
    "                if module.num_terms > 15:  # Arbitrary condition for demo\n",
    "                    sketched_bonus += 0.02\n",
    "                if module.low_rank < 10:  # Arbitrary condition for demo\n",
    "                    sketched_bonus += 0.01\n",
    "\n",
    "    # Simulate some noise or dependency on parameters\n",
    "    if num_sketched > 0:\n",
    "        # Small random factor to make tuning non-deterministic if not for seed\n",
    "        return min(1.0, base_accuracy + sketched_bonus + random.uniform(-0.01, 0.01))\n",
    "    return base_accuracy + random.uniform(-0.01, 0.01)\n",
    "\n",
    "\n",
    "def dummy_optimization_eval_func(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    A dummy optimization evaluation function (e.g., inference speed).\n",
    "    Higher is better. This function simulates that sketched layers are faster.\n",
    "    \"\"\"\n",
    "    simulated_latency = 0.05  # Base latency\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            params = sum(p.numel() for p in module.parameters())\n",
    "            if \"SK\" in type(module).__name__:  # Sketched layer\n",
    "                simulated_latency += 0.0000005 * params  # Sketched layers are faster\n",
    "            else:  # Original layer\n",
    "                simulated_latency += 0.0000025 * params\n",
    "\n",
    "    # Score is inverse of latency (higher score = faster)\n",
    "    return 1.0 / simulated_latency if simulated_latency > 0 else 0.0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"SKAutoTuner Example Script\")\n",
    "    print(\"==========================\")\n",
    "\n",
    "    # Create dummy input for model (batch_size=1, 3 channels, 32x32 image)\n",
    "    dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "    # --- Initial Model ---\n",
    "    original_model = SimpleModel()\n",
    "    print(\"\\n--- Original Model Summary ---\")\n",
    "    # To use print_model_summary, we need a tuner instance with the model\n",
    "    temp_tuner_orig = SKAutoTuner(original_model, TuningConfigs([]), lambda m: 0.0)\n",
    "    temp_tuner_orig.print_model_summary()\n",
    "\n",
    "    # --- Configuration for Tuning ---\n",
    "    # Define which layers to tune and with what parameters\n",
    "    # Note: Keep parameter ranges small for quick example execution.\n",
    "    config1 = LayerConfig(\n",
    "        layer_names=[\"conv1\"],\n",
    "        params={\"num_terms\": [10, 20], \"low_rank\": [5, 8]},\n",
    "        separate=True,  # Tune this layer group separately\n",
    "        copy_weights=True,\n",
    "    )\n",
    "    config2 = LayerConfig(\n",
    "        layer_names=[\"fc1\"],\n",
    "        params={\"num_terms\": [15, 25], \"low_rank\": [6, 10]},\n",
    "        separate=True,\n",
    "        copy_weights=True,\n",
    "    )\n",
    "    tuning_configs = TuningConfigs(configs=[config1, config2])\n",
    "\n",
    "    # Create a copy of the model for the main tuning process\n",
    "    model_for_tuning = copy.deepcopy(original_model)\n",
    "\n",
    "    # --- Instantiate SKAutoTuner ---\n",
    "    print(\"\\n--- Initializing SKAutoTuner for Tuning ---\")\n",
    "    tuner = SKAutoTuner(\n",
    "        model=model_for_tuning,\n",
    "        configs=tuning_configs,\n",
    "        accuracy_eval_func=dummy_accuracy_eval_func,\n",
    "        optmization_eval_func=dummy_optimization_eval_func,\n",
    "        accuracy_threshold=0.65,  # Aim for at least this accuracy\n",
    "        verbose=True,\n",
    "        num_runs_per_param=1,  # For faster example execution\n",
    "    )\n",
    "\n",
    "    # --- 1. Tune the model ---\n",
    "    print(\"\\n--- Starting Tuning Process (tune) ---\")\n",
    "    tuner.tune()\n",
    "    print(\"Tuning finished.\")\n",
    "\n",
    "    # --- 2. Get Best Parameters ---\n",
    "    print(\"\\n--- Best Parameters Found (get_best_params) ---\")\n",
    "    best_params = tuner.get_best_params()\n",
    "    for layer_name, params_info in best_params.items():\n",
    "        print(f\"Layer: {layer_name}, Best Params: {params_info['params']}\")\n",
    "\n",
    "    # --- 3. Get Results DataFrame ---\n",
    "    print(\"\\n--- Tuning Results DataFrame (get_results_dataframe) ---\")\n",
    "    # This requires pandas to be installed.\n",
    "    try:\n",
    "        results_df = tuner.get_results_dataframe()\n",
    "        print(results_df.to_string())\n",
    "    except ImportError:\n",
    "        print(\"Pandas not installed. Skipping get_results_dataframe().\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate DataFrame: {e}\")\n",
    "\n",
    "    # --- 4. Apply Best Parameters ---\n",
    "    print(\"\\n--- Applying Best Parameters to the Model (apply_best_params) ---\")\n",
    "    # tuner.model is modified in-place by apply_best_params()\n",
    "    tuned_model_explicit_return = tuner.apply_best_params()\n",
    "    print(\"Best parameters applied. Model summary after tuning:\")\n",
    "    tuner.print_model_summary()  # Shows the state of tuner.model\n",
    "\n",
    "    # --- 5. Visualize Tuning Results ---\n",
    "    print(\"\\n--- Visualizing Tuning Results (visualize_tuning_results) ---\")\n",
    "    # This requires matplotlib and pandas.\n",
    "    # Create a directory for plots if it doesn't exist.\n",
    "    viz_dir = \"tuning_visualizations\"\n",
    "    if not os.path.exists(viz_dir):\n",
    "        os.makedirs(viz_dir)\n",
    "    viz_path = os.path.join(viz_dir, \"tuning_visualization.png\")\n",
    "    try:\n",
    "        tuner.visualize_tuning_results(save_path=viz_path, show_plot=False)\n",
    "        print(f\"Tuning visualization saved to {viz_path}\")\n",
    "        print(\"To view the plot, open the saved image file.\")\n",
    "    except ImportError:\n",
    "        print(\n",
    "            \"Matplotlib or Pandas not installed. Skipping visualize_tuning_results().\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Could not visualize results: {e}\")\n",
    "        if \"No variable parameters found to visualize\" in str(e):\n",
    "            print(\n",
    "                \"This can happen if all parameter combinations resulted in the same score or only one combination was tried.\"\n",
    "            )\n",
    "\n",
    "    # --- 7. Get Model Summary (Explicitly from Tuned Model) ---\n",
    "    print(\"\\n--- Explicit call to get_model_summary (on tuned model) ---\")\n",
    "    # tuner.print_model_summary() was already called after apply_best_params,\n",
    "    # this shows how to get the raw dictionary.\n",
    "    model_summary_dict = tuner.get_model_summary()\n",
    "    print(f\"Total parameters in tuned model: {model_summary_dict['total_params']}\")\n",
    "    print(f\"Number of sketched layers: {model_summary_dict['sketched_layers']}\")\n",
    "    # print(f\"Full summary dict: {model_summary_dict}\") # Uncomment to see full structure\n",
    "\n",
    "    # --- 8. Save Tuning Results ---\n",
    "    print(\"\\n--- Saving Tuning Results (save_tuning_results) ---\")\n",
    "    results_dir = \"tuning_results_data\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "    results_file_path = os.path.join(results_dir, \"tuning_session.pkl\")\n",
    "    tuner.save_tuning_results(results_file_path)\n",
    "    print(f\"Tuning results (including best_params) saved to {results_file_path}\")\n",
    "\n",
    "    # --- 9. Load Tuning Results (Example on a new Tuner instance) ---\n",
    "    print(\"\\n--- Loading Tuning Results (load_tuning_results) ---\")\n",
    "    # Create a new model and tuner instance to load into\n",
    "    model_for_loading = copy.deepcopy(original_model)\n",
    "    tuner_for_loading = SKAutoTuner(\n",
    "        model=model_for_loading,\n",
    "        configs=tuning_configs,  # Important: Configs should match the saved session for meaningful application\n",
    "        accuracy_eval_func=dummy_accuracy_eval_func,  # Required by constructor\n",
    "        verbose=False,  # Keep output clean for this demo part\n",
    "    )\n",
    "    try:\n",
    "        tuner_for_loading.load_tuning_results(results_file_path)\n",
    "        print(\n",
    "            f\"Successfully loaded tuning results from {results_file_path} into a new tuner instance.\"\n",
    "        )\n",
    "        loaded_best_params = tuner_for_loading.get_best_params()\n",
    "        print(\"Best parameters from loaded results:\")\n",
    "        if loaded_best_params:\n",
    "            for layer_name, params_info in loaded_best_params.items():\n",
    "                # Check if params_info is not None and 'params' key exists\n",
    "                if params_info and \"params\" in params_info:\n",
    "                    print(\n",
    "                        f\"  Layer: {layer_name}, Best Params: {params_info['params']}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"  Layer: {layer_name}, No best params data found in loaded results for this layer.\"\n",
    "                    )\n",
    "        else:\n",
    "            print(\n",
    "                \"  No best parameters were loaded (or best_params was empty in the file).\"\n",
    "            )\n",
    "\n",
    "        # Optionally, apply these loaded parameters\n",
    "        # tuner_for_loading.apply_best_params()\n",
    "        # print(\"Model summary after loading results and applying them:\")\n",
    "        # tuner_for_loading.print_model_summary()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Could not load results - File not found: {results_file_path}\")\n",
    "    except (\n",
    "        Exception\n",
    "    ) as e:  # Catch other potential errors like pickle issues or invalid format\n",
    "        print(f\"ERROR: Could not load results: {e}\")\n",
    "\n",
    "    # --- 10. Export Tuned Model State ---\n",
    "    # The 'tuner' instance still holds the model that was tuned and had best params applied\n",
    "    print(\"\\n--- Exporting Tuned Model State (export_model) ---\")\n",
    "    model_export_dir = \"exported_models\"\n",
    "    if not os.path.exists(model_export_dir):\n",
    "        os.makedirs(model_export_dir)\n",
    "    tuned_model_path = os.path.join(model_export_dir, \"tuned_simple_model.pth\")\n",
    "    tuner.export_model(tuned_model_path)  # Exports tuner.model.state_dict()\n",
    "    print(f\"Tuned model state_dict exported to {tuned_model_path}\")\n",
    "\n",
    "    # --- 11. Visualize Parameter Distribution of Tuned Model ---\n",
    "    print(\n",
    "        \"\\n--- Visualizing Parameter Distribution of Tuned Model (visualize_parameter_distribution) ---\"\n",
    "    )\n",
    "    # This uses the model currently in the 'tuner' instance (which is the tuned one)\n",
    "    param_dist_path = os.path.join(viz_dir, \"tuned_model_parameter_distribution.png\")\n",
    "    try:\n",
    "        tuner.visualize_parameter_distribution(\n",
    "            save_path=param_dist_path, show_plot=False\n",
    "        )\n",
    "        print(f\"Parameter distribution visualization saved to {param_dist_path}\")\n",
    "    except ImportError:\n",
    "        print(\n",
    "            \"Matplotlib or Pandas not installed. Skipping visualize_parameter_distribution().\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Could not visualize parameter distribution: {e}\")\n",
    "\n",
    "    # --- 12. Get Inference Benchmark ---\n",
    "    print(\"\\n--- Getting Inference Benchmark (get_inference_benchmark) ---\")\n",
    "    # Prepare a dummy input tensor for benchmarking\n",
    "    # Ensure it's on the same device as the model if using GPU later\n",
    "    benchmark_input = torch.randn(1, 3, 32, 32)  # Same as dummy_input earlier\n",
    "\n",
    "    print(\"\\nBenchmarking original model:\")\n",
    "    original_model_for_bench = copy.deepcopy(original_model)  # Use a fresh copy\n",
    "    # Need a tuner instance to call get_inference_benchmark\n",
    "    tuner_for_original_bench = SKAutoTuner(\n",
    "        original_model_for_bench, TuningConfigs([]), lambda m: 0.0\n",
    "    )\n",
    "    original_benchmark_results = None  # Initialize for robust access later\n",
    "    try:\n",
    "        original_benchmark_results = tuner_for_original_bench.get_inference_benchmark(\n",
    "            benchmark_input, num_runs=50, warm_up=5\n",
    "        )\n",
    "        print(\n",
    "            f\"  Original Model Benchmark (avg time): {original_benchmark_results.get('mean_time', 'N/A'):.6f} s, FPS: {original_benchmark_results.get('fps', 'N/A'):.2f}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not benchmark original model: {e}\")\n",
    "\n",
    "    print(\"\\nBenchmarking tuned model (from main 'tuner' instance):\")\n",
    "    tuned_benchmark_results = None  # Initialize for robust access later\n",
    "    try:\n",
    "        # tuner.model is already the tuned one\n",
    "        tuned_benchmark_results = tuner.get_inference_benchmark(\n",
    "            benchmark_input, num_runs=50, warm_up=5\n",
    "        )\n",
    "        print(\n",
    "            f\"  Tuned Model Benchmark (avg time): {tuned_benchmark_results.get('mean_time', 'N/A'):.6f} s, FPS: {tuned_benchmark_results.get('fps', 'N/A'):.2f}\"\n",
    "        )\n",
    "\n",
    "        # Compare benchmark results\n",
    "        if (\n",
    "            original_benchmark_results\n",
    "            and tuned_benchmark_results\n",
    "            and original_benchmark_results.get(\"mean_time\") is not None\n",
    "            and tuned_benchmark_results.get(\"mean_time\") is not None\n",
    "        ):\n",
    "            original_time = original_benchmark_results[\"mean_time\"]\n",
    "            tuned_time = tuned_benchmark_results[\"mean_time\"]\n",
    "            if original_time > 0:  # Avoid division by zero\n",
    "                speedup = (original_time - tuned_time) / original_time * 100\n",
    "                print(\n",
    "                    f\"  Potential speedup: {speedup:.2f}% (based on mean inference time)\"\n",
    "                )\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not benchmark tuned model: {e}\")\n",
    "\n",
    "    # --- 6. Demonstrate replace_without_tuning ---\n",
    "    print(\"\\n--- Demonstrating Replace Without Tuning (replace_without_tuning) ---\")\n",
    "    # Use a fresh copy of the original model for this demonstration\n",
    "    model_for_replace = copy.deepcopy(original_model)\n",
    "\n",
    "    print(\"Model summary before replace_without_tuning:\")\n",
    "    temp_tuner_replace_before = SKAutoTuner(\n",
    "        model_for_replace,\n",
    "        TuningConfigs([]),\n",
    "        lambda m: 0.0,\n",
    "        # ^ This tuner is just for print_model_summary, uses empty configs\n",
    "    )\n",
    "    temp_tuner_replace_before.print_model_summary()\n",
    "\n",
    "    # Re-initialize tuner with the fresh model and original configs\n",
    "    # No need for eval funcs or accuracy_threshold for replace_without_tuning\n",
    "    tuner_for_replace = SKAutoTuner(\n",
    "        model=model_for_replace,\n",
    "        configs=tuning_configs,  # <--- Key point: The full 'tuning_configs' IS PROVIDED HERE\n",
    "        accuracy_eval_func=dummy_accuracy_eval_func,  # Required by __init__, though not used by this specific method\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # replace_without_tuning uses the *first* parameter from the lists in LayerConfig\n",
    "    replaced_model_explicit_return = (\n",
    "        tuner_for_replace.replace_without_tuning()\n",
    "    )  # This calls the method on tuner_for_replace\n",
    "    print(\"\\nModel summary after replace_without_tuning:\")\n",
    "    tuner_for_replace.print_model_summary()  # Shows the state of tuner_for_replace.model\n",
    "\n",
    "    print(\"\\n--- Additional SKAutoTuner features demonstrated ---\")\n",
    "    # print_comparison_summary was already called, it uses compare_models internally.\n",
    "    # This shows how to get the raw dictionary from compare_models.\n",
    "    print(\n",
    "        \"\\n- Model Comparison (print_comparison_summary - uses compare_models internally):\"\n",
    "    )\n",
    "    # Ensure original_model is pristine.\n",
    "    tuner.print_comparison_summary(original_model=original_model)\n",
    "\n",
    "    print(\"\\n- Explicit call to compare_models to get the raw dictionary:\")\n",
    "    try:\n",
    "        # The 'tuner' instance holds the tuned model.\n",
    "        # 'original_model' is the one from the beginning of the script.\n",
    "        comparison_dict = tuner.compare_models(original_model=original_model)\n",
    "        print(\n",
    "            f\"  Parameter reduction from compare_models dict: {comparison_dict.get('param_reduction_percent', 0):.2f}%\"\n",
    "        )\n",
    "        # print(f\"  Full comparison_dict: {comparison_dict}\") # Uncomment to see all details\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not get comparison dictionary: {e}\")\n",
    "\n",
    "    print(\"\\n--- SKAutoTuner Example Script Finished ---\")\n",
    "    print(\"Note: This script uses dummy evaluation functions.\")\n",
    "    print(\"In a real application, provide actual model evaluation logic.\")\n",
    "    print(\n",
    "        \"Ensure 'matplotlib' and 'pandas' are installed to see visualizations and dataframes.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}