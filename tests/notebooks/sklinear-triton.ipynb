{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"github_repos_wildcard\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-19T23:38:07.036106Z","iopub.execute_input":"2025-03-19T23:38:07.036441Z","iopub.status.idle":"2025-03-19T23:38:07.148963Z","shell.execute_reply.started":"2025-03-19T23:38:07.036412Z","shell.execute_reply":"2025-03-19T23:38:07.148012Z"}},"outputs":[],"execution_count":120},{"cell_type":"code","source":"# import shutil\n# shutil.rmtree(\"panther\")\n\nGITHUB_TOKEN = secret_value_0\nUSER = \"gaserSami\"\nCLONE_URL = f\"https://{USER}:{GITHUB_TOKEN}@github.com/{USER}/panther.git\"\nget_ipython().system(f\"git clone --branch testing_triton {CLONE_URL}\")\n\nimport sys\nsys.path.append(\"panther\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T23:38:07.150302Z","iopub.execute_input":"2025-03-19T23:38:07.150636Z","iopub.status.idle":"2025-03-19T23:38:07.308630Z","shell.execute_reply.started":"2025-03-19T23:38:07.150602Z","shell.execute_reply":"2025-03-19T23:38:07.307705Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'panther' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":121},{"cell_type":"code","source":"!pip install --upgrade \\\n  torch torchvision torchaudio \\\n  --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T23:38:07.310602Z","iopub.execute_input":"2025-03-19T23:38:07.310861Z","iopub.status.idle":"2025-03-19T23:38:11.659864Z","shell.execute_reply.started":"2025-03-19T23:38:07.310837Z","shell.execute_reply":"2025-03-19T23:38:11.658950Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.6.0+cu118)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.21.0+cu118)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.6.0+cu118)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.87)\nRequirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch) (11.11.3.6)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.0.86)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.1.48)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.5.86)\nRequirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.86)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":122},{"cell_type":"code","source":"# import os\n# os.environ[\"TORCH_COMPILE_DEBUG\"] = \"1\"\n# print(os.environ[\"TORCH_COMPILE_DEBUG\"], \"\\n\")\n\nimport torch\nprint(torch.__version__)\nimport triton\nprint(triton.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T23:38:11.661356Z","iopub.execute_input":"2025-03-19T23:38:11.661677Z","iopub.status.idle":"2025-03-19T23:38:11.666723Z","shell.execute_reply.started":"2025-03-19T23:38:11.661651Z","shell.execute_reply":"2025-03-19T23:38:11.665907Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu118\n3.2.0\n","output_type":"stream"}],"execution_count":123},{"cell_type":"code","source":"!export LC_ALL=\"en_US.UTF-8\"\n!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n!ldconfig /usr/lib64-nvidia","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T23:38:11.667645Z","iopub.execute_input":"2025-03-19T23:38:11.667971Z","iopub.status.idle":"2025-03-19T23:38:12.326538Z","shell.execute_reply.started":"2025-03-19T23:38:11.667918Z","shell.execute_reply":"2025-03-19T23:38:12.325576Z"}},"outputs":[{"name":"stdout","text":"/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n","output_type":"stream"}],"execution_count":124},{"cell_type":"code","source":"import time\nimport math\nfrom typing import Any, Dict, List, Optional, Union, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport torch._dynamo\nimport torch.nn as nn\n\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.nn import init\n\nfrom panther.nn import SKLinear\nfrom panther.random import scaled_sign_sketch as gen_U","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T23:38:12.328815Z","iopub.execute_input":"2025-03-19T23:38:12.329098Z","iopub.status.idle":"2025-03-19T23:38:12.334159Z","shell.execute_reply.started":"2025-03-19T23:38:12.329074Z","shell.execute_reply":"2025-03-19T23:38:12.333281Z"}},"outputs":[],"execution_count":125},{"cell_type":"code","source":"import triton.language as tl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T23:38:12.335455Z","iopub.execute_input":"2025-03-19T23:38:12.335676Z","iopub.status.idle":"2025-03-19T23:38:12.349166Z","shell.execute_reply.started":"2025-03-19T23:38:12.335657Z","shell.execute_reply":"2025-03-19T23:38:12.348345Z"}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"@triton.autotune(\n    configs=[\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=1, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_D2': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_D2': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4)\n],\n    key=['BSIZE', 'K', 'd2', 'L'],\n)\n@triton.jit\ndef first_pass_kernel(\n        hin_ptr, S1s_ptr, U2s_ptr, out1_ptr, out2_ptr,\n        BSIZE, K, d2, L,\n        stride_hin_bsize, stride_hin_d2,\n        stride_su_l, stride_su_d2, stride_su_k,\n        stride_out_l, stride_out_bsize, stride_out_k,\n        BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_D2: tl.constexpr,\n        GROUP_SIZE_BSIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=1)\n    batch_id = tl.program_id(axis=0)\n    \n    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_k\n    group_id = pid // num_pid_in_group\n    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n    pid_k = (pid % num_pid_in_group) // group_size_bsize\n\n    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n    offs_k = pid_k *  BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_d2 = tl.arange(0, BLOCK_SIZE_D2)\n\n    offs_bsize = tl.max_contiguous(tl.multiple_of(offs_bsize, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_K), BLOCK_SIZE_K)\n    offs_d2 = tl.max_contiguous(tl.multiple_of(offs_d2, BLOCK_SIZE_D2), BLOCK_SIZE_D2)\n    \n    hin_ptrs = hin_ptr + (offs_bsize[:, None] * stride_hin_bsize + offs_d2[None, :] * stride_hin_d2)\n\n    su_tmp = batch_id * stride_su_l + (offs_d2[:, None] * stride_su_d2 + offs_k[None, :] * stride_su_k)\n    S1s_ptrs = S1s_ptr + su_tmp\n    U2s_ptrs = U2s_ptr + su_tmp\n\n    accumulator1 = tl.full(shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), value=0.0, dtype=tl.float32)\n    accumulator2 = tl.full(shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), value=0.0, dtype=tl.float32)\n    \n    for d2_i in range(0, tl.cdiv(d2, BLOCK_SIZE_D2)):\n        hin_mask = (offs_bsize[:, None] < BSIZE) & (offs_d2[None, :] < d2 - d2_i * BLOCK_SIZE_D2)\n        hin = tl.load(hin_ptrs, mask=hin_mask, other=0.0)\n        \n        su_mask = (offs_d2[:, None] < d2 - d2_i * BLOCK_SIZE_D2) & (offs_k[None, :] < K)\n        S1s = tl.load(S1s_ptrs, mask=su_mask, other=0.0)\n        U2s = tl.load(U2s_ptrs, mask=su_mask, other=0.0)\n        \n        accumulator1 += tl.dot(hin, S1s)\n        accumulator2 += tl.dot(hin, U2s)\n        \n        hin_ptrs += BLOCK_SIZE_D2 * stride_hin_d2\n        S1s_ptrs += BLOCK_SIZE_D2 * stride_su_d2\n        U2s_ptrs += BLOCK_SIZE_D2 * stride_su_d2\n\n    accumulator1 = accumulator1.to(tl.float16)\n    accumulator2 = accumulator2.to(tl.float16)\n\n    out_tmp = batch_id * stride_out_l + stride_out_bsize * offs_bsize[:, None] + stride_out_k * offs_k[None, :]\n    out1_ptrs = out1_ptr + out_tmp\n    out2_ptrs = out2_ptr + out_tmp\n    \n    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_k[None, :] < K)\n    \n    tl.store(out1_ptrs, accumulator1, mask=out_mask)\n    tl.store(out2_ptrs, accumulator2, mask=out_mask)\n\ndef first_pass(hin, S1s, U2s):\n    device = 'cuda'\n    assert hin.shape[1] == S1s.shape[1], \"Incompatible dimensions\"\n    assert hin.shape[1] == U2s.shape[1], \"Incompatible dimensions\"\n    assert hin.is_contiguous(), \"Matrix A must be contiguous\"\n    assert S1s.is_contiguous(), \"Matrix A must be contiguous\"\n    assert U2s.is_contiguous(), \"Matrix A must be contiguous\"\n    assert S1s.stride() == U2s.stride(), \"Matrix A must be contiguous\"\n    \n    BSIZE, d2 = hin.shape\n    L, _, K = S1s.shape\n    \n    out1 = torch.empty((L, BSIZE, K), dtype=torch.float16, device=device)\n    out2 = torch.empty((L, BSIZE, K), dtype=torch.float16, device=device)\n\n    stride_hin_bsize, stride_hin_d2 = hin.stride()\n    stride_su_l, stride_su_d2, stride_su_k = S1s.stride()\n    stride_out_l, stride_out_bsize, stride_out_k = out1.stride()\n    \n    assert out1.stride() == out2.stride(), \"Matrix A must be contiguous\"\n    \n    grid = lambda META: (L, triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]) * triton.cdiv(K, META[\"BLOCK_SIZE_K\"]), )\n    \n    first_pass_kernel[grid](\n        hin, S1s, U2s, out1, out2,\n        BSIZE, K, d2, L,\n        stride_hin_bsize, stride_hin_d2,\n        stride_su_l, stride_su_d2, stride_su_k,\n        stride_out_l, stride_out_bsize, stride_out_k,\n    )\n    \n    return out1, out2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T23:38:12.350232Z","iopub.execute_input":"2025-03-19T23:38:12.350539Z","iopub.status.idle":"2025-03-19T23:38:12.381949Z","shell.execute_reply.started":"2025-03-19T23:38:12.350508Z","shell.execute_reply":"2025-03-19T23:38:12.381161Z"}},"outputs":[],"execution_count":127},{"cell_type":"code","source":"@triton.autotune(\n    configs = [\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 64, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_BSIZE': 8}, num_stages=1, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_D1': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_D1': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_D1': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4, num_warps=4)\n],\n    key=['BSIZE', 'd1', 'K', 'L'],\n)\n@triton.jit\ndef second_pass_kernel(\n        in1_ptr, in2_ptr, U1s_ptr, S2s_ptr, bias_ptr, out_ptr,\n        BSIZE, d1, K, L,\n        stride_in12_l, stride_in12_bsize, stride_in12_k,\n        stride_us_l, stride_us_k, stride_us_d1,\n        stride_bias_bsize, stride_bias_d1,\n        stride_out_bsize, stride_out_d1,\n        BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_D1: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n        GROUP_SIZE_BSIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    \n    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n    num_pid_d1 = tl.cdiv(d1, BLOCK_SIZE_D1)\n    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_d1\n    group_id = pid // num_pid_in_group\n    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n    GROUP_SIZE_BSIZE = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % GROUP_SIZE_BSIZE)\n    pid_d1 = (pid % num_pid_in_group) // GROUP_SIZE_BSIZE\n\n    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n    offs_d1 = pid_d1 *  BLOCK_SIZE_D1 + tl.arange(0, BLOCK_SIZE_D1)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    offs_bsize = tl.max_contiguous(tl.multiple_of(offs_bsize, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n    offs_d1 = tl.max_contiguous(tl.multiple_of(offs_d1, BLOCK_SIZE_D1), BLOCK_SIZE_D1)\n    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_K), BLOCK_SIZE_K)\n\n    in_tmp = offs_bsize[:, None] * stride_in12_bsize + offs_k[None, :] * stride_in12_k\n    us_tmp = offs_k[:, None] * stride_us_k + offs_d1[None, :] * stride_us_d1\n\n    accumulator = tl.full(shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_D1), value=0.0, dtype=tl.float32)\n    \n    for l in range(0, L):\n        l_tmp_stride = l * stride_in12_l\n        \n        in1_ptrs = l_tmp_stride + in1_ptr + in_tmp\n        in2_ptrs = l_tmp_stride + in2_ptr + in_tmp\n\n        U1s_ptrs = l_tmp_stride + U1s_ptr + us_tmp\n        S2s_ptrs = l_tmp_stride + S2s_ptr + us_tmp\n        \n        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n            in_mask = offs_k[None, :] < K - k * BLOCK_SIZE_K\n            in1 = tl.load(in1_ptrs, mask=in_mask, other=0.0)\n            in2 = tl.load(in2_ptrs, mask=in_mask, other=0.0)\n            \n            us_mask = offs_k[:, None] < K - k * BLOCK_SIZE_K\n            U1s = tl.load(U1s_ptrs, mask=us_mask, other=0.0)\n            S2s = tl.load(S2s_ptrs, mask=us_mask, other=0.0)\n            \n            accumulator += tl.dot(in1, U1s)\n            accumulator += tl.dot(in2, S2s)\n\n            in_inc = BLOCK_SIZE_K * stride_in12_k\n            in1_ptrs += in_inc\n            in2_ptrs += in_inc\n            \n            us_inc = BLOCK_SIZE_K * stride_us_k\n            U1s_ptrs += us_inc\n            S2s_ptrs += us_inc\n\n    accumulator = accumulator.to(tl.float16)\n    \n    bias_ptrs = bias_ptr + offs_d1[None, :] * stride_bias_d1\n    bias_mask = (offs_d1[None, :] < d1)\n    bias = tl.load(bias_ptrs, mask=bias_mask, other=0.0)\n\n    accumulator *= (1.0/ (2.0 * L))\n    accumulator += bias\n\n    out_ptrs = out_ptr + stride_out_bsize * offs_bsize[:, None] + stride_out_d1 * offs_d1[None, :]\n    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_d1[None, :] < d1)\n    \n    tl.store(out_ptrs, accumulator, mask=out_mask)\n\ndef second_pass(in1, in2, U1s, S2s, bias):\n    assert in1.shape[2] == U1s.shape[1], \"Incompatible dimensions\"\n    assert in2.shape[2] == S2s.shape[1], \"Incompatible dimensions\"\n    assert in1.is_contiguous(), \"Matrix A must be contiguous\"\n    assert in2.is_contiguous(), \"Matrix A must be contiguous\"\n    assert U1s.is_contiguous(), \"Matrix A must be contiguous\"\n    assert S2s.is_contiguous(), \"Matrix A must be contiguous\"\n    assert bias.is_contiguous(), \"Matrix A must be contiguous\"\n    assert U1s.stride() == S2s.stride(), \"Matrix A must be contiguous\"\n    assert in1.stride() == in2.stride(), \"Matrix A must be contiguous\"\n    \n    L, BSIZE, K = in1.shape\n    _, _, d1 = U1s.shape\n    \n    out = torch.empty((BSIZE, d1), dtype=torch.float16, device=device)\n\n    stride_in12_l, stride_in12_bsize, stride_in12_k = in1.stride()\n    stride_us_l, stride_us_k, stride_us_d1 = U1s.stride()\n    stride_bias_bsize, stride_bias_d1 = bias.stride()\n    stride_out_bsize, stride_out_d1 = out.stride()\n    \n    grid = lambda META: (triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]) * triton.cdiv(d1, META[\"BLOCK_SIZE_D1\"]), )\n    \n    second_pass_kernel[grid](\n        in1, in2, U1s, S2s, bias, out,\n        BSIZE, d1, K, L,\n        stride_in12_l, stride_in12_bsize, stride_in12_k,\n        stride_us_l, stride_us_k, stride_us_d1,\n        stride_bias_bsize, stride_bias_d1,\n        stride_out_bsize, stride_out_d1,\n    )\n    \n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T23:38:12.382880Z","iopub.execute_input":"2025-03-19T23:38:12.383255Z","iopub.status.idle":"2025-03-19T23:38:12.411601Z","shell.execute_reply.started":"2025-03-19T23:38:12.383218Z","shell.execute_reply":"2025-03-19T23:38:12.410620Z"}},"outputs":[],"execution_count":128},{"cell_type":"code","source":"class SketchedLinearFunction_triton(Function):\n    # Note that forward, setup_context, and backward are @staticmethods\n    @staticmethod\n    def forward(\n        input: torch.Tensor,\n        S1s: torch.Tensor,\n        S2s: torch.Tensor,\n        U1s: torch.Tensor,\n        U2s: torch.Tensor,\n        bias: torch.Tensor,\n    ):\n        L = S2s.shape[0]\n        in1, in2 = first_pass(input, S1s, U2s)\n\n        ###################################################################\n        # torch_output1 = (input.unsqueeze(0).expand(L, input.shape[0], input.shape[1])).bmm(S1s)\n        # torch_output2 = (input.unsqueeze(0).expand(L, input.shape[0], input.shape[1])).bmm(U2s)\n        \n        # rtol = 10e-1\n        # if torch.allclose(in1, torch_output1, atol=1e-2, rtol=rtol):\n        #     print(\"✅ Triton and Torch match 1\")\n        # else:\n        #     print(\"❌ Triton and Torch differ 1\")\n        \n        # if torch.allclose(in2, torch_output2, atol=1e-2, rtol=rtol):\n        #     print(\"✅ Triton and Torch match 2\")\n        # else:\n        #     print(\"❌ Triton and Torch differ 2\")\n\n        # max_diff = torch.max(torch.abs(in1 - torch_output1))\n        # print(f\"Max difference: {max_diff.item()}\\n\")\n\n        # max_diff = torch.max(torch.abs(in2 - torch_output2))\n        # print(f\"Max difference: {max_diff.item()}\\n\")\n\n        # print(\"1: \")\n        # print(f\"in1: {in1}\")\n        # print(f\"torch_output1: {torch_output1}\")\n        # print(\"2: \")\n        # print(f\"in1: {in2}\")\n        # print(f\"torch_output1: {torch_output2}\")\n        ###################################################################\n        triton_output = second_pass(in1, in2, U1s, S2s, bias.unsqueeze(0))\n\n        ###################################################################\n        # torch_output = (in1.bmm(U1s).mean(0) / 2) + (in2.bmm(S2s).mean(0) / 2) + bias\n\n        # rtol = 10e-1\n        # if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=rtol):\n        #     print(\"✅ Triton and Torch match 3\")\n        # else:\n        #     print(\"❌ Triton and Torch differ 3\")\n\n        # max_diff = torch.max(torch.abs(torch_output - triton_output))\n        # print(f\"Max difference: {max_diff.item()}\\n\")\n\n        # print(\"3: \")\n        # print(f\"in1: {triton_output}\")\n        # print(f\"torch_output1: {torch_output}\")\n        #################################################################\n        \n        return triton_output\n\n    @staticmethod\n    # inputs is a Tuple of all of the inputs passed to forward.\n    # output is the output of the forward().\n    def setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any):\n        pass\n        # input, S1s, S2s, U1s, U2s, bias = inputs\n        # ctx.save_for_backward(input, S1s, S2s, U1s, U2s, bias)\n\n    @staticmethod\n    def backward(ctx: Any, *grad_output: Any) -> Any:\n        # dl/dS2_i = U1_i g h_in^T / 2 * l\n        # dl/dS1_i = g h_in^T U2_i^T / 2 * l\n        # dl/dh_in = 1/(2*l) * (sum_{i=1}^{l} (S1_i^T U1_i g) + sum_{i=1}^{l} (U2_i^T S2_i g))\n        # dl/db = g\n        input, S1s, S2s, U1s, U2s, _ = ctx.saved_tensors\n        num_terms = S2s.shape[0]\n        g = grad_output[0] / (2 * num_terms)\n        g = g.unsqueeze(0).expand(num_terms, g.shape[0], g.shape[1])\n        input = (\n            input.unsqueeze(0)\n            .expand(num_terms, input.shape[0], input.shape[1])\n            .transpose(1, 2)\n        )\n        U1s = U1s.transpose(1, 2)\n        S1s = S1s.transpose(1, 2)\n        U2s = U2s.transpose(1, 2)\n        S2s = S2s.transpose(1, 2)\n        t1 = g.bmm(U1s)\n        grad = t1.bmm(S1s).sum(0) + g.bmm(S2s).bmm(U2s).sum(0)\n        grad_S2s = (U2s.bmm(input)).bmm(g)\n        grad_S1s = input.bmm(g.bmm(U1s))\n\n        g = g[0]\n        return (\n            grad,\n            grad_S1s,\n            grad_S2s,\n            None,\n            None,\n            # sum g on batch dimension input.shape[0]\n            g.reshape(input.shape[2], -1).sum(0),\n        )\n\n\nclass SKLinear_triton(nn.Module):\n    __constants__ = [\"in_features\", \"out_features\", \"num_terms\", \"low_rank\"]\n    in_features: int\n    out_features: int\n    num_terms: int\n    low_rank: int\n    S1s: torch.Tensor\n    S2s: torch.Tensor\n    U1s: torch.Tensor\n    U2s: torch.Tensor\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        num_terms: int,\n        low_rank: int,\n        W_init=None,\n        bias: bool = True,\n        dtype=None,\n        device=None,\n    ):\n        factory_kwargs = {\"dtype\": dtype, \"device\": device}\n        super(SKLinear_triton, self).__init__()\n\n        # if (\n        #     2 * num_terms * low_rank * (out_features + in_features)\n        #     > out_features * in_features\n        # ):\n        #     raise ValueError(\n        #         \"The number of parameters in the sketching layer is larger \"\n        #         + \"than the number of parameters in the fully connected layer.\"\n        #     )\n\n        self.num_terms = num_terms # l\n        self.low_rank = low_rank # k\n        self.out_features = out_features\n        self.in_features = in_features\n\n        # Register U1s and U2s as buffers since they are not learnable\n        self.register_buffer(\n            \"U1s\",\n            torch.stack(\n                [\n                    gen_U(low_rank, out_features, **factory_kwargs)\n                    for _ in range(num_terms)\n                ]\n            ),\n        )  # k(low rank)xd1(out) stacked along the zeros dimension (l) -> l x k x d1\n        self.register_buffer(\n            \"U2s\",\n            torch.stack(\n                [\n                    gen_U(in_features, low_rank, **factory_kwargs)\n                    for _ in range(num_terms)\n                ]\n            ),\n        )  # d2xk stacked along the zeros dimension (l) -> l x d2 x k\n\n        # W is used to only initialize S\n        if W_init is None:\n            W = torch.empty(in_features, out_features, **factory_kwargs) # d2 * d1\n            init.kaiming_uniform_(W, a=math.sqrt(5))\n        else:\n            W = W_init.T.detach().clone()\n\n        # S1s and S2s are precomputed but not updated in the backward pass\n        self.S1s = nn.Parameter(\n            torch.stack([torch.matmul(W, self.U1s[i].T) for i in range(num_terms)])\n        )  # d2xk stacked along the zeros dimension (l) -> l x d2 x k\n        self.S2s = nn.Parameter(\n            torch.stack([torch.matmul(self.U2s[i].T, W) for i in range(num_terms)])\n        )  # kxd1 stacked along the zeros dimension (l) -> l x k x d1\n\n        # Bias term initialized with a small standard deviation\n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_features, **factory_kwargs)) #1 * d1\n            fan_in, _ = init._calculate_fan_in_and_fan_out(W)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            init.uniform_(self.bias, -bound, bound)\n        else:\n            self.register_parameter(\"bias\", None)\n\n    def forward(self, h_in):\n        return SketchedLinearFunction_triton.apply(\n            h_in, self.S1s, self.S2s, self.U1s, self.U2s, self.bias\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T23:38:12.412888Z","iopub.execute_input":"2025-03-19T23:38:12.413240Z","iopub.status.idle":"2025-03-19T23:38:12.429101Z","shell.execute_reply.started":"2025-03-19T23:38:12.413207Z","shell.execute_reply":"2025-03-19T23:38:12.428345Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"# Set up parameters\nin_features = 1024\nout_features = 512\nnum_terms = 4\nlow_rank = 32\nbatch_size = 64\n#################\nnum_runs = 30\nwarmup = 4\n\ndevice = 'cuda'\n\nscale = 0.0001\n\ntorch.manual_seed(42)\ntriton_model = SKLinear_triton(in_features, out_features, num_terms, low_rank, dtype=torch.float16, device=device)\ntriton_model.bias = nn.Parameter(triton_model.bias * scale)\ntriton_model.U1s.copy_(triton_model.U1s * scale)\ntriton_model.S2s = nn.Parameter(triton_model.S2s * scale)\n\ntorch.manual_seed(42)\nnormal_model = SKLinear(in_features, out_features, num_terms, low_rank, dtype=torch.float16, device=device)\nnormal_model.bias = nn.Parameter(normal_model.bias * scale)\nnormal_model.U1s.copy_(normal_model.U1s * scale)\nnormal_model.S2s = nn.Parameter(normal_model.S2s * scale)\n\nassert torch.allclose(triton_model.bias,normal_model.bias)\nassert torch.allclose(triton_model.U1s,normal_model.U1s)\nassert torch.allclose(triton_model.S2s,normal_model.S2s)\nprint(triton_model.bias,normal_model.bias)\nprint(triton_model.U1s,normal_model.U1s)\nprint(triton_model.S2s,normal_model.S2s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T23:38:12.430193Z","iopub.execute_input":"2025-03-19T23:38:12.430640Z","iopub.status.idle":"2025-03-19T23:38:12.494208Z","shell.execute_reply.started":"2025-03-19T23:38:12.430586Z","shell.execute_reply":"2025-03-19T23:38:12.493363Z"}},"outputs":[{"name":"stdout","text":"Parameter containing:\ntensor([-1.0729e-06,  2.5630e-06,  2.6226e-06,  3.3975e-06, -7.1526e-07,\n         4.7684e-07,  3.2783e-06, -1.9073e-06,  3.5763e-07, -2.9802e-06,\n        -3.2783e-06,  2.3842e-07, -2.6822e-06, -4.2319e-06,  2.9206e-06,\n         8.3447e-07, -1.5497e-06, -1.1325e-06, -1.3709e-06, -3.2187e-06,\n        -2.5630e-06, -1.8477e-06,  1.8477e-06, -1.1325e-06, -2.7418e-06,\n        -4.4107e-06,  3.6955e-06,  2.2650e-06, -2.2650e-06, -1.4901e-06,\n         2.3842e-07,  2.6226e-06, -3.6955e-06, -1.6093e-06, -2.9802e-07,\n        -2.0266e-06, -1.7881e-06, -7.7486e-07,  2.2650e-06,  3.4571e-06,\n         2.2054e-06,  2.0266e-06, -8.3447e-07, -3.4571e-06, -2.2650e-06,\n        -2.7418e-06,  1.7881e-06, -1.9073e-06, -1.9073e-06, -3.1590e-06,\n        -3.7551e-06,  2.6226e-06, -1.1921e-06, -8.9407e-07, -1.3709e-06,\n         2.0266e-06,  1.9073e-06, -2.3246e-06, -3.7551e-06, -8.9407e-07,\n         3.5763e-07, -2.2650e-06, -1.9073e-06, -1.9073e-06,  4.0531e-06,\n        -2.9802e-06, -3.7551e-06, -1.7881e-06,  3.2783e-06, -2.0862e-06,\n        -4.3511e-06, -8.9407e-07, -7.1526e-07,  2.5630e-06,  7.1526e-07,\n        -3.1590e-06,  2.3246e-06,  3.3379e-06, -2.0862e-06, -3.8147e-06,\n         2.8610e-06, -3.0398e-06,  4.1723e-07,  2.8610e-06,  3.9339e-06,\n         4.1723e-07, -2.1458e-06,  3.5763e-06, -2.6822e-06, -2.8610e-06,\n        -4.7684e-07, -1.9670e-06, -3.5167e-06, -2.7418e-06, -1.7881e-07,\n        -1.7285e-06,  2.3842e-07, -2.3842e-07,  1.4901e-06,  3.3379e-06,\n        -3.8147e-06, -2.0862e-06, -0.0000e+00, -4.1723e-07, -2.5034e-06,\n         9.5367e-07,  3.2187e-06,  5.9605e-07,  4.4107e-06, -2.3842e-06,\n         3.8147e-06, -2.6822e-06, -2.9802e-07,  5.9605e-07,  4.1127e-06,\n         6.5565e-07,  3.7551e-06, -2.5630e-06, -2.1458e-06, -5.3644e-07,\n         3.2783e-06, -5.9605e-07, -3.6955e-06,  2.7418e-06,  3.9339e-06,\n         2.8014e-06,  4.2319e-06,  1.5497e-06,  2.2054e-06,  2.5630e-06,\n         2.3246e-06,  1.3113e-06,  2.1458e-06, -2.9802e-06, -2.4438e-06,\n         8.9407e-07,  2.5630e-06, -1.2517e-06,  4.7684e-07,  4.1723e-06,\n         1.3113e-06, -3.9339e-06,  1.8477e-06, -3.9935e-06, -1.0133e-06,\n         2.7418e-06,  7.1526e-07, -1.0729e-06, -2.1458e-06,  9.5367e-07,\n         1.3113e-06,  1.6093e-06,  2.1458e-06, -3.3975e-06,  3.4571e-06,\n         1.4305e-06,  1.0729e-06, -6.5565e-07, -4.2915e-06, -4.1127e-06,\n        -4.1723e-06,  0.0000e+00,  3.6955e-06, -1.4901e-06, -1.5497e-06,\n         2.3842e-06,  1.2517e-06,  3.0398e-06, -1.9073e-06,  2.9802e-07,\n         5.9605e-07, -1.3113e-06, -2.1458e-06, -3.7551e-06, -2.0862e-06,\n         1.0133e-06,  3.2187e-06,  4.1127e-06,  2.9206e-06, -2.2650e-06,\n        -3.9339e-06, -2.9206e-06,  2.6822e-06, -2.6226e-06, -3.8743e-06,\n         4.3511e-06, -1.3709e-06,  0.0000e+00, -3.9935e-06, -8.3447e-07,\n         1.2517e-06,  6.5565e-07,  1.9073e-06,  2.1458e-06,  2.0862e-06,\n         1.1325e-06,  7.1526e-07, -1.0729e-06, -3.9339e-06,  3.0398e-06,\n         2.3842e-07, -2.4438e-06, -2.4438e-06, -3.6359e-06, -3.8743e-06,\n        -3.7551e-06, -5.3644e-07, -3.4571e-06, -7.1526e-07,  1.9670e-06,\n        -2.9802e-06,  3.0994e-06,  4.2915e-06, -3.8147e-06, -2.1458e-06,\n        -3.2187e-06,  1.7881e-06, -2.3246e-06, -2.5630e-06,  3.7551e-06,\n        -2.2054e-06,  1.5497e-06, -1.3113e-06, -2.5034e-06,  2.0862e-06,\n         1.9073e-06, -1.0133e-06, -3.2783e-06, -3.4571e-06,  1.7285e-06,\n         1.7285e-06, -2.6822e-06,  5.3644e-07,  3.2783e-06,  2.6822e-06,\n        -3.0994e-06,  1.3709e-06, -3.9935e-06,  3.5763e-06, -3.9339e-06,\n         4.2915e-06, -8.9407e-07, -0.0000e+00, -4.2319e-06, -1.0133e-06,\n        -2.6226e-06, -4.1723e-07, -3.2783e-06, -1.1921e-07,  2.9802e-07,\n        -3.3975e-06, -4.1723e-06, -3.0398e-06,  2.8014e-06,  3.6955e-06,\n         4.1723e-06, -1.8477e-06, -2.3246e-06, -3.6955e-06,  4.1127e-06,\n         2.9802e-07, -5.3644e-07,  0.0000e+00,  5.9605e-08,  3.1590e-06,\n         3.0398e-06, -3.2783e-06,  3.5167e-06, -3.3379e-06, -1.5497e-06,\n        -3.3975e-06, -0.0000e+00,  7.1526e-07,  5.3644e-07,  2.7418e-06,\n        -3.7551e-06, -6.5565e-07, -4.4107e-06,  2.9802e-06,  8.9407e-07,\n         1.1921e-06,  3.3379e-06, -3.5763e-07, -1.5497e-06, -5.9605e-08,\n         9.5367e-07, -3.5763e-06,  4.1127e-06, -3.4571e-06, -2.7418e-06,\n        -3.9935e-06, -0.0000e+00,  1.7881e-07,  2.6822e-06, -1.6093e-06,\n        -2.9802e-07,  9.5367e-07,  2.7418e-06,  1.1921e-07,  4.0531e-06,\n        -2.9802e-06, -2.7418e-06, -2.1458e-06,  8.9407e-07, -2.9206e-06,\n         1.0729e-06, -3.9935e-06,  1.7881e-06, -1.9670e-06,  4.2915e-06,\n        -1.1921e-06,  3.6955e-06,  2.9802e-07, -2.9802e-06, -3.3379e-06,\n        -3.2187e-06, -1.4305e-06,  2.2054e-06, -2.3842e-07, -5.3644e-07,\n         1.0133e-06, -2.8610e-06, -3.5167e-06,  2.2650e-06, -2.0862e-06,\n        -5.9605e-08, -5.9605e-08,  4.4107e-06, -2.9802e-07,  1.1921e-07,\n        -1.9073e-06,  2.8610e-06, -2.3842e-06,  2.5034e-06,  4.3511e-06,\n         2.0862e-06, -2.8610e-06, -2.8014e-06, -2.0266e-06, -3.5763e-06,\n        -5.9605e-07,  2.0862e-06,  1.1325e-06,  2.5630e-06, -4.2915e-06,\n        -1.1921e-07, -3.6955e-06,  1.0729e-06, -1.8477e-06,  2.9802e-07,\n        -3.5763e-06, -2.6226e-06,  3.6359e-06,  2.4438e-06,  1.9670e-06,\n         1.0133e-06,  1.7881e-07,  2.9206e-06,  4.1723e-06,  6.5565e-07,\n        -3.3379e-06, -2.5630e-06, -2.3842e-06,  3.0398e-06, -6.5565e-07,\n         7.7486e-07, -1.8477e-06,  2.7418e-06, -3.1590e-06, -3.8743e-06,\n         5.3644e-07,  2.6822e-06, -2.6226e-06, -3.4571e-06,  1.7881e-06,\n        -3.6955e-06,  4.2915e-06,  3.1590e-06, -3.1590e-06,  3.5167e-06,\n        -5.3644e-07, -4.3511e-06, -2.9802e-06, -4.1723e-07, -2.9802e-06,\n         9.5367e-07,  2.6226e-06, -2.8610e-06, -3.8147e-06, -2.5034e-06,\n         2.0862e-06,  2.0862e-06,  5.9605e-07, -1.2517e-06,  3.8743e-06,\n        -1.3113e-06,  2.9802e-07, -2.5630e-06, -4.4107e-06, -6.5565e-07,\n         3.1590e-06,  1.4901e-06, -1.8477e-06, -3.9935e-06, -3.4571e-06,\n        -2.4438e-06,  1.7285e-06, -4.1723e-06,  2.3246e-06,  3.8147e-06,\n        -0.0000e+00,  4.2319e-06,  3.4571e-06,  3.8743e-06, -3.6955e-06,\n         2.1458e-06,  8.9407e-07,  2.8610e-06, -3.5763e-06,  3.5763e-06,\n         1.3709e-06, -8.3447e-07,  3.8147e-06,  3.1590e-06, -1.7285e-06,\n        -1.7881e-07, -2.2054e-06,  1.6689e-06,  2.0862e-06,  3.2187e-06,\n         6.5565e-07,  7.7486e-07,  3.6359e-06,  1.2517e-06, -2.8014e-06,\n        -2.7418e-06,  3.0994e-06, -4.1723e-06, -2.9802e-06,  1.4305e-06,\n         3.8147e-06, -1.3113e-06, -2.9206e-06,  5.9605e-08,  2.5034e-06,\n         2.3842e-07,  6.5565e-07, -2.6822e-06, -2.3246e-06, -3.6359e-06,\n        -0.0000e+00,  0.0000e+00, -5.9605e-08, -1.5497e-06, -3.6955e-06,\n         3.2187e-06,  4.1723e-07,  1.7881e-07, -2.9206e-06, -1.9670e-06,\n        -3.0398e-06, -1.4305e-06, -1.9073e-06, -2.2054e-06,  3.3975e-06,\n         4.1127e-06, -2.9802e-07,  3.3379e-06, -8.9407e-07, -3.6359e-06,\n         1.5497e-06, -3.9935e-06, -1.6093e-06, -2.9802e-07, -2.6226e-06,\n        -1.7881e-07,  3.0398e-06, -1.2517e-06, -3.8743e-06, -2.3842e-07,\n        -8.9407e-07, -2.8014e-06, -2.3842e-07,  2.3246e-06, -3.5763e-07,\n         4.2915e-06,  3.9339e-06,  4.2915e-06, -3.4571e-06,  2.3246e-06,\n         3.9339e-06, -2.2054e-06,  2.5034e-06, -4.4107e-06, -3.0994e-06,\n        -1.3709e-06,  1.6093e-06, -1.1921e-06, -1.1325e-06,  3.8147e-06,\n         1.1921e-07, -4.3511e-06,  3.5167e-06, -4.7684e-07, -2.2054e-06,\n         5.9605e-07, -1.9073e-06, -1.3709e-06, -1.6689e-06,  1.7881e-06,\n        -4.0531e-06, -3.6359e-06], device='cuda:0', dtype=torch.float16,\n       requires_grad=True) Parameter containing:\ntensor([-1.0729e-06,  2.5630e-06,  2.6226e-06,  3.3975e-06, -7.1526e-07,\n         4.7684e-07,  3.2783e-06, -1.9073e-06,  3.5763e-07, -2.9802e-06,\n        -3.2783e-06,  2.3842e-07, -2.6822e-06, -4.2319e-06,  2.9206e-06,\n         8.3447e-07, -1.5497e-06, -1.1325e-06, -1.3709e-06, -3.2187e-06,\n        -2.5630e-06, -1.8477e-06,  1.8477e-06, -1.1325e-06, -2.7418e-06,\n        -4.4107e-06,  3.6955e-06,  2.2650e-06, -2.2650e-06, -1.4901e-06,\n         2.3842e-07,  2.6226e-06, -3.6955e-06, -1.6093e-06, -2.9802e-07,\n        -2.0266e-06, -1.7881e-06, -7.7486e-07,  2.2650e-06,  3.4571e-06,\n         2.2054e-06,  2.0266e-06, -8.3447e-07, -3.4571e-06, -2.2650e-06,\n        -2.7418e-06,  1.7881e-06, -1.9073e-06, -1.9073e-06, -3.1590e-06,\n        -3.7551e-06,  2.6226e-06, -1.1921e-06, -8.9407e-07, -1.3709e-06,\n         2.0266e-06,  1.9073e-06, -2.3246e-06, -3.7551e-06, -8.9407e-07,\n         3.5763e-07, -2.2650e-06, -1.9073e-06, -1.9073e-06,  4.0531e-06,\n        -2.9802e-06, -3.7551e-06, -1.7881e-06,  3.2783e-06, -2.0862e-06,\n        -4.3511e-06, -8.9407e-07, -7.1526e-07,  2.5630e-06,  7.1526e-07,\n        -3.1590e-06,  2.3246e-06,  3.3379e-06, -2.0862e-06, -3.8147e-06,\n         2.8610e-06, -3.0398e-06,  4.1723e-07,  2.8610e-06,  3.9339e-06,\n         4.1723e-07, -2.1458e-06,  3.5763e-06, -2.6822e-06, -2.8610e-06,\n        -4.7684e-07, -1.9670e-06, -3.5167e-06, -2.7418e-06, -1.7881e-07,\n        -1.7285e-06,  2.3842e-07, -2.3842e-07,  1.4901e-06,  3.3379e-06,\n        -3.8147e-06, -2.0862e-06, -0.0000e+00, -4.1723e-07, -2.5034e-06,\n         9.5367e-07,  3.2187e-06,  5.9605e-07,  4.4107e-06, -2.3842e-06,\n         3.8147e-06, -2.6822e-06, -2.9802e-07,  5.9605e-07,  4.1127e-06,\n         6.5565e-07,  3.7551e-06, -2.5630e-06, -2.1458e-06, -5.3644e-07,\n         3.2783e-06, -5.9605e-07, -3.6955e-06,  2.7418e-06,  3.9339e-06,\n         2.8014e-06,  4.2319e-06,  1.5497e-06,  2.2054e-06,  2.5630e-06,\n         2.3246e-06,  1.3113e-06,  2.1458e-06, -2.9802e-06, -2.4438e-06,\n         8.9407e-07,  2.5630e-06, -1.2517e-06,  4.7684e-07,  4.1723e-06,\n         1.3113e-06, -3.9339e-06,  1.8477e-06, -3.9935e-06, -1.0133e-06,\n         2.7418e-06,  7.1526e-07, -1.0729e-06, -2.1458e-06,  9.5367e-07,\n         1.3113e-06,  1.6093e-06,  2.1458e-06, -3.3975e-06,  3.4571e-06,\n         1.4305e-06,  1.0729e-06, -6.5565e-07, -4.2915e-06, -4.1127e-06,\n        -4.1723e-06,  0.0000e+00,  3.6955e-06, -1.4901e-06, -1.5497e-06,\n         2.3842e-06,  1.2517e-06,  3.0398e-06, -1.9073e-06,  2.9802e-07,\n         5.9605e-07, -1.3113e-06, -2.1458e-06, -3.7551e-06, -2.0862e-06,\n         1.0133e-06,  3.2187e-06,  4.1127e-06,  2.9206e-06, -2.2650e-06,\n        -3.9339e-06, -2.9206e-06,  2.6822e-06, -2.6226e-06, -3.8743e-06,\n         4.3511e-06, -1.3709e-06,  0.0000e+00, -3.9935e-06, -8.3447e-07,\n         1.2517e-06,  6.5565e-07,  1.9073e-06,  2.1458e-06,  2.0862e-06,\n         1.1325e-06,  7.1526e-07, -1.0729e-06, -3.9339e-06,  3.0398e-06,\n         2.3842e-07, -2.4438e-06, -2.4438e-06, -3.6359e-06, -3.8743e-06,\n        -3.7551e-06, -5.3644e-07, -3.4571e-06, -7.1526e-07,  1.9670e-06,\n        -2.9802e-06,  3.0994e-06,  4.2915e-06, -3.8147e-06, -2.1458e-06,\n        -3.2187e-06,  1.7881e-06, -2.3246e-06, -2.5630e-06,  3.7551e-06,\n        -2.2054e-06,  1.5497e-06, -1.3113e-06, -2.5034e-06,  2.0862e-06,\n         1.9073e-06, -1.0133e-06, -3.2783e-06, -3.4571e-06,  1.7285e-06,\n         1.7285e-06, -2.6822e-06,  5.3644e-07,  3.2783e-06,  2.6822e-06,\n        -3.0994e-06,  1.3709e-06, -3.9935e-06,  3.5763e-06, -3.9339e-06,\n         4.2915e-06, -8.9407e-07, -0.0000e+00, -4.2319e-06, -1.0133e-06,\n        -2.6226e-06, -4.1723e-07, -3.2783e-06, -1.1921e-07,  2.9802e-07,\n        -3.3975e-06, -4.1723e-06, -3.0398e-06,  2.8014e-06,  3.6955e-06,\n         4.1723e-06, -1.8477e-06, -2.3246e-06, -3.6955e-06,  4.1127e-06,\n         2.9802e-07, -5.3644e-07,  0.0000e+00,  5.9605e-08,  3.1590e-06,\n         3.0398e-06, -3.2783e-06,  3.5167e-06, -3.3379e-06, -1.5497e-06,\n        -3.3975e-06, -0.0000e+00,  7.1526e-07,  5.3644e-07,  2.7418e-06,\n        -3.7551e-06, -6.5565e-07, -4.4107e-06,  2.9802e-06,  8.9407e-07,\n         1.1921e-06,  3.3379e-06, -3.5763e-07, -1.5497e-06, -5.9605e-08,\n         9.5367e-07, -3.5763e-06,  4.1127e-06, -3.4571e-06, -2.7418e-06,\n        -3.9935e-06, -0.0000e+00,  1.7881e-07,  2.6822e-06, -1.6093e-06,\n        -2.9802e-07,  9.5367e-07,  2.7418e-06,  1.1921e-07,  4.0531e-06,\n        -2.9802e-06, -2.7418e-06, -2.1458e-06,  8.9407e-07, -2.9206e-06,\n         1.0729e-06, -3.9935e-06,  1.7881e-06, -1.9670e-06,  4.2915e-06,\n        -1.1921e-06,  3.6955e-06,  2.9802e-07, -2.9802e-06, -3.3379e-06,\n        -3.2187e-06, -1.4305e-06,  2.2054e-06, -2.3842e-07, -5.3644e-07,\n         1.0133e-06, -2.8610e-06, -3.5167e-06,  2.2650e-06, -2.0862e-06,\n        -5.9605e-08, -5.9605e-08,  4.4107e-06, -2.9802e-07,  1.1921e-07,\n        -1.9073e-06,  2.8610e-06, -2.3842e-06,  2.5034e-06,  4.3511e-06,\n         2.0862e-06, -2.8610e-06, -2.8014e-06, -2.0266e-06, -3.5763e-06,\n        -5.9605e-07,  2.0862e-06,  1.1325e-06,  2.5630e-06, -4.2915e-06,\n        -1.1921e-07, -3.6955e-06,  1.0729e-06, -1.8477e-06,  2.9802e-07,\n        -3.5763e-06, -2.6226e-06,  3.6359e-06,  2.4438e-06,  1.9670e-06,\n         1.0133e-06,  1.7881e-07,  2.9206e-06,  4.1723e-06,  6.5565e-07,\n        -3.3379e-06, -2.5630e-06, -2.3842e-06,  3.0398e-06, -6.5565e-07,\n         7.7486e-07, -1.8477e-06,  2.7418e-06, -3.1590e-06, -3.8743e-06,\n         5.3644e-07,  2.6822e-06, -2.6226e-06, -3.4571e-06,  1.7881e-06,\n        -3.6955e-06,  4.2915e-06,  3.1590e-06, -3.1590e-06,  3.5167e-06,\n        -5.3644e-07, -4.3511e-06, -2.9802e-06, -4.1723e-07, -2.9802e-06,\n         9.5367e-07,  2.6226e-06, -2.8610e-06, -3.8147e-06, -2.5034e-06,\n         2.0862e-06,  2.0862e-06,  5.9605e-07, -1.2517e-06,  3.8743e-06,\n        -1.3113e-06,  2.9802e-07, -2.5630e-06, -4.4107e-06, -6.5565e-07,\n         3.1590e-06,  1.4901e-06, -1.8477e-06, -3.9935e-06, -3.4571e-06,\n        -2.4438e-06,  1.7285e-06, -4.1723e-06,  2.3246e-06,  3.8147e-06,\n        -0.0000e+00,  4.2319e-06,  3.4571e-06,  3.8743e-06, -3.6955e-06,\n         2.1458e-06,  8.9407e-07,  2.8610e-06, -3.5763e-06,  3.5763e-06,\n         1.3709e-06, -8.3447e-07,  3.8147e-06,  3.1590e-06, -1.7285e-06,\n        -1.7881e-07, -2.2054e-06,  1.6689e-06,  2.0862e-06,  3.2187e-06,\n         6.5565e-07,  7.7486e-07,  3.6359e-06,  1.2517e-06, -2.8014e-06,\n        -2.7418e-06,  3.0994e-06, -4.1723e-06, -2.9802e-06,  1.4305e-06,\n         3.8147e-06, -1.3113e-06, -2.9206e-06,  5.9605e-08,  2.5034e-06,\n         2.3842e-07,  6.5565e-07, -2.6822e-06, -2.3246e-06, -3.6359e-06,\n        -0.0000e+00,  0.0000e+00, -5.9605e-08, -1.5497e-06, -3.6955e-06,\n         3.2187e-06,  4.1723e-07,  1.7881e-07, -2.9206e-06, -1.9670e-06,\n        -3.0398e-06, -1.4305e-06, -1.9073e-06, -2.2054e-06,  3.3975e-06,\n         4.1127e-06, -2.9802e-07,  3.3379e-06, -8.9407e-07, -3.6359e-06,\n         1.5497e-06, -3.9935e-06, -1.6093e-06, -2.9802e-07, -2.6226e-06,\n        -1.7881e-07,  3.0398e-06, -1.2517e-06, -3.8743e-06, -2.3842e-07,\n        -8.9407e-07, -2.8014e-06, -2.3842e-07,  2.3246e-06, -3.5763e-07,\n         4.2915e-06,  3.9339e-06,  4.2915e-06, -3.4571e-06,  2.3246e-06,\n         3.9339e-06, -2.2054e-06,  2.5034e-06, -4.4107e-06, -3.0994e-06,\n        -1.3709e-06,  1.6093e-06, -1.1921e-06, -1.1325e-06,  3.8147e-06,\n         1.1921e-07, -4.3511e-06,  3.5167e-06, -4.7684e-07, -2.2054e-06,\n         5.9605e-07, -1.9073e-06, -1.3709e-06, -1.6689e-06,  1.7881e-06,\n        -4.0531e-06, -3.6359e-06], device='cuda:0', dtype=torch.float16,\n       requires_grad=True)\ntensor([[[ 1.7703e-05,  1.7703e-05, -1.7703e-05,  ..., -1.7703e-05,\n          -1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05, -1.7703e-05,  1.7703e-05,  ..., -1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05, -1.7703e-05, -1.7703e-05,  ..., -1.7703e-05,\n          -1.7703e-05, -1.7703e-05],\n         ...,\n         [-1.7703e-05, -1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05, -1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n          -1.7703e-05, -1.7703e-05],\n         [-1.7703e-05, -1.7703e-05, -1.7703e-05,  ..., -1.7703e-05,\n          -1.7703e-05,  1.7703e-05]],\n\n        [[ 1.7703e-05,  1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05,  1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [-1.7703e-05, -1.7703e-05,  1.7703e-05,  ..., -1.7703e-05,\n          -1.7703e-05, -1.7703e-05],\n         ...,\n         [ 1.7703e-05, -1.7703e-05, -1.7703e-05,  ..., -1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05,  1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n          -1.7703e-05, -1.7703e-05],\n         [ 1.7703e-05, -1.7703e-05, -1.7703e-05,  ..., -1.7703e-05,\n           1.7703e-05, -1.7703e-05]],\n\n        [[ 1.7703e-05,  1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05,  1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05, -1.7703e-05,  1.7703e-05,  ..., -1.7703e-05,\n           1.7703e-05, -1.7703e-05],\n         ...,\n         [ 1.7703e-05, -1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n          -1.7703e-05, -1.7703e-05],\n         [-1.7703e-05,  1.7703e-05,  1.7703e-05,  ..., -1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05, -1.7703e-05,  1.7703e-05,  ..., -1.7703e-05,\n          -1.7703e-05, -1.7703e-05]],\n\n        [[ 1.7703e-05,  1.7703e-05,  1.7703e-05,  ..., -1.7703e-05,\n          -1.7703e-05, -1.7703e-05],\n         [-1.7703e-05,  1.7703e-05, -1.7703e-05,  ..., -1.7703e-05,\n          -1.7703e-05, -1.7703e-05],\n         [ 1.7703e-05, -1.7703e-05,  1.7703e-05,  ..., -1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         ...,\n         [ 1.7703e-05, -1.7703e-05,  1.7703e-05,  ...,  1.7703e-05,\n          -1.7703e-05,  1.7703e-05],\n         [-1.7703e-05,  1.7703e-05,  1.7703e-05,  ...,  1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [-1.7703e-05,  1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n          -1.7703e-05,  1.7703e-05]]], device='cuda:0', dtype=torch.float16) tensor([[[ 1.7703e-05,  1.7703e-05, -1.7703e-05,  ..., -1.7703e-05,\n          -1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05, -1.7703e-05,  1.7703e-05,  ..., -1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05, -1.7703e-05, -1.7703e-05,  ..., -1.7703e-05,\n          -1.7703e-05, -1.7703e-05],\n         ...,\n         [-1.7703e-05, -1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05, -1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n          -1.7703e-05, -1.7703e-05],\n         [-1.7703e-05, -1.7703e-05, -1.7703e-05,  ..., -1.7703e-05,\n          -1.7703e-05,  1.7703e-05]],\n\n        [[ 1.7703e-05,  1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05,  1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [-1.7703e-05, -1.7703e-05,  1.7703e-05,  ..., -1.7703e-05,\n          -1.7703e-05, -1.7703e-05],\n         ...,\n         [ 1.7703e-05, -1.7703e-05, -1.7703e-05,  ..., -1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05,  1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n          -1.7703e-05, -1.7703e-05],\n         [ 1.7703e-05, -1.7703e-05, -1.7703e-05,  ..., -1.7703e-05,\n           1.7703e-05, -1.7703e-05]],\n\n        [[ 1.7703e-05,  1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05,  1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05, -1.7703e-05,  1.7703e-05,  ..., -1.7703e-05,\n           1.7703e-05, -1.7703e-05],\n         ...,\n         [ 1.7703e-05, -1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n          -1.7703e-05, -1.7703e-05],\n         [-1.7703e-05,  1.7703e-05,  1.7703e-05,  ..., -1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [ 1.7703e-05, -1.7703e-05,  1.7703e-05,  ..., -1.7703e-05,\n          -1.7703e-05, -1.7703e-05]],\n\n        [[ 1.7703e-05,  1.7703e-05,  1.7703e-05,  ..., -1.7703e-05,\n          -1.7703e-05, -1.7703e-05],\n         [-1.7703e-05,  1.7703e-05, -1.7703e-05,  ..., -1.7703e-05,\n          -1.7703e-05, -1.7703e-05],\n         [ 1.7703e-05, -1.7703e-05,  1.7703e-05,  ..., -1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         ...,\n         [ 1.7703e-05, -1.7703e-05,  1.7703e-05,  ...,  1.7703e-05,\n          -1.7703e-05,  1.7703e-05],\n         [-1.7703e-05,  1.7703e-05,  1.7703e-05,  ...,  1.7703e-05,\n           1.7703e-05,  1.7703e-05],\n         [-1.7703e-05,  1.7703e-05, -1.7703e-05,  ...,  1.7703e-05,\n          -1.7703e-05,  1.7703e-05]]], device='cuda:0', dtype=torch.float16)\nParameter containing:\ntensor([[[-1.3113e-06, -2.7418e-06,  3.9339e-06,  ..., -1.9670e-06,\n          -3.7551e-06,  3.8743e-06],\n         [ 4.1723e-07,  2.8610e-06,  2.2650e-06,  ...,  2.3246e-06,\n          -1.3709e-06,  0.0000e+00],\n         [ 1.7881e-07,  8.9407e-07, -4.2319e-06,  ..., -3.8743e-06,\n           7.7486e-07,  2.0862e-06],\n         ...,\n         [ 2.6226e-06, -1.6093e-06, -6.5565e-07,  ...,  1.9670e-06,\n           3.3379e-06, -1.9670e-06],\n         [-5.3644e-07, -1.4901e-06,  7.1526e-07,  ...,  4.2915e-06,\n          -8.3447e-07,  9.5367e-07],\n         [ 1.9670e-06, -8.3447e-07, -4.3511e-06,  ...,  5.9605e-07,\n           1.3113e-06, -7.7486e-07]],\n\n        [[ 2.0266e-06, -4.4703e-06,  8.3447e-07,  ..., -3.5763e-07,\n          -4.7088e-06,  1.1921e-06],\n         [ 6.5565e-07, -3.5763e-07,  9.5367e-07,  ...,  3.5763e-07,\n           2.9802e-07,  8.3447e-07],\n         [-1.0133e-06,  3.3379e-06, -2.4438e-06,  ..., -2.8610e-06,\n          -1.7285e-06,  2.2650e-06],\n         ...,\n         [ 2.0266e-06, -4.1723e-07, -3.8743e-06,  ..., -1.7285e-06,\n           5.9605e-07, -1.5497e-06],\n         [-4.5300e-06,  7.1526e-07,  3.5763e-07,  ...,  5.9605e-07,\n           5.3644e-07,  7.1526e-07],\n         [ 2.9802e-06, -2.9206e-06,  8.9407e-07,  ..., -3.6359e-06,\n           3.5763e-07,  1.0729e-06]],\n\n        [[ 8.3447e-07,  2.2054e-06, -2.2054e-06,  ..., -6.6161e-06,\n          -2.9206e-06, -8.3447e-07],\n         [-1.1921e-06,  1.8477e-06,  4.6492e-06,  ..., -1.2517e-06,\n           1.1921e-06,  1.4305e-06],\n         [-9.5367e-07,  1.0133e-06, -3.3975e-06,  ..., -1.9073e-06,\n          -4.5896e-06,  1.1325e-06],\n         ...,\n         [-3.6955e-06,  1.7285e-06, -5.9605e-08,  ...,  4.1127e-06,\n           2.2650e-06, -4.1723e-07],\n         [ 6.0797e-06,  1.1325e-06, -3.4571e-06,  ..., -1.2517e-06,\n           5.3644e-06,  2.5034e-06],\n         [-3.0994e-06,  3.9935e-06,  1.1325e-06,  ...,  7.7486e-07,\n           7.1526e-07,  7.1526e-07]],\n\n        [[ 8.9407e-07, -2.2054e-06, -8.9407e-07,  ...,  3.9935e-06,\n          -8.3447e-07,  3.2187e-06],\n         [-1.1921e-06, -7.1526e-07, -1.3113e-06,  ..., -5.9009e-06,\n           2.2054e-06, -2.5630e-06],\n         [ 5.0664e-06, -1.7881e-06, -4.8876e-06,  ..., -1.3709e-06,\n          -3.6955e-06,  8.9407e-07],\n         ...,\n         [-9.5367e-07, -1.9073e-06, -1.3709e-06,  ..., -5.6028e-06,\n           1.9073e-06,  1.1921e-07],\n         [ 2.2054e-06,  5.9605e-07,  5.3644e-07,  ..., -2.9802e-06,\n          -1.8477e-06, -9.5367e-07],\n         [ 1.1921e-07, -1.3709e-06,  3.5763e-07,  ...,  4.0531e-06,\n           2.9206e-06, -7.7486e-07]]], device='cuda:0', dtype=torch.float16,\n       requires_grad=True) Parameter containing:\ntensor([[[-1.3113e-06, -2.7418e-06,  3.9339e-06,  ..., -1.9670e-06,\n          -3.7551e-06,  3.8743e-06],\n         [ 4.1723e-07,  2.8610e-06,  2.2650e-06,  ...,  2.3246e-06,\n          -1.3709e-06,  0.0000e+00],\n         [ 1.7881e-07,  8.9407e-07, -4.2319e-06,  ..., -3.8743e-06,\n           7.7486e-07,  2.0862e-06],\n         ...,\n         [ 2.6226e-06, -1.6093e-06, -6.5565e-07,  ...,  1.9670e-06,\n           3.3379e-06, -1.9670e-06],\n         [-5.3644e-07, -1.4901e-06,  7.1526e-07,  ...,  4.2915e-06,\n          -8.3447e-07,  9.5367e-07],\n         [ 1.9670e-06, -8.3447e-07, -4.3511e-06,  ...,  5.9605e-07,\n           1.3113e-06, -7.7486e-07]],\n\n        [[ 2.0266e-06, -4.4703e-06,  8.3447e-07,  ..., -3.5763e-07,\n          -4.7088e-06,  1.1921e-06],\n         [ 6.5565e-07, -3.5763e-07,  9.5367e-07,  ...,  3.5763e-07,\n           2.9802e-07,  8.3447e-07],\n         [-1.0133e-06,  3.3379e-06, -2.4438e-06,  ..., -2.8610e-06,\n          -1.7285e-06,  2.2650e-06],\n         ...,\n         [ 2.0266e-06, -4.1723e-07, -3.8743e-06,  ..., -1.7285e-06,\n           5.9605e-07, -1.5497e-06],\n         [-4.5300e-06,  7.1526e-07,  3.5763e-07,  ...,  5.9605e-07,\n           5.3644e-07,  7.1526e-07],\n         [ 2.9802e-06, -2.9206e-06,  8.9407e-07,  ..., -3.6359e-06,\n           3.5763e-07,  1.0729e-06]],\n\n        [[ 8.3447e-07,  2.2054e-06, -2.2054e-06,  ..., -6.6161e-06,\n          -2.9206e-06, -8.3447e-07],\n         [-1.1921e-06,  1.8477e-06,  4.6492e-06,  ..., -1.2517e-06,\n           1.1921e-06,  1.4305e-06],\n         [-9.5367e-07,  1.0133e-06, -3.3975e-06,  ..., -1.9073e-06,\n          -4.5896e-06,  1.1325e-06],\n         ...,\n         [-3.6955e-06,  1.7285e-06, -5.9605e-08,  ...,  4.1127e-06,\n           2.2650e-06, -4.1723e-07],\n         [ 6.0797e-06,  1.1325e-06, -3.4571e-06,  ..., -1.2517e-06,\n           5.3644e-06,  2.5034e-06],\n         [-3.0994e-06,  3.9935e-06,  1.1325e-06,  ...,  7.7486e-07,\n           7.1526e-07,  7.1526e-07]],\n\n        [[ 8.9407e-07, -2.2054e-06, -8.9407e-07,  ...,  3.9935e-06,\n          -8.3447e-07,  3.2187e-06],\n         [-1.1921e-06, -7.1526e-07, -1.3113e-06,  ..., -5.9009e-06,\n           2.2054e-06, -2.5630e-06],\n         [ 5.0664e-06, -1.7881e-06, -4.8876e-06,  ..., -1.3709e-06,\n          -3.6955e-06,  8.9407e-07],\n         ...,\n         [-9.5367e-07, -1.9073e-06, -1.3709e-06,  ..., -5.6028e-06,\n           1.9073e-06,  1.1921e-07],\n         [ 2.2054e-06,  5.9605e-07,  5.3644e-07,  ..., -2.9802e-06,\n          -1.8477e-06, -9.5367e-07],\n         [ 1.1921e-07, -1.3709e-06,  3.5763e-07,  ...,  4.0531e-06,\n           2.9206e-06, -7.7486e-07]]], device='cuda:0', dtype=torch.float16,\n       requires_grad=True)\n","output_type":"stream"}],"execution_count":130},{"cell_type":"code","source":"######## normal test\nx = torch.randn(batch_size, in_features, dtype=torch.float16, device=device)\nx2 = x.clone().detach()\n\nout_triton, out_normal = None, None\n\n# Ensure outputs match\nwith torch.no_grad():\n    # print(\"triton model stuff\")\n    # print(f\"x: {x}, \\n S1s {triton_model.S1s},\\n S2s: {triton_model.S2s}\\n, U1s: {triton_model.U1s},\\n U2s:{triton_model.U2s},\\n bias: {triton_model.bias}\\n\")\n    out_triton = triton_model(x)\n    # print(f\"x: {x}, \\n S1s {triton_model.S1s},\\n S2s: {triton_model.S2s}\\n, U1s: {triton_model.U1s},\\n U2s:{triton_model.U2s},\\n bias: {triton_model.bias}\\n\")\n    # print(\"normal model stuff\")\n    # print(f\"x: {x2}, \\n S1s {normal_model.S1s},\\n S2s: {normal_model.S2s}\\n, U1s: {normal_model.U1s},\\n U2s:{normal_model.U2s},\\n bias: {normal_model.bias}\\n\")\n    out_normal = normal_model(x2)\n    # print(f\"x: {x2}, \\n S1s {normal_model.S1s},\\n S2s: {normal_model.S2s}\\n, U1s: {normal_model.U1s},\\n U2s:{normal_model.U2s},\\n bias: {normal_model.bias}\\n\")\n\n# Check if outputs are close\noutputs_match = torch.allclose(out_triton, out_normal, rtol=2e-1, atol=1e-3)\nprint(f\"Outputs match: {outputs_match} 4\")\n\nif not outputs_match:\n    max_diff = torch.max(torch.abs(out_triton - out_normal))\n    print(f\"Max difference: {max_diff.item()}\")\n    print(f\"out_triton: {out_triton}\")\n    print(f\"out_normal: {out_normal}\")\n\nprint(\"first\")\nnum_terms = triton_model.S2s.shape[0]\nx = x.unsqueeze(0).expand(num_terms, x.shape[0], x.shape[1])\nx2 = x2.unsqueeze(0).expand(num_terms, x2.shape[0], x2.shape[1])\nprint(((x.bmm(triton_model.S1s)).bmm(triton_model.U1s)).mean(0) / 2 + ((x.bmm(triton_model.U2s)).bmm(triton_model.S2s)).mean(0) / 2 + triton_model.bias)\nprint(\"second\")\nprint(((x2.bmm(normal_model.S1s)).bmm(normal_model.U1s)).mean(0) / 2 + ((x2.bmm(normal_model.U2s)).bmm(normal_model.S2s)).mean(0) / 2 + normal_model.bias)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T23:38:12.495096Z","iopub.execute_input":"2025-03-19T23:38:12.495366Z","iopub.status.idle":"2025-03-19T23:38:14.545111Z","shell.execute_reply.started":"2025-03-19T23:38:12.495344Z","shell.execute_reply":"2025-03-19T23:38:14.544147Z"}},"outputs":[{"name":"stdout","text":"Outputs match: True 4\nfirst\ntensor([[-2.1875e-04, -1.5283e-04,  2.3031e-04,  ..., -1.4603e-04,\n          1.6749e-05, -4.4763e-05],\n        [ 4.5776e-05, -9.4175e-06,  5.1856e-06,  ...,  5.6565e-05,\n          2.3901e-05,  4.4584e-05],\n        [ 2.9981e-05,  4.7207e-05, -3.8683e-05,  ..., -7.0214e-05,\n          1.6880e-04, -9.9599e-05],\n        ...,\n        [ 3.4869e-05,  2.2113e-05,  2.9147e-05,  ..., -1.0085e-04,\n          5.0306e-05,  6.8545e-06],\n        [-3.5048e-05,  4.2439e-05, -4.1008e-05,  ...,  1.0407e-04,\n         -2.6166e-05,  4.6253e-05],\n        [ 1.4222e-04,  8.0764e-05,  1.0878e-04,  ...,  2.6405e-05,\n         -5.4955e-05,  7.3433e-05]], device='cuda:0', dtype=torch.float16,\n       grad_fn=<AddBackward0>)\nsecond\ntensor([[-2.1875e-04, -1.5283e-04,  2.3031e-04,  ..., -1.4603e-04,\n          1.6749e-05, -4.4763e-05],\n        [ 4.5776e-05, -9.4175e-06,  5.1856e-06,  ...,  5.6565e-05,\n          2.3901e-05,  4.4584e-05],\n        [ 2.9981e-05,  4.7207e-05, -3.8683e-05,  ..., -7.0214e-05,\n          1.6880e-04, -9.9599e-05],\n        ...,\n        [ 3.4869e-05,  2.2113e-05,  2.9147e-05,  ..., -1.0085e-04,\n          5.0306e-05,  6.8545e-06],\n        [-3.5048e-05,  4.2439e-05, -4.1008e-05,  ...,  1.0407e-04,\n         -2.6166e-05,  4.6253e-05],\n        [ 1.4222e-04,  8.0764e-05,  1.0878e-04,  ...,  2.6405e-05,\n         -5.4955e-05,  7.3433e-05]], device='cuda:0', dtype=torch.float16,\n       grad_fn=<AddBackward0>)\n","output_type":"stream"}],"execution_count":131},{"cell_type":"code","source":"import torch\nimport torch._inductor.config as config\nconfig.max_autotune_gemm = False\nimport time\nimport matplotlib.pyplot as plt\n# import numpy as np\nfrom torch.utils.benchmark import Timer\n\ndef benchmark_models(triton_model_compiled, normal_model_compiled, \n                     in_features, out_features, num_terms, low_rank, \n                     batch_size, num_runs, warmup, device='cuda'):\n    \"\"\"Benchmark forward pass for both compiled models with proper warmup.\"\"\"\n    # Set models to eval mode\n    triton_model_compiled.eval()\n    normal_model_compiled.eval()\n    \n    # Create input tensor with appropriate size\n    x = torch.randn(batch_size, in_features, dtype=torch.float16, device=device)\n    \n    # Ensure outputs match\n    with torch.no_grad():\n        out_triton = triton_model_compiled(x)\n        out_normal = normal_model_compiled(x)\n    \n    # Check if outputs are close\n    # outputs_match = torch.allclose(out_triton, out_normal, rtol=10e-1, atol=1e-3)\n    # print(f\"Outputs match: {outputs_match}\")\n    \n    # if not outputs_match:\n    # max_diff = torch.max(torch.abs(out_triton - out_normal))\n    # print(f\"Max difference: {max_diff.item()}\")\n    # print(f\"out_triton: {out_triton}\")\n    # print(f\"out_normal: {out_normal}\")\n    \n    # Benchmark triton model\n    triton_times = []\n    with torch.no_grad():\n        # Warmup\n        for _ in range(warmup):\n            _ = triton_model_compiled(x)\n        torch.cuda.synchronize()\n        \n        # Actual timing\n        for _ in range(num_runs):\n            start = time.perf_counter()\n            _ = triton_model_compiled(x)\n            torch.cuda.synchronize()\n            end = time.perf_counter()\n            triton_times.append((end - start) * 1000)  # Convert to ms\n    \n    # Benchmark normal model\n    normal_times = []\n    with torch.no_grad():\n        # Warmup\n        for _ in range(warmup):\n            _ = normal_model_compiled(x)\n        torch.cuda.synchronize()\n        \n        # Actual timing\n        for _ in range(num_runs):\n            start = time.perf_counter()\n            _ = normal_model_compiled(x)\n            torch.cuda.synchronize()\n            end = time.perf_counter()\n            normal_times.append((end - start) * 1000)  # Convert to ms\n    \n    triton_mean = np.mean(triton_times)\n    normal_mean = np.mean(normal_times)\n    \n    print(f\"Triton model: {triton_mean:.3f} ms\")\n    print(f\"Normal model: {normal_mean:.3f} ms\")\n    print(f\"Speedup: {normal_mean/triton_mean:.2f}x\")\n    \n    return {\n        'triton_mean': triton_mean,\n        'normal_mean': normal_mean,\n        'triton_times': triton_times,\n        'normal_times': normal_times,\n        'speedup': normal_mean/triton_mean\n    }\n\n# Function to vary input features and benchmark\ndef benchmark_varying_in_features(triton_model, normal_model, \n                                  out_features, num_terms, low_rank, \n                                  batch_size, num_runs, warmup, device='cuda'):\n    input_sizes = [512, 1024, 2048, 4096, 8192]\n    results = []\n    \n    for in_feat in input_sizes:\n        print(f\"\\nBenchmarking with in_features={in_feat}\")\n        # Create new models with updated input size\n        triton_model_new = SKLinear_triton(in_feat, out_features, num_terms, low_rank, \n                                           dtype=torch.float16, device=device)\n        normal_model_new = SKLinear(in_feat, out_features, num_terms, low_rank, \n                                    dtype=torch.float16, device=device)\n        \n        # Compile models\n        triton_model_compiled = torch.compile(\n            triton_model_new,\n            backend=\"inductor\",\n            fullgraph=True,\n            dynamic=False\n        )\n        \n        normal_model_compiled = torch.compile(\n            normal_model_new,\n            backend=\"inductor\",\n            fullgraph=True,\n            dynamic=False\n        )\n        \n        # Run benchmark\n        result = benchmark_models(triton_model_compiled, normal_model_compiled,\n                                 in_feat, out_features, num_terms, low_rank,\n                                 batch_size, num_runs, warmup, device)\n        \n        results.append({\n            'in_features': in_feat,\n            'triton_mean': result['triton_mean'],\n            'normal_mean': result['normal_mean'],\n            'speedup': result['speedup']\n        })\n    \n    return results\n\n# Function to vary output features and benchmark\ndef benchmark_varying_out_features(triton_model, normal_model, \n                                  in_features, num_terms, low_rank, \n                                  batch_size, num_runs, warmup, device='cuda'):\n    output_sizes = [128, 256, 512, 1024, 2048]\n    results = []\n    \n    for out_feat in output_sizes:\n        print(f\"\\nBenchmarking with out_features={out_feat}\")\n        # Create new models with updated output size\n        triton_model_new = SKLinear_triton(in_features, out_feat, num_terms, low_rank, \n                                           dtype=torch.float16, device=device)\n        normal_model_new = SKLinear(in_features, out_feat, num_terms, low_rank, \n                                    dtype=torch.float16, device=device)\n        \n        # Compile models\n        triton_model_compiled = torch.compile(\n            triton_model_new,\n            backend=\"inductor\",\n            fullgraph=True,\n            dynamic=False\n        )\n        \n        normal_model_compiled = torch.compile(\n            normal_model_new,\n            backend=\"inductor\",\n            fullgraph=True,\n            dynamic=False\n        )\n        \n        # Run benchmark\n        result = benchmark_models(triton_model_compiled, normal_model_compiled,\n                                 in_features, out_feat, num_terms, low_rank,\n                                 batch_size, num_runs, warmup, device)\n        \n        results.append({\n            'out_features': out_feat,\n            'triton_mean': result['triton_mean'],\n            'normal_mean': result['normal_mean'],\n            'speedup': result['speedup']\n        })\n    \n    return results\n\n# Function to plot the results\ndef plot_benchmark_results(in_feature_results, out_feature_results):\n    # Plot for varying input features\n    plt.figure(figsize=(12, 10))\n    \n    # Plot 1: Execution times\n    plt.subplot(2, 2, 1)\n    in_sizes = [r['in_features'] for r in in_feature_results]\n    triton_times = [r['triton_mean'] for r in in_feature_results]\n    normal_times = [r['normal_mean'] for r in in_feature_results]\n    \n    plt.plot(in_sizes, triton_times, 'o-', label='Triton Model')\n    plt.plot(in_sizes, normal_times, 's-', label='Normal Model')\n    plt.xlabel('Input Features')\n    plt.ylabel('Time (ms)')\n    plt.title('Forward Pass Time vs Input Size')\n    plt.grid(True)\n    plt.legend()\n    \n    # Plot 2: Speedup for input features\n    plt.subplot(2, 2, 2)\n    speedups = [r['speedup'] for r in in_feature_results]\n    plt.plot(in_sizes, speedups, 'D-')\n    plt.axhline(y=1.0, color='r', linestyle='--')\n    plt.xlabel('Input Features')\n    plt.ylabel('Speedup (Normal/Triton)')\n    plt.title('Speedup vs Input Size')\n    plt.grid(True)\n    \n    # Plot 3: Execution times for output features\n    plt.subplot(2, 2, 3)\n    out_sizes = [r['out_features'] for r in out_feature_results]\n    triton_times = [r['triton_mean'] for r in out_feature_results]\n    normal_times = [r['normal_mean'] for r in out_feature_results]\n    \n    plt.plot(out_sizes, triton_times, 'o-', label='Triton Model')\n    plt.plot(out_sizes, normal_times, 's-', label='Normal Model')\n    plt.xlabel('Output Features')\n    plt.ylabel('Time (ms)')\n    plt.title('Forward Pass Time vs Output Size')\n    plt.grid(True)\n    plt.legend()\n    \n    # Plot 4: Speedup for output features\n    plt.subplot(2, 2, 4)\n    speedups = [r['speedup'] for r in out_feature_results]\n    plt.plot(out_sizes, speedups, 'D-')\n    plt.axhline(y=1.0, color='r', linestyle='--')\n    plt.xlabel('Output Features')\n    plt.ylabel('Speedup (Normal/Triton)')\n    plt.title('Speedup vs Output Size')\n    plt.grid(True)\n    \n    plt.tight_layout()\n    return plt\n\n# Run the benchmarks\nprint(\"Benchmarking with varying input features...\")\nin_feature_results = benchmark_varying_in_features(triton_model, normal_model, \n                                                  out_features, num_terms, low_rank, \n                                                  batch_size, num_runs, warmup, device)\n\nprint(\"\\nBenchmarking with varying output features...\")\nout_feature_results = benchmark_varying_out_features(triton_model, normal_model, \n                                                    in_features, num_terms, low_rank, \n                                                    batch_size, num_runs, warmup, device)\n\n# Plot the results\nplt = plot_benchmark_results(in_feature_results, out_feature_results)\nplt.savefig('sklinear_benchmark_results.png')\nplt.show()\n\n# Print summary\nprint(\"\\n=== SUMMARY ===\")\nprint(\"Input Feature Scaling:\")\nfor result in in_feature_results:\n    print(f\"In Features: {result['in_features']}, Speedup: {result['speedup']:.2f}x\")\n\nprint(\"\\nOutput Feature Scaling:\")\nfor result in out_feature_results:\n    print(f\"Out Features: {result['out_features']}, Speedup: {result['speedup']:.2f}x\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T23:38:14.546268Z","iopub.execute_input":"2025-03-19T23:38:14.546628Z","iopub.status.idle":"2025-03-19T23:38:17.945698Z","shell.execute_reply.started":"2025-03-19T23:38:14.546591Z","shell.execute_reply":"2025-03-19T23:38:17.944495Z"}},"outputs":[{"name":"stdout","text":"Benchmarking with varying input features...\n\nBenchmarking with in_features=512\nTriton model: 0.135 ms\nNormal model: 0.197 ms\nSpeedup: 1.45x\n\nBenchmarking with in_features=1024\nTriton model: 0.137 ms\nNormal model: 0.195 ms\nSpeedup: 1.42x\n\nBenchmarking with in_features=2048\nTriton model: 0.141 ms\nNormal model: 0.209 ms\nSpeedup: 1.48x\n\nBenchmarking with in_features=4096\nTriton model: 0.195 ms\nNormal model: 0.296 ms\nSpeedup: 1.52x\n\nBenchmarking with in_features=8192\nTriton model: 0.709 ms\nNormal model: 1.129 ms\nSpeedup: 1.59x\n\nBenchmarking with varying output features...\n\nBenchmarking with out_features=128\nTriton model: 0.171 ms\nNormal model: 0.269 ms\nSpeedup: 1.57x\n\nBenchmarking with out_features=256\nTriton model: 0.176 ms\nNormal model: 0.278 ms\nSpeedup: 1.58x\n\nBenchmarking with out_features=512\nTriton model: 0.178 ms\nNormal model: 0.281 ms\nSpeedup: 1.58x\n\nBenchmarking with out_features=1024\n","output_type":"stream"},{"name":"stderr","text":"W0319 23:38:17.907000 31 torch/_dynamo/convert_frame.py:906] [3/8] torch._dynamo hit config.cache_size_limit (8)\nW0319 23:38:17.907000 31 torch/_dynamo/convert_frame.py:906] [3/8]    function: 'forward' (<ipython-input-129-c27fab37f971>:191)\nW0319 23:38:17.907000 31 torch/_dynamo/convert_frame.py:906] [3/8]    last reason: 3/0: tensor 'L['h_in']' size mismatch at index 1. expected 512, actual 1024\nW0319 23:38:17.907000 31 torch/_dynamo/convert_frame.py:906] [3/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\nW0319 23:38:17.907000 31 torch/_dynamo/convert_frame.py:906] [3/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n","output_type":"stream"},{"name":"stdout","text":"Triton model: 0.176 ms\nNormal model: 0.287 ms\nSpeedup: 1.63x\n\nBenchmarking with out_features=2048\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRecompileLimitExceeded\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-132-e966e0ca83e0>\u001b[0m in \u001b[0;36m<cell line: 233>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBenchmarking with varying output features...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m out_feature_results = benchmark_varying_out_features(triton_model, normal_model, \n\u001b[0m\u001b[1;32m    234\u001b[0m                                                     \u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_rank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                                                     batch_size, num_runs, warmup, device)\n","\u001b[0;32m<ipython-input-132-e966e0ca83e0>\u001b[0m in \u001b[0;36mbenchmark_varying_out_features\u001b[0;34m(triton_model, normal_model, in_features, num_terms, low_rank, batch_size, num_runs, warmup, device)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# Run benchmark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         result = benchmark_models(triton_model_compiled, normal_model_compiled,\n\u001b[0m\u001b[1;32m    158\u001b[0m                                  \u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_rank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                                  batch_size, num_runs, warmup, device)\n","\u001b[0;32m<ipython-input-132-e966e0ca83e0>\u001b[0m in \u001b[0;36mbenchmark_models\u001b[0;34m(triton_model_compiled, normal_model_compiled, in_features, out_features, num_terms, low_rank, batch_size, num_runs, warmup, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Ensure outputs match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mout_triton\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriton_model_compiled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mout_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormal_model_compiled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                 \u001b[0;31m# Restore the dynamic layer stack depth if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_lock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_current_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0;31m# skip=1: skip this frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m             return self._torchdynamo_orig_callable(\n\u001b[0m\u001b[1;32m   1381\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCompileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             return _compile(\n\u001b[0m\u001b[1;32m    548\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0;34m\"pytorch/compiler:skip_code_recursive_on_cache_limit_hit\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             ):\n\u001b[0;32m--> 925\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRecompileLimitExceeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{limit_type} reached\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                 \u001b[0;31m# do not recursively skip frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRecompileLimitExceeded\u001b[0m: cache_size_limit reached"],"ename":"RecompileLimitExceeded","evalue":"cache_size_limit reached","output_type":"error"}],"execution_count":132}]}