{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"github_repos_wildcard\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:13:14.090841Z","iopub.execute_input":"2025-04-26T12:13:14.091235Z","iopub.status.idle":"2025-04-26T12:13:14.248293Z","shell.execute_reply.started":"2025-04-26T12:13:14.091200Z","shell.execute_reply":"2025-04-26T12:13:14.247617Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"repo_url = f\"https://{token}@github.com/gaserSami/panther.git\"\nbranch = \"graphing\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:13:14.249781Z","iopub.execute_input":"2025-04-26T12:13:14.250062Z","iopub.status.idle":"2025-04-26T12:13:14.254121Z","shell.execute_reply.started":"2025-04-26T12:13:14.250037Z","shell.execute_reply":"2025-04-26T12:13:14.253304Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!git clone -b {branch} {repo_url}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:13:14.254968Z","iopub.execute_input":"2025-04-26T12:13:14.255259Z","iopub.status.idle":"2025-04-26T12:13:17.919719Z","shell.execute_reply.started":"2025-04-26T12:13:14.255240Z","shell.execute_reply":"2025-04-26T12:13:17.918976Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'panther'...\nremote: Enumerating objects: 625, done.\u001b[K\nremote: Counting objects: 100% (123/123), done.\u001b[K\nremote: Compressing objects: 100% (54/54), done.\u001b[K\nremote: Total 625 (delta 105), reused 75 (delta 69), pack-reused 502 (from 1)\u001b[K\nReceiving objects: 100% (625/625), 27.49 MiB | 19.03 MiB/s, done.\nResolving deltas: 100% (351/351), done.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!mv panther Panther","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:13:17.921727Z","iopub.execute_input":"2025-04-26T12:13:17.921960Z","iopub.status.idle":"2025-04-26T12:13:18.054031Z","shell.execute_reply.started":"2025-04-26T12:13:17.921939Z","shell.execute_reply":"2025-04-26T12:13:18.053057Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# !pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:13:18.055185Z","iopub.execute_input":"2025-04-26T12:13:18.055409Z","iopub.status.idle":"2025-04-26T12:13:18.059522Z","shell.execute_reply.started":"2025-04-26T12:13:18.055389Z","shell.execute_reply":"2025-04-26T12:13:18.058669Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# import torch\n# print(torch.__version__)\n# import triton\n# print(triton.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:13:18.060377Z","iopub.execute_input":"2025-04-26T12:13:18.060740Z","iopub.status.idle":"2025-04-26T12:13:18.077603Z","shell.execute_reply.started":"2025-04-26T12:13:18.060715Z","shell.execute_reply":"2025-04-26T12:13:18.077015Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# !export LC_ALL=\"en_US.UTF-8\"\n# !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n# !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n# !ldconfig /usr/lib64-nvidia","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:13:18.078322Z","iopub.execute_input":"2025-04-26T12:13:18.078535Z","iopub.status.idle":"2025-04-26T12:13:18.096409Z","shell.execute_reply.started":"2025-04-26T12:13:18.078520Z","shell.execute_reply":"2025-04-26T12:13:18.095835Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"%%writefile /kaggle/working/Panther/pawX/setup.py\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name=\"pawX\",\n    ext_modules=[\n        CUDAExtension(\n            name=\"pawX\",\n            sources=[\n                \"skops.cpp\",\n                \"bindings.cpp\",\n                \"linear.cpp\",\n                \"linear_cuda.cu\",\n                \"cqrrpt.cpp\",\n                \"rsvd.cpp\",\n                \"attention.cpp\",\n            ],\n            # Use system includes and libraries\n            include_dirs=[\"/usr/include/x86_64-linux-gnu\"],\n            library_dirs=[],\n            libraries=[\"openblas\"],\n            extra_compile_args={\"cxx\": [\"-O2\", \"-fopenmp\"], \"nvcc\": [\"-O2\"]},\n            extra_link_args=[\"-llapacke\", \"-lopenblas\"]\n        )\n    ],\n    cmdclass={\"build_ext\": BuildExtension},\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:13:18.097234Z","iopub.execute_input":"2025-04-26T12:13:18.097452Z","iopub.status.idle":"2025-04-26T12:13:18.116325Z","shell.execute_reply.started":"2025-04-26T12:13:18.097433Z","shell.execute_reply":"2025-04-26T12:13:18.115579Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/Panther/pawX/setup.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# %%writefile /kaggle/working/Panther/panther/nn/linear.py\n# import math\n# from typing import Any, Tuple\n\n# import torch\n# import torch.nn as nn\n# from torch.autograd import Function\n# from torch.nn import init\n\n# from panther.sketch import scaled_sign_sketch as gen_U\n# from pawX import sketched_linear_backward, sketched_linear_forward\n\n\n# class SketchedLinearFunction(Function):\n#     # Note that forward, setup_context, and backward are @staticmethods\n#     @staticmethod\n#     def forward(\n#         ctx,\n#         input: torch.Tensor,\n#         S1s: torch.Tensor,\n#         S2s: torch.Tensor,\n#         U1s: torch.Tensor,\n#         U2s: torch.Tensor,\n#         bias: torch.Tensor,\n#         mode: int,\n#     ):\n#         return sketched_linear_forward(input, S1s, S2s, U1s, U2s, bias, mode)\n\n#     @staticmethod\n#     # inputs is a Tuple of all of the inputs passed to forward.\n#     # output is the output of the forward().\n#     def setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any):\n#         input, S1s, S2s, U1s, U2s, bias, mode = inputs\n#         ctx.save_for_backward(input, S1s, S2s, U1s, U2s, bias, mode)\n\n#     @staticmethod\n#     def backward(ctx: Any, *grad_output: Any) -> Any:\n#         # dl/dS2_i = U1_i g h_in^T / 2 * l\n#         # dl/dS1_i = g h_in^T U2_i^T / 2 * l\n#         # dl/dh_in = 1/(2*l) * (sum_{i=1}^{l} (S1_i^T U1_i g) + sum_{i=1}^{l} (U2_i^T S2_i g))\n#         # dl/db = g\n#         input, S1s, S2s, U1s, U2s, _ = ctx.saved_tensors\n#         grads = sketched_linear_backward(\n#             grad_output=grad_output[0],\n#             input=input,\n#             S1s=S1s,\n#             S2s=S2s,\n#             U1s=U1s,\n#             U2s=U2s,\n#         )\n#         return (\n#             grads[0],  # h_in\n#             grads[1],  # S1s\n#             grads[2],  # S2s\n#             None,  # U1s\n#             None,  # U2s\n#             grads[3],  # bias\n#             None,\n#         )\n\n\n# class SKLinear(nn.Module):\n#     __constants__ = [\"in_features\", \"out_features\", \"num_terms\", \"low_rank\"]\n#     in_features: int\n#     out_features: int\n#     num_terms: int\n#     low_rank: int\n#     S1s: torch.Tensor\n#     S2s: torch.Tensor\n#     U1s: torch.Tensor\n#     U2s: torch.Tensor\n\n#     def __init__(\n#         self,\n#         in_features: int,\n#         out_features: int,\n#         num_terms: int,\n#         low_rank: int,\n#         mode: int,\n#         W_init=None,\n#         bias: bool = True,\n#         dtype=None,\n#         device=None,\n#     ):\n#         factory_kwargs = {\"device\": device, \"dtype\": dtype}\n#         super(SKLinear, self).__init__()\n\n#         # if (\n#         #     2 * num_terms * low_rank * (out_features + in_features)\n#         #     > out_features * in_features\n#         # ):\n#         #     raise ValueError(\n#         #         \"The number of parameters in the sketching layer is larger \"\n#         #         + \"than the number of parameters in the fully connected layer.\"\n#         #     )\n#         self.mode = mode\n#         self.num_terms = num_terms\n#         self.low_rank = low_rank\n#         self.out_features = out_features\n#         self.in_features = in_features\n\n#         # Register U1s and U2s as buffers since they are not learnable\n#         self.register_buffer(\n#             \"U1s\",\n#             torch.stack(\n#                 [\n#                     gen_U(low_rank, out_features, **factory_kwargs)\n#                     for _ in range(num_terms)\n#                 ]\n#             ),\n#         )  # kxd1\n#         self.register_buffer(\n#             \"U2s\",\n#             torch.stack(\n#                 [\n#                     gen_U(low_rank, in_features, **factory_kwargs).T\n#                     for _ in range(num_terms)\n#                 ]\n#             ),\n#         )  # d2xk\n\n#         # W is used to only initialize S\n#         if W_init is None:\n#             W = torch.empty(in_features, out_features, **factory_kwargs)\n#             init.kaiming_uniform_(W, a=math.sqrt(5))\n#         else:\n#             W = W_init.T.detach().clone()\n\n#         # S1s and S2s are precomputed but not updated in the backward pass\n#         self.S1s = nn.Parameter(\n#             torch.stack([torch.matmul(W, self.U1s[i].T) for i in range(num_terms)])\n#         )  # d2xk\n#         self.S2s = nn.Parameter(\n#             torch.stack([torch.matmul(self.U2s[i].T, W) for i in range(num_terms)])\n#         )  # kxd1\n\n#         # Bias term initialized with a small standard deviation\n#         if bias:\n#             self.bias = nn.Parameter(torch.empty(out_features, **factory_kwargs))\n#             fan_in, _ = init._calculate_fan_in_and_fan_out(W)\n#             bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n#             init.uniform_(self.bias, -bound, bound)\n#         else:\n#             self.register_parameter(\"bias\", None)\n\n#     def forward(self, h_in):\n#         return SketchedLinearFunction.apply(\n#             h_in, self.S1s, self.S2s, self.U1s, self.U2s, self.bias, 0\n#         )\n\n\n# if __name__ == \"__main__\":\n#     linear = SKLinear(\n#         in_features=10,\n#         out_features=10,\n#         num_terms=1,\n#         low_rank=1,\n#         mode=0,\n#         dtype=torch.float32,\n#         device=torch.device(\"cuda:0\"),\n#     )\n#     input = torch.randn(1, 10, dtype=torch.float32, device=torch.device(\"cuda:0\"))\n#     output = linear(input)\n#     print(output)\n#     print(output.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:13:18.117217Z","iopub.execute_input":"2025-04-26T12:13:18.117563Z","iopub.status.idle":"2025-04-26T12:13:18.134110Z","shell.execute_reply.started":"2025-04-26T12:13:18.117547Z","shell.execute_reply":"2025-04-26T12:13:18.133530Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!sudo apt-get install liblapacke-dev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:13:18.136524Z","iopub.execute_input":"2025-04-26T12:13:18.136755Z","iopub.status.idle":"2025-04-26T12:13:31.636559Z","shell.execute_reply.started":"2025-04-26T12:13:18.136740Z","shell.execute_reply":"2025-04-26T12:13:31.635800Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  liblapacke libtmglib-dev libtmglib3\nSuggested packages:\n  liblapack-doc\nThe following NEW packages will be installed:\n  liblapacke liblapacke-dev libtmglib-dev libtmglib3\n0 upgraded, 4 newly installed, 0 to remove and 122 not upgraded.\nNeed to get 1,071 kB of archives.\nAfter this operation, 12.3 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtmglib3 amd64 3.10.0-2ubuntu1 [144 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblapacke amd64 3.10.0-2ubuntu1 [435 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtmglib-dev amd64 3.10.0-2ubuntu1 [134 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblapacke-dev amd64 3.10.0-2ubuntu1 [358 kB]\nFetched 1,071 kB in 1s (969 kB/s)       \ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\ndebconf: falling back to frontend: Readline\nSelecting previously unselected package libtmglib3:amd64.\n(Reading database ... 128691 files and directories currently installed.)\nPreparing to unpack .../libtmglib3_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking libtmglib3:amd64 (3.10.0-2ubuntu1) ...\nSelecting previously unselected package liblapacke:amd64.\nPreparing to unpack .../liblapacke_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking liblapacke:amd64 (3.10.0-2ubuntu1) ...\nSelecting previously unselected package libtmglib-dev:amd64.\nPreparing to unpack .../libtmglib-dev_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking libtmglib-dev:amd64 (3.10.0-2ubuntu1) ...\nSelecting previously unselected package liblapacke-dev:amd64.\nPreparing to unpack .../liblapacke-dev_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking liblapacke-dev:amd64 (3.10.0-2ubuntu1) ...\nSetting up libtmglib3:amd64 (3.10.0-2ubuntu1) ...\nSetting up liblapacke:amd64 (3.10.0-2ubuntu1) ...\nSetting up libtmglib-dev:amd64 (3.10.0-2ubuntu1) ...\nSetting up liblapacke-dev:amd64 (3.10.0-2ubuntu1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!cd /kaggle/working/Panther/pawX; python setup.py install\n!cd /kaggle/working/Panther/pawX; pip install --no-build-isolation -e .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:13:31.637653Z","iopub.execute_input":"2025-04-26T12:13:31.637888Z","iopub.status.idle":"2025-04-26T12:16:35.584659Z","shell.execute_reply.started":"2025-04-26T12:13:31.637867Z","shell.execute_reply":"2025-04-26T12:16:35.583834Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` and ``easy_install``.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://github.com/pypa/setuptools/issues/917 for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:416: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:426: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nEmitting ninja build file /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/build.ninja...\nCompiling objects...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/7] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/linear.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[2/7] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/cqrrpt.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[3/7] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/attention.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[4/7] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/rsvd.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n/kaggle/working/Panther/pawX/rsvd.cpp: In function ‘std::tuple<at::Tensor, at::Tensor, at::Tensor> randomized_svd(const at::Tensor&, int64_t, double)’:\n/kaggle/working/Panther/pawX/rsvd.cpp:19:13: warning: unused variable ‘m’ [-Wunused-variable]\n   19 |     int64_t m = A.size(0);\n      |             ^\n[5/7] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/bindings.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\nIn file included from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/Exceptions.h:12,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/python.h:11,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/extension.h:9,\n                 from /kaggle/working/Panther/pawX/attention.h:3,\n                 from /kaggle/working/Panther/pawX/bindings.cpp:1:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘class pybind11::class_<DistributionFamily>’:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2264:7:   required from ‘class pybind11::enum_<DistributionFamily>’\n/kaggle/working/Panther/pawX/bindings.cpp:26:60:   required from here\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1588:7: warning: ‘pybind11::class_<DistributionFamily>’ declared with greater visibility than its base ‘pybind11::detail::generic_type’ [-Wattributes]\n 1588 | class class_ : public detail::generic_type {\n      |       ^~~~~~\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘pybind11::class_< <template-parameter-1-1>, <template-parameter-1-2> >::class_(pybind11::handle, const char*, const Extra& ...) [with Extra = {}; type_ = DistributionFamily; options = {}]’:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2280:67:   required from ‘pybind11::enum_<Type>::enum_(const pybind11::handle&, const char*, const Extra& ...) [with Extra = {}; Type = DistributionFamily]’\n/kaggle/working/Panther/pawX/bindings.cpp:26:60:   required from here\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1648:28: warning: ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>’ declared with greater visibility than the type of its field ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>::<record capture>’ [-Wattributes]\n 1648 |             with_internals([&](internals &internals) {\n      |                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1649 |                 auto &instances = record.module_local ? get_local_internals().registered_types_cpp\n      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1650 |                                                       : internals.registered_types_cpp;\n      |                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1651 |                 instances[std::type_index(typeid(type_alias))]\n      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1652 |                     = instances[std::type_index(typeid(type))];\n      |                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1653 |             });\n      |             ~               \n[6/7] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/skops.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[7/7] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o.d -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/linear_cuda.cu -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pawX-0.0.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n\u001b[0mObtaining file:///kaggle/working/Panther/pawX\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nInstalling collected packages: pawX\n  Attempting uninstall: pawX\n    Found existing installation: pawX 0.0.0\n    Uninstalling pawX-0.0.0:\n      Successfully uninstalled pawX-0.0.0\n  Running setup.py develop for pawX\nSuccessfully installed pawX-0.0.0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:16:35.585785Z","iopub.execute_input":"2025-04-26T12:16:35.586075Z","iopub.status.idle":"2025-04-26T12:16:35.590647Z","shell.execute_reply.started":"2025-04-26T12:16:35.586044Z","shell.execute_reply":"2025-04-26T12:16:35.589965Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/Panther/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:16:35.591500Z","iopub.execute_input":"2025-04-26T12:16:35.591746Z","iopub.status.idle":"2025-04-26T12:16:35.614177Z","shell.execute_reply.started":"2025-04-26T12:16:35.591716Z","shell.execute_reply":"2025-04-26T12:16:35.613544Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:16:35.615043Z","iopub.execute_input":"2025-04-26T12:16:35.615571Z","iopub.status.idle":"2025-04-26T12:16:35.746375Z","shell.execute_reply.started":"2025-04-26T12:16:35.615546Z","shell.execute_reply":"2025-04-26T12:16:35.745583Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/Panther\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import time\nimport numpy as np\nimport torch\nimport torch._dynamo\nimport torch._inductor.config as config\nimport itertools\nimport pandas as pd\n\n# Configure torch\nconfig.max_autotune_gemm = False\ntorch._dynamo.config.cache_size_limit = 1024\ntorch._dynamo.config.accumulated_cache_size_limit = 1024\n\ndef is_valid_params(in_features, out_features, num_terms, low_rank):\n    \"\"\"\n    Check if parameter combination is valid:\n    A combination is invalid if 2 * num_terms * low_rank * (out_features + in_features) >= out_features * in_features\n    \"\"\"\n    return 2 * num_terms * low_rank * (out_features + in_features) < out_features * in_features\n\nclass BenchmarkParams:\n    def __init__(self, \n                 in_features=256, \n                 out_features=256,\n                 num_terms=3,\n                 low_rank=8,\n                 batch_size=64, \n                 num_runs=200, \n                 warmup=15, \n                 device=torch.device(\"cuda\"),\n                 dtype=torch.float32\n                ,mode=0):\n        self.in_features = in_features\n        self.out_features = out_features\n        self.num_terms = num_terms\n        self.low_rank = low_rank\n        self.batch_size = batch_size\n        self.num_runs = num_runs\n        self.warmup = warmup\n        self.device = device\n        self.dtype = dtype\n        self.mode = mode\n\ndef benchmark_model(model, x, model_name, params):\n    \"\"\"\n    Generic benchmarking function for any PyTorch model.\n    \n    Args:\n        model: The PyTorch model to benchmark\n        x: Input tensor\n        model_name: Name of the model for logging\n        params: Benchmark parameters\n    \n    Returns:\n        Dictionary with benchmark results\n    \"\"\"\n    # Compile the model\n    # model_compiled = torch.compile(\n    #     model,\n    #     backend=\"inductor\",\n    #     fullgraph=True,\n    #     dynamic=False\n    # )\n    model_compiled = model\n    \n    # Benchmark forward pass\n    print(f\"\\n=== {model_name} FORWARD PASS BENCHMARK ===\")\n    \n    # Warmup runs for forward pass\n    model_compiled.eval()\n    with torch.no_grad():\n        for _ in range(params.warmup):\n            _ = model_compiled(x)\n    \n    torch.cuda.synchronize()\n    \n    # Actual timed runs for forward\n    forward_times = []\n    with torch.no_grad():\n        for _ in range(params.num_runs):\n            torch.cuda.synchronize()\n            start = time.perf_counter()\n            _ = model_compiled(x)\n            torch.cuda.synchronize()\n            end = time.perf_counter()\n            \n            forward_times.append((end - start) * 1000)  # Convert to ms\n    \n    mean_forward = np.mean(forward_times)\n    std_forward = np.std(forward_times)\n    print(f\"{model_name} forward: {mean_forward:.3f} ± {std_forward:.3f} ms\")\n    \n    # Benchmark backward pass\n    print(f\"\\n=== {model_name} BACKWARD PASS BENCHMARK ===\")\n    \n    # Warmup runs for backward pass\n    model_compiled.train()\n    for _ in range(params.warmup):\n        out = model_compiled(x)\n        loss = out.sum()\n        loss.backward()\n        x.grad.zero_()\n    \n    torch.cuda.synchronize()\n    \n    # Actual timed runs for backward\n    backward_times = []\n    for _ in range(params.num_runs):\n        out = model_compiled(x)\n        loss = out.sum()\n        \n        torch.cuda.synchronize()\n        start = time.perf_counter()\n        loss.backward()\n        torch.cuda.synchronize()\n        end = time.perf_counter()\n        \n        backward_times.append((end - start) * 1000)  # Convert to ms\n        x.grad.zero_()\n    \n    mean_backward = np.mean(backward_times)\n    std_backward = np.std(backward_times)\n    print(f\"{model_name} backward: {mean_backward:.3f} ± {std_backward:.3f} ms\")\n    \n    return {\n        \"forward\": {\n            \"mean\": mean_forward,\n            \"std\": std_forward,\n            \"times\": forward_times\n        },\n        \"backward\": {\n            \"mean\": mean_backward,\n            \"std\": std_backward,\n            \"times\": backward_times\n        }\n    }\n\ndef benchmark_model_factory(model_factory, model_name, params):\n    \"\"\"\n    Benchmark a model using a factory function.\n    \n    Args:\n        model_factory: Function that creates the model\n        model_name: Name of the model for logging\n        params: Benchmark parameters\n    \n    Returns:\n        Dictionary with benchmark results\n    \"\"\"\n    # Create the model\n    torch.manual_seed(42)\n    model = model_factory(params)\n    \n    # Create input tensor for benchmarking\n    x = torch.randn(params.batch_size, params.in_features, \n                  dtype=params.dtype, device=params.device, requires_grad=True)\n    \n    return benchmark_model(model, x, model_name, params)\n\nif __name__ == \"__main__\":\n    import torch.nn as nn\n    from panther.nn import SKLinear\n    \n    # Parameter combinations to test\n    ratios = [(1, 128), (128, 1), (1, 1), (2, 1), (1, 2)]\n    base_sizes = [256, 512, 1024, 8192, 16384]\n    num_terms_options = [1, 2, 3]\n    low_rank_options = [10, 15, 20, 50, 100, 150]\n    \n    # ratios = [(1, 128)]\n    # base_sizes = [256, ]\n    # num_terms_options = [3]\n    # low_rank_options = [15]\n    \n    # Define model factories\n    def create_sklinear(p):\n        return SKLinear(in_features=p.in_features, out_features=p.out_features, \n                       num_terms=p.num_terms, low_rank=p.low_rank, mode=p.mode,\n                       dtype=p.dtype, device=p.device)\n    \n    models_to_benchmark = [\n        (create_sklinear, \"SKLinear\")\n    ]\n    \n    # Prepare data structure to store all results\n    results_data = []\n    \n    # Iterate through all parameter combinations\n    total_combinations = len(ratios) * len(base_sizes) * len(num_terms_options) * len(low_rank_options)\n    current_combo = 0\n    \n    for ratio, base_size in itertools.product(ratios, base_sizes):\n        ratio_in, ratio_out = ratio\n        \n        # Calculate actual dimensions based on ratio and base size\n        if ratio_in == 1:\n            in_features = base_size\n            out_features = base_size * ratio_out\n        else:\n            out_features = base_size\n            in_features = base_size * ratio_in\n        \n        for num_terms, low_rank in itertools.product(num_terms_options, low_rank_options):\n            current_combo += 1\n            print(f\"\\n\\n{'='*20} COMBINATION {current_combo}/{total_combinations} {'='*20}\")\n            print(f\"In features: {in_features}, Out features: {out_features}, Ratio: {ratio_in}:{ratio_out}\")\n            print(f\"Base size: {base_size}, Num terms: {num_terms}, Low rank: {low_rank}\")\n            \n            # Check if parameters are valid\n            is_valid = is_valid_params(in_features, out_features, num_terms, low_rank)\n            \n            if not is_valid:\n                print(f\"INVALID COMBINATION: 2 * {num_terms} * {low_rank} * ({out_features} + {in_features}) >= {out_features} * {in_features}\")\n                print(\"Skipping benchmarks for this invalid combination\")\n                \n                # Add invalid entry to results data\n                for model_name in [m[1] for m in models_to_benchmark]:\n                    results_data.append({\n                        'model': model_name,\n                        'in_features': in_features,\n                        'out_features': out_features,\n                        'ratio': f\"{ratio_in}:{ratio_out}\",\n                        'base_size': base_size,\n                        'num_terms': num_terms,\n                        'low_rank': low_rank,\n                        'forward_mean_ms': float('nan'),\n                        'forward_std_ms': float('nan'),\n                        'backward_mean_ms': float('nan'),\n                        'backward_std_ms': float('nan'),\n                        'is_valid': False,\n                        'error': \"Invalid parameter combination\"\n                    })\n                continue\n            \n            # Create parameter object for this combination\n            params = BenchmarkParams(\n                in_features=in_features,\n                out_features=out_features,\n                num_terms=num_terms,\n                low_rank=low_rank\n            )\n\n            \n            all_results = {}\n            for model_factory, model_name in models_to_benchmark:\n                print(f\"\\n{'='*20} Benchmarking {model_name} {'='*20}\")\n                if True:\n                    results = benchmark_model_factory(model_factory, model_name, params)\n                    all_results[model_name] = results\n                    \n                    # Add result to our data collection\n                    results_data.append({\n                        'model': model_name,\n                        'in_features': in_features,\n                        'out_features': out_features,\n                        'ratio': f\"{ratio_in}:{ratio_out}\",\n                        'base_size': base_size,\n                        'num_terms': num_terms,\n                        'low_rank': low_rank,\n                        'forward_mean_ms': results['forward']['mean'],\n                        'forward_std_ms': results['forward']['std'],\n                        'backward_mean_ms': results['backward']['mean'],\n                        'backward_std_ms': results['backward']['std'],\n                        'is_valid': True\n                    })\n                # except Exception as e:\n                    print(f\"Error benchmarking {model_name}: {e}\")\n                    # Add error entry to data\n                    results_data.append({\n                        'model': model_name,\n                        'in_features': in_features,\n                        'out_features': out_features,\n                        'ratio': f\"{ratio_in}:{ratio_out}\",\n                        'base_size': base_size,\n                        'num_terms': num_terms,\n                        'low_rank': low_rank,\n                        'forward_mean_ms': float('nan'),\n                        'forward_std_ms': float('nan'),\n                        'backward_mean_ms': float('nan'),\n                        'backward_std_ms': float('nan'),\n                        'is_valid': True,\n                        'error': str(e)\n                    })\n            \n            # Print comparative summary for this combination\n            if all_results:\n                print(\"\\n\" + \"=\"*60)\n                print(f\"{'='*20} SUMMARY FOR CURRENT COMBINATION {'='*20}\")\n                print(\"=\"*60)\n                print(f\"{'Model':<20} {'Forward (ms)':<25} {'Backward (ms)':<25}\")\n                print(\"-\"*60)\n                \n                for model_name, results in all_results.items():\n                    fwd = f\"{results['forward']['mean']:.3f} ± {results['forward']['std']:.3f}\"\n                    bwd = f\"{results['backward']['mean']:.3f} ± {results['backward']['std']:.3f}\"\n                    print(f\"{model_name:<20} {fwd:<25} {bwd:<25}\")\n    \n    # Create a DataFrame with all results\n    df = pd.DataFrame(results_data)\n    \n    # Save results to CSV\n    results_file = \"benchmark_results.csv\"\n    df.to_csv(results_file, index=False)\n    print(f\"\\nAll benchmark results saved to {results_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:16:35.747602Z","iopub.execute_input":"2025-04-26T12:16:35.747813Z","execution_failed":"2025-04-26T12:31:54.175Z"}},"outputs":[{"name":"stdout","text":"\n\n==================== COMBINATION 1/450 ====================\nIn features: 256, Out features: 32768, Ratio: 1:128\nBase size: 256, Num terms: 1, Low rank: 10\n\n==================== Benchmarking SKLinear ====================\n\n=== SKLinear FORWARD PASS BENCHMARK ===\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\nIdeal Block Size (1D): 512\nResident Threads per SM: 1024 (out of 1024 available)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/Panther/tests\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-26T12:31:54.176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pytest","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-26T12:31:54.176Z"}},"outputs":[],"execution_count":null}]}