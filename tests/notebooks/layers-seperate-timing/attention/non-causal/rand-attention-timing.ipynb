{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b089f7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-07T18:39:33.991104Z",
     "iopub.status.busy": "2025-05-07T18:39:33.990902Z",
     "iopub.status.idle": "2025-05-07T18:39:34.121406Z",
     "shell.execute_reply": "2025-05-07T18:39:34.120932Z"
    },
    "papermill": {
     "duration": 0.135876,
     "end_time": "2025-05-07T18:39:34.122557",
     "exception": false,
     "start_time": "2025-05-07T18:39:33.986681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "token = user_secrets.get_secret(\"github_repos_wildcard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce621334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:39:34.128768Z",
     "iopub.status.busy": "2025-05-07T18:39:34.128401Z",
     "iopub.status.idle": "2025-05-07T18:39:34.131586Z",
     "shell.execute_reply": "2025-05-07T18:39:34.130906Z"
    },
    "papermill": {
     "duration": 0.00717,
     "end_time": "2025-05-07T18:39:34.132627",
     "exception": false,
     "start_time": "2025-05-07T18:39:34.125457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_url = f\"https://{token}@github.com/gaserSami/panther.git\"\n",
    "branch = \"autotuner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0aaa527",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:39:34.138273Z",
     "iopub.status.busy": "2025-05-07T18:39:34.138030Z",
     "iopub.status.idle": "2025-05-07T18:39:37.770908Z",
     "shell.execute_reply": "2025-05-07T18:39:37.769961Z"
    },
    "papermill": {
     "duration": 3.637394,
     "end_time": "2025-05-07T18:39:37.772520",
     "exception": false,
     "start_time": "2025-05-07T18:39:34.135126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'panther'...\r\n",
      "remote: Enumerating objects: 1046, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (181/181), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (96/96), done.\u001b[K\r\n",
      "remote: Total 1046 (delta 125), reused 110 (delta 85), pack-reused 865 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (1046/1046), 27.77 MiB | 16.94 MiB/s, done.\r\n",
      "Resolving deltas: 100% (626/626), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone -b {branch} {repo_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8be9c87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:39:37.780690Z",
     "iopub.status.busy": "2025-05-07T18:39:37.780434Z",
     "iopub.status.idle": "2025-05-07T18:39:37.899220Z",
     "shell.execute_reply": "2025-05-07T18:39:37.898294Z"
    },
    "papermill": {
     "duration": 0.124053,
     "end_time": "2025-05-07T18:39:37.900470",
     "exception": false,
     "start_time": "2025-05-07T18:39:37.776417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv panther Panther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9874e70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:39:37.908323Z",
     "iopub.status.busy": "2025-05-07T18:39:37.907844Z",
     "iopub.status.idle": "2025-05-07T18:42:07.895662Z",
     "shell.execute_reply": "2025-05-07T18:42:07.894904Z"
    },
    "papermill": {
     "duration": 149.993156,
     "end_time": "2025-05-07T18:42:07.897027",
     "exception": false,
     "start_time": "2025-05-07T18:39:37.903871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.5.1+cu124\r\n",
      "Uninstalling torch-2.5.1+cu124:\r\n",
      "  Successfully uninstalled torch-2.5.1+cu124\r\n",
      "Found existing installation: torchvision 0.20.1+cu124\r\n",
      "Uninstalling torchvision-0.20.1+cu124:\r\n",
      "  Successfully uninstalled torchvision-0.20.1+cu124\r\n",
      "Found existing installation: torchaudio 2.5.1+cu124\r\n",
      "Uninstalling torchaudio-2.5.1+cu124:\r\n",
      "  Successfully uninstalled torchaudio-2.5.1+cu124\r\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu124\r\n",
      "Collecting torch==2.6.0+cu124\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (28 kB)\r\n",
      "Collecting torchvision==0.21.0+cu124\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\r\n",
      "Collecting torchaudio==2.6.0+cu124\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (4.13.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting triton==3.2.0 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (1.13.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (11.1.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0+cu124) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0+cu124) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0+cu124) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0+cu124) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.21.0+cu124) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.21.0+cu124) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.21.0+cu124) (2024.2.0)\r\n",
      "Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (768.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl (7.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchaudio, torchvision\r\n",
      "  Attempting uninstall: triton\r\n",
      "    Found existing installation: triton 3.1.0\r\n",
      "    Uninstalling triton-3.1.0:\r\n",
      "      Successfully uninstalled triton-3.1.0\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu124 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nvjitlink-cu12-12.4.127 torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124 triton-3.2.0\r\n"
     ]
    }
   ],
   "source": [
    "# First uninstall existing torch, torchvision, torchaudio\n",
    "!pip uninstall -y torch torchvision torchaudio\n",
    "\n",
    "# Install the specified versions from PyTorch's official CUDA 12.4 wheels\n",
    "!pip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "856952cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:42:08.019132Z",
     "iopub.status.busy": "2025-05-07T18:42:08.018523Z",
     "iopub.status.idle": "2025-05-07T18:42:10.042661Z",
     "shell.execute_reply": "2025-05-07T18:42:10.041791Z"
    },
    "papermill": {
     "duration": 2.108145,
     "end_time": "2025-05-07T18:42:10.043935",
     "exception": false,
     "start_time": "2025-05-07T18:42:07.935790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "import triton\n",
    "\n",
    "print(triton.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "494ab589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:42:10.121418Z",
     "iopub.status.busy": "2025-05-07T18:42:10.121079Z",
     "iopub.status.idle": "2025-05-07T18:42:10.124231Z",
     "shell.execute_reply": "2025-05-07T18:42:10.123676Z"
    },
    "papermill": {
     "duration": 0.042516,
     "end_time": "2025-05-07T18:42:10.125177",
     "exception": false,
     "start_time": "2025-05-07T18:42:10.082661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !export LC_ALL=\"en_US.UTF-8\"\n",
    "# !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "# !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "# !ldconfig /usr/lib64-nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "125a9bbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:42:10.202278Z",
     "iopub.status.busy": "2025-05-07T18:42:10.201762Z",
     "iopub.status.idle": "2025-05-07T18:42:10.206766Z",
     "shell.execute_reply": "2025-05-07T18:42:10.206194Z"
    },
    "papermill": {
     "duration": 0.044696,
     "end_time": "2025-05-07T18:42:10.207770",
     "exception": false,
     "start_time": "2025-05-07T18:42:10.163074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/Panther/pawX/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/Panther/pawX/setup.py\n",
    "from setuptools import setup\n",
    "from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n",
    "\n",
    "setup(\n",
    "    name=\"pawX\",\n",
    "    ext_modules=[\n",
    "        CUDAExtension(\n",
    "            name=\"pawX\",\n",
    "            sources=[\n",
    "                \"skops.cpp\",\n",
    "                \"bindings.cpp\",\n",
    "                \"linear.cpp\",\n",
    "                \"linear_cuda.cu\",\n",
    "                \"cqrrpt.cpp\",\n",
    "                \"rsvd.cpp\",\n",
    "                \"attention.cpp\",\n",
    "                \"conv2d.cpp\"\n",
    "            ],\n",
    "            # Use system includes and libraries\n",
    "            include_dirs=[\"/usr/include/x86_64-linux-gnu\"],\n",
    "            library_dirs=[],\n",
    "            libraries=[\"openblas\"],\n",
    "            extra_compile_args={\"cxx\": [\"-O2\", \"-fopenmp\"], \"nvcc\": [\"-O2\"]},\n",
    "            extra_link_args=[\"-llapacke\", \"-lopenblas\"]\n",
    "        )\n",
    "    ],\n",
    "    cmdclass={\"build_ext\": BuildExtension},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9db179b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:42:10.285894Z",
     "iopub.status.busy": "2025-05-07T18:42:10.285684Z",
     "iopub.status.idle": "2025-05-07T18:42:18.567333Z",
     "shell.execute_reply": "2025-05-07T18:42:18.566621Z"
    },
    "papermill": {
     "duration": 8.322742,
     "end_time": "2025-05-07T18:42:18.568728",
     "exception": false,
     "start_time": "2025-05-07T18:42:10.245986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "The following additional packages will be installed:\r\n",
      "  liblapacke libtmglib-dev libtmglib3\r\n",
      "Suggested packages:\r\n",
      "  liblapack-doc\r\n",
      "The following NEW packages will be installed:\r\n",
      "  liblapacke liblapacke-dev libtmglib-dev libtmglib3\r\n",
      "0 upgraded, 4 newly installed, 0 to remove and 122 not upgraded.\r\n",
      "Need to get 1,071 kB of archives.\r\n",
      "After this operation, 12.3 MB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtmglib3 amd64 3.10.0-2ubuntu1 [144 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblapacke amd64 3.10.0-2ubuntu1 [435 kB]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtmglib-dev amd64 3.10.0-2ubuntu1 [134 kB]\r\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblapacke-dev amd64 3.10.0-2ubuntu1 [358 kB]\r\n",
      "Fetched 1,071 kB in 1s (1,006 kB/s)\r\n",
      "debconf: unable to initialize frontend: Dialog\r\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\r\n",
      "debconf: falling back to frontend: Readline\r\n",
      "Selecting previously unselected package libtmglib3:amd64.\r\n",
      "(Reading database ... 128691 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libtmglib3_3.10.0-2ubuntu1_amd64.deb ...\r\n",
      "Unpacking libtmglib3:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Selecting previously unselected package liblapacke:amd64.\r\n",
      "Preparing to unpack .../liblapacke_3.10.0-2ubuntu1_amd64.deb ...\r\n",
      "Unpacking liblapacke:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Selecting previously unselected package libtmglib-dev:amd64.\r\n",
      "Preparing to unpack .../libtmglib-dev_3.10.0-2ubuntu1_amd64.deb ...\r\n",
      "Unpacking libtmglib-dev:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Selecting previously unselected package liblapacke-dev:amd64.\r\n",
      "Preparing to unpack .../liblapacke-dev_3.10.0-2ubuntu1_amd64.deb ...\r\n",
      "Unpacking liblapacke-dev:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Setting up libtmglib3:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Setting up liblapacke:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Setting up libtmglib-dev:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Setting up liblapacke-dev:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install liblapacke-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e03e142",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:42:18.649689Z",
     "iopub.status.busy": "2025-05-07T18:42:18.649056Z",
     "iopub.status.idle": "2025-05-07T18:44:43.265936Z",
     "shell.execute_reply": "2025-05-07T18:44:43.265194Z"
    },
    "papermill": {
     "duration": 144.658113,
     "end_time": "2025-05-07T18:44:43.267411",
     "exception": false,
     "start_time": "2025-05-07T18:42:18.609298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\r\n",
      "!!\r\n",
      "\r\n",
      "        ********************************************************************************\r\n",
      "        Please avoid running ``setup.py`` directly.\r\n",
      "        Instead, use pypa/build, pypa/installer or other\r\n",
      "        standards-based tools.\r\n",
      "\r\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\r\n",
      "        ********************************************************************************\r\n",
      "\r\n",
      "!!\r\n",
      "  self.initialize_options()\r\n",
      "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\r\n",
      "!!\r\n",
      "\r\n",
      "        ********************************************************************************\r\n",
      "        Please avoid running ``setup.py`` and ``easy_install``.\r\n",
      "        Instead, use pypa/build, pypa/installer or other\r\n",
      "        standards-based tools.\r\n",
      "\r\n",
      "        See https://github.com/pypa/setuptools/issues/917 for details.\r\n",
      "        ********************************************************************************\r\n",
      "\r\n",
      "!!\r\n",
      "  self.initialize_options()\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\r\n",
      "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\r\n",
      "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\r\n",
      "  warnings.warn(\r\n",
      "Emitting ninja build file /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/build.ninja...\r\n",
      "Compiling objects...\r\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n",
      "[1/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/conv2d.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/conv2d.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/conv2d.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "/kaggle/working/Panther/pawX/conv2d.cpp: In function ‘std::vector<at::Tensor> sketched_conv2d_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const std::vector<long int>&, const std::vector<long int>&, const std::vector<long int>&, const at::Tensor&)’:\r\n",
      "/kaggle/working/Panther/pawX/conv2d.cpp:19:43: warning: unused variable ‘H’ [-Wunused-variable]\r\n",
      "   19 |     int64_t B = x.size(0), C = x.size(1), H = x.size(2), W = x.size(3);\r\n",
      "      |                                           ^\r\n",
      "/kaggle/working/Panther/pawX/conv2d.cpp:19:58: warning: unused variable ‘W’ [-Wunused-variable]\r\n",
      "   19 |     int64_t B = x.size(0), C = x.size(1), H = x.size(2), W = x.size(3);\r\n",
      "      |                                                          ^\r\n",
      "[2/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/cqrrpt.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "[3/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/linear.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "[4/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/attention.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "[5/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/bindings.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "In file included from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/Exceptions.h:12,\r\n",
      "                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/python.h:11,\r\n",
      "                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/extension.h:9,\r\n",
      "                 from /kaggle/working/Panther/pawX/attention.h:3,\r\n",
      "                 from /kaggle/working/Panther/pawX/bindings.cpp:1:\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘class pybind11::class_<DistributionFamily>’:\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2216:7:   required from ‘class pybind11::enum_<DistributionFamily>’\r\n",
      "/kaggle/working/Panther/pawX/bindings.cpp:24:58:   required from here\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1539:7: warning: ‘pybind11::class_<DistributionFamily>’ declared with greater visibility than its base ‘pybind11::detail::generic_type’ [-Wattributes]\r\n",
      " 1539 | class class_ : public detail::generic_type {\r\n",
      "      |       ^~~~~~\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘pybind11::class_< <template-parameter-1-1>, <template-parameter-1-2> >::class_(pybind11::handle, const char*, const Extra& ...) [with Extra = {}; type_ = DistributionFamily; options = {}]’:\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2232:67:   required from ‘pybind11::enum_<Type>::enum_(const pybind11::handle&, const char*, const Extra& ...) [with Extra = {}; Type = DistributionFamily]’\r\n",
      "/kaggle/working/Panther/pawX/bindings.cpp:24:58:   required from here\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1599:28: warning: ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>’ declared with greater visibility than the type of its field ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>::<record capture>’ [-Wattributes]\r\n",
      " 1599 |             with_internals([&](internals &internals) {\r\n",
      "      |                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1600 |                 auto &instances = record.module_local ? get_local_internals().registered_types_cpp\r\n",
      "      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1601 |                                                       : internals.registered_types_cpp;\r\n",
      "      |                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1602 |                 instances[std::type_index(typeid(type_alias))]\r\n",
      "      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1603 |                     = instances[std::type_index(typeid(type))];\r\n",
      "      |                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1604 |             });\r\n",
      "      |             ~               \r\n",
      "[6/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/rsvd.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "[7/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/skops.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "[8/8] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o.d -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/linear_cuda.cu -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\r\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pawX-0.0.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\r\n",
      "\u001b[0mObtaining file:///kaggle/working/Panther/pawX\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Installing collected packages: pawX\r\n",
      "  Attempting uninstall: pawX\r\n",
      "    Found existing installation: pawX 0.0.0\r\n",
      "    Uninstalling pawX-0.0.0:\r\n",
      "      Successfully uninstalled pawX-0.0.0\r\n",
      "  Running setup.py develop for pawX\r\n",
      "Successfully installed pawX-0.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!cd /kaggle/working/Panther/pawX; python setup.py install\n",
    "!cd /kaggle/working/Panther/pawX; pip install --no-build-isolation -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d50a721a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:44:43.347892Z",
     "iopub.status.busy": "2025-05-07T18:44:43.347636Z",
     "iopub.status.idle": "2025-05-07T18:44:43.351259Z",
     "shell.execute_reply": "2025-05-07T18:44:43.350712Z"
    },
    "papermill": {
     "duration": 0.044776,
     "end_time": "2025-05-07T18:44:43.352449",
     "exception": false,
     "start_time": "2025-05-07T18:44:43.307673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9f723dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:44:43.432398Z",
     "iopub.status.busy": "2025-05-07T18:44:43.431754Z",
     "iopub.status.idle": "2025-05-07T18:44:43.434974Z",
     "shell.execute_reply": "2025-05-07T18:44:43.434505Z"
    },
    "papermill": {
     "duration": 0.044242,
     "end_time": "2025-05-07T18:44:43.435975",
     "exception": false,
     "start_time": "2025-05-07T18:44:43.391733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/kaggle/working/Panther/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01e83036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:44:43.515523Z",
     "iopub.status.busy": "2025-05-07T18:44:43.515332Z",
     "iopub.status.idle": "2025-05-07T18:44:43.639794Z",
     "shell.execute_reply": "2025-05-07T18:44:43.639117Z"
    },
    "papermill": {
     "duration": 0.165334,
     "end_time": "2025-05-07T18:44:43.640944",
     "exception": false,
     "start_time": "2025-05-07T18:44:43.475610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Panther\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a42ecd05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:44:43.722402Z",
     "iopub.status.busy": "2025-05-07T18:44:43.722180Z",
     "iopub.status.idle": "2025-05-08T02:50:56.539822Z",
     "shell.execute_reply": "2025-05-08T02:50:56.539018Z"
    },
    "papermill": {
     "duration": 29172.905367,
     "end_time": "2025-05-08T02:50:56.586252",
     "exception": false,
     "start_time": "2025-05-07T18:44:43.680885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: botorch is not available. Install with: pip install botorch\n",
      "\n",
      "\n",
      "==================== COMBINATION 1/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 8.913 ± 0.464 ms, Memory: 344.89 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 13.668 ± 0.057 ms, Memory: 405.45 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         8.913 ± 0.464             13.668 ± 0.057            344.89                    405.45                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 2/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 17.441 ± 0.116 ms, Memory: 689.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 27.012 ± 0.133 ms, Memory: 792.08 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         17.441 ± 0.116            27.012 ± 0.133            689.51                    792.08                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 3/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 35.096 ± 0.226 ms, Memory: 1362.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 54.733 ± 0.323 ms, Memory: 1565.33 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         35.096 ± 0.226            54.733 ± 0.323            1362.51                   1565.33                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 4/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 71.778 ± 0.418 ms, Memory: 2708.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 112.473 ± 1.208 ms, Memory: 3111.83 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         71.778 ± 0.418            112.473 ± 1.208           2708.51                   3111.83                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 5/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 155.406 ± 1.156 ms, Memory: 5400.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 229.714 ± 1.377 ms, Memory: 6204.83 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         155.406 ± 1.156           229.714 ± 1.377           5400.51                   6204.83                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 6/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 6.575 ± 0.036 ms, Memory: 272.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 8.456 ± 0.064 ms, Memory: 371.45 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         6.575 ± 0.036             8.456 ± 0.064             272.51                    371.45                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 7/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 12.960 ± 0.084 ms, Memory: 528.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 16.422 ± 0.119 ms, Memory: 724.08 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         12.960 ± 0.084            16.422 ± 0.119            528.51                    724.08                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 8/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 25.637 ± 0.189 ms, Memory: 1040.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 32.417 ± 0.388 ms, Memory: 1429.33 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         25.637 ± 0.189            32.417 ± 0.388            1040.51                   1429.33                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 9/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 51.356 ± 0.520 ms, Memory: 2064.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 64.641 ± 0.453 ms, Memory: 2839.83 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         51.356 ± 0.520            64.641 ± 0.453            2064.51                   2839.83                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 10/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 104.582 ± 0.944 ms, Memory: 4112.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 130.784 ± 0.726 ms, Memory: 5660.83 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         104.582 ± 0.944           130.784 ± 0.726           4112.51                   5660.83                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 11/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 14.664 ± 0.127 ms, Memory: 577.02 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 22.641 ± 0.146 ms, Memory: 615.52 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         14.664 ± 0.127            22.641 ± 0.146            577.02                    615.52                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 12/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 28.981 ± 0.165 ms, Memory: 1137.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 44.947 ± 0.271 ms, Memory: 1242.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         28.981 ± 0.165            44.947 ± 0.271            1137.52                   1242.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 13/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 58.297 ± 0.609 ms, Memory: 2258.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 90.679 ± 0.575 ms, Memory: 2431.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         58.297 ± 0.609            90.679 ± 0.575            2258.52                   2431.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 14/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 117.100 ± 0.857 ms, Memory: 4500.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 182.136 ± 1.425 ms, Memory: 4841.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         117.100 ± 0.857           182.136 ± 1.425           4500.52                   4841.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 15/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 242.478 ± 0.424 ms, Memory: 8984.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 369.003 ± 1.013 ms, Memory: 9661.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         242.478 ± 0.424           369.003 ± 1.013           8984.52                   9661.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 16/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 10.333 ± 0.073 ms, Memory: 432.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 12.654 ± 0.088 ms, Memory: 581.52 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         10.333 ± 0.073            12.654 ± 0.088            432.52                    581.52                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 17/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 20.260 ± 0.247 ms, Memory: 848.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 24.852 ± 0.195 ms, Memory: 1174.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         20.260 ± 0.247            24.852 ± 0.195            848.52                    1174.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 18/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 41.069 ± 0.426 ms, Memory: 1680.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 49.456 ± 0.396 ms, Memory: 2295.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         41.069 ± 0.426            49.456 ± 0.396            1680.52                   2295.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 19/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 81.522 ± 1.054 ms, Memory: 3344.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 99.788 ± 0.596 ms, Memory: 4569.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         81.522 ± 1.054            99.788 ± 0.596            3344.52                   4569.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 20/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 168.433 ± 1.362 ms, Memory: 6672.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 205.283 ± 0.777 ms, Memory: 9117.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         168.433 ± 1.362           205.283 ± 0.777           6672.52                   9117.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 21/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 25.060 ± 0.121 ms, Memory: 1025.03 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 39.396 ± 0.165 ms, Memory: 1067.79 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         25.060 ± 0.121            39.396 ± 0.165            1025.03                   1067.79                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 22/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 50.112 ± 0.374 ms, Memory: 2033.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 79.193 ± 0.714 ms, Memory: 2110.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         50.112 ± 0.374            79.193 ± 0.714            2033.53                   2110.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 23/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 100.900 ± 0.267 ms, Memory: 4050.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 161.597 ± 1.351 ms, Memory: 4195.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         100.900 ± 0.267           161.597 ± 1.351           4050.53                   4195.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 24/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 202.056 ± 0.509 ms, Memory: 8084.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 323.960 ± 0.699 ms, Memory: 8365.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         202.056 ± 0.509           323.960 ± 0.699           8084.53                   8365.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 25/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 804.12 MiB is free. Process 4036 has 13.95 GiB memory in use. Of the allocated memory 13.77 GiB is allocated by PyTorch, and 13.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 26/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 18.171 ± 0.153 ms, Memory: 752.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 21.331 ± 0.140 ms, Memory: 1033.79 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         18.171 ± 0.153            21.331 ± 0.140            752.53                    1033.79                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 27/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 36.306 ± 0.243 ms, Memory: 1488.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 42.511 ± 0.478 ms, Memory: 2042.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         36.306 ± 0.243            42.511 ± 0.478            1488.53                   2042.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 28/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 72.909 ± 0.756 ms, Memory: 2960.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 86.176 ± 0.776 ms, Memory: 4059.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         72.909 ± 0.756            86.176 ± 0.776            2960.53                   4059.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 29/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 147.225 ± 1.541 ms, Memory: 5904.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 176.710 ± 0.766 ms, Memory: 8093.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         147.225 ± 1.541           176.710 ± 0.766           5904.53                   8093.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 30/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 307.114 ± 1.068 ms, Memory: 11792.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 548.12 MiB is free. Process 4036 has 14.20 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 13.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 31/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 15.560 ± 0.106 ms, Memory: 577.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 23.461 ± 0.114 ms, Memory: 616.01 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         15.560 ± 0.106            23.461 ± 0.114            577.51                    616.01                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 32/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 30.951 ± 0.209 ms, Memory: 1138.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 46.800 ± 0.327 ms, Memory: 1245.01 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         30.951 ± 0.209            46.800 ± 0.327            1138.51                   1245.01                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 33/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 61.508 ± 0.583 ms, Memory: 2260.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 94.086 ± 0.359 ms, Memory: 2439.01 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         61.508 ± 0.583            94.086 ± 0.359            2260.51                   2439.01                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 34/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 124.321 ± 0.906 ms, Memory: 4504.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 187.154 ± 0.944 ms, Memory: 4859.01 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         124.321 ± 0.906           187.154 ± 0.944           4504.51                   4859.01                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 35/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 254.281 ± 0.681 ms, Memory: 8992.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 376.209 ± 2.197 ms, Memory: 9699.01 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         254.281 ± 0.681           376.209 ± 2.197           8992.51                   9699.01                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 36/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 10.560 ± 0.098 ms, Memory: 432.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 12.578 ± 0.099 ms, Memory: 580.01 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         10.560 ± 0.098            12.578 ± 0.099            432.51                    580.01                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 37/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 20.981 ± 0.185 ms, Memory: 848.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 24.723 ± 0.146 ms, Memory: 1173.01 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         20.981 ± 0.185            24.723 ± 0.146            848.51                    1173.01                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 38/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 41.738 ± 0.416 ms, Memory: 1680.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 48.899 ± 0.208 ms, Memory: 2295.01 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         41.738 ± 0.416            48.899 ± 0.208            1680.51                   2295.01                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 39/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 83.790 ± 0.881 ms, Memory: 3344.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 98.085 ± 0.206 ms, Memory: 4571.01 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         83.790 ± 0.881            98.085 ± 0.206            3344.51                   4571.01                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 40/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 171.553 ± 1.173 ms, Memory: 6672.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 197.368 ± 0.690 ms, Memory: 9123.01 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         171.553 ± 1.173           197.368 ± 0.690           6672.51                   9123.01                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 41/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 25.729 ± 0.150 ms, Memory: 1025.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 39.623 ± 0.175 ms, Memory: 1066.27 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         25.729 ± 0.150            39.623 ± 0.175            1025.51                   1066.27                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 42/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 50.869 ± 0.234 ms, Memory: 2034.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 79.246 ± 0.461 ms, Memory: 2111.27 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         50.869 ± 0.234            79.246 ± 0.461            2034.51                   2111.27                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 43/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 102.148 ± 0.578 ms, Memory: 4052.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 161.587 ± 0.452 ms, Memory: 4201.27 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         102.148 ± 0.578           161.587 ± 0.452           4052.51                   4201.27                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 44/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 207.007 ± 1.277 ms, Memory: 8088.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 321.078 ± 0.462 ms, Memory: 8381.27 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         207.007 ± 1.277           321.078 ± 0.462           8088.51                   8381.27                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 45/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 804.12 MiB is free. Process 4036 has 13.95 GiB memory in use. Of the allocated memory 11.80 GiB is allocated by PyTorch, and 1.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 46/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 18.150 ± 0.169 ms, Memory: 752.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 20.814 ± 0.115 ms, Memory: 1030.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         18.150 ± 0.169            20.814 ± 0.115            752.51                    1030.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 47/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 35.939 ± 0.196 ms, Memory: 1488.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 41.105 ± 0.189 ms, Memory: 2039.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         35.939 ± 0.196            41.105 ± 0.189            1488.51                   2039.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 48/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 71.777 ± 0.523 ms, Memory: 2960.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 82.806 ± 0.464 ms, Memory: 4057.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         71.777 ± 0.523            82.806 ± 0.464            2960.51                   4057.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 49/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 143.526 ± 1.312 ms, Memory: 5904.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 168.076 ± 1.018 ms, Memory: 8093.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         143.526 ± 1.312           168.076 ± 1.018           5904.51                   8093.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 50/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 301.201 ± 1.089 ms, Memory: 11792.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 548.12 MiB is free. Process 4036 has 14.20 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 17.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 51/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 45.997 ± 0.218 ms, Memory: 1921.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 72.737 ± 0.439 ms, Memory: 1966.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         45.997 ± 0.218            72.737 ± 0.439            1921.52                   1966.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 52/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 92.116 ± 0.423 ms, Memory: 3826.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 147.268 ± 1.128 ms, Memory: 3907.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         92.116 ± 0.423            147.268 ± 1.128           3826.52                   3907.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 53/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 185.157 ± 1.395 ms, Memory: 7636.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 297.980 ± 1.770 ms, Memory: 7789.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         185.157 ± 1.395           297.980 ± 1.770           7636.52                   7789.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 54/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.54 GiB is free. Process 4036 has 13.20 GiB memory in use. Of the allocated memory 12.90 GiB is allocated by PyTorch, and 141.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 55/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 4036 has 11.19 GiB memory in use. Of the allocated memory 9.80 GiB is allocated by PyTorch, and 1.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 56/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 33.301 ± 0.165 ms, Memory: 1392.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 37.562 ± 0.187 ms, Memory: 1930.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         33.301 ± 0.165            37.562 ± 0.187            1392.52                   1930.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 57/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 66.668 ± 0.337 ms, Memory: 2768.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 75.223 ± 0.305 ms, Memory: 3835.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         66.668 ± 0.337            75.223 ± 0.305            2768.52                   3835.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 58/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 134.016 ± 1.109 ms, Memory: 5520.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 153.384 ± 1.978 ms, Memory: 7645.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         134.016 ± 1.109           153.384 ± 1.978           5520.52                   7645.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 59/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 276.672 ± 0.818 ms, Memory: 11024.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.43 GiB is free. Process 4036 has 13.31 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 124.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 60/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 4036 has 11.19 GiB memory in use. Of the allocated memory 9.52 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 61/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 27.663 ± 0.170 ms, Memory: 1026.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 41.469 ± 0.240 ms, Memory: 1069.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         27.663 ± 0.170            41.469 ± 0.240            1026.51                   1069.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 62/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 55.308 ± 0.315 ms, Memory: 2036.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 83.572 ± 0.482 ms, Memory: 2119.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         55.308 ± 0.315            83.572 ± 0.482            2036.51                   2119.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 63/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 110.980 ± 0.757 ms, Memory: 4056.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 167.194 ± 0.681 ms, Memory: 4219.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         110.980 ± 0.757           167.194 ± 0.681           4056.51                   4219.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 64/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 222.389 ± 0.644 ms, Memory: 8096.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 332.582 ± 1.457 ms, Memory: 8419.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         222.389 ± 0.644           332.582 ± 1.457           8096.51                   8419.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 65/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 4036 has 13.19 GiB memory in use. Of the allocated memory 11.83 GiB is allocated by PyTorch, and 1.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 66/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 18.613 ± 0.181 ms, Memory: 752.50 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 20.897 ± 0.120 ms, Memory: 1029.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         18.613 ± 0.181            20.897 ± 0.120            752.50                    1029.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 67/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 37.317 ± 0.247 ms, Memory: 1488.50 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 41.456 ± 0.224 ms, Memory: 2039.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         37.317 ± 0.247            41.456 ± 0.224            1488.50                   2039.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 68/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 74.428 ± 0.471 ms, Memory: 2960.50 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 83.024 ± 0.192 ms, Memory: 4059.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         74.428 ± 0.471            83.024 ± 0.192            2960.50                   4059.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 69/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 149.179 ± 0.956 ms, Memory: 5904.50 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 166.664 ± 0.682 ms, Memory: 8099.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         149.179 ± 0.956           166.664 ± 0.682           5904.50                   8099.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 70/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 309.431 ± 0.962 ms, Memory: 11792.50 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 4036 has 13.19 GiB memory in use. Of the allocated memory 11.52 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 71/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 47.645 ± 0.192 ms, Memory: 1922.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 73.721 ± 0.255 ms, Memory: 1967.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         47.645 ± 0.192            73.721 ± 0.255            1922.51                   1967.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 72/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 95.254 ± 0.314 ms, Memory: 3828.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 148.530 ± 0.517 ms, Memory: 3913.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         95.254 ± 0.314            148.530 ± 0.517           3828.51                   3913.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 73/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 192.283 ± 0.768 ms, Memory: 7640.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 296.167 ± 0.833 ms, Memory: 7805.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         192.283 ± 0.768           296.167 ± 0.833           7640.51                   7805.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 74/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 4036 has 13.19 GiB memory in use. Of the allocated memory 10.92 GiB is allocated by PyTorch, and 2.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 75/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 4036 has 11.19 GiB memory in use. Of the allocated memory 9.83 GiB is allocated by PyTorch, and 1.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 76/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 33.543 ± 0.209 ms, Memory: 1392.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 37.363 ± 0.210 ms, Memory: 1927.76 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         33.543 ± 0.209            37.363 ± 0.210            1392.51                   1927.76                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 77/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 66.829 ± 0.348 ms, Memory: 2768.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 75.098 ± 0.214 ms, Memory: 3833.76 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         66.829 ± 0.348            75.098 ± 0.214            2768.51                   3833.76                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 78/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 134.432 ± 1.084 ms, Memory: 5520.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 149.952 ± 0.293 ms, Memory: 7645.76 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         134.432 ± 1.084           149.952 ± 0.293           5520.51                   7645.76                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 79/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 276.790 ± 1.084 ms, Memory: 11024.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.43 GiB is free. Process 4036 has 13.31 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 128.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 80/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 4036 has 11.19 GiB memory in use. Of the allocated memory 9.52 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 81/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 88.029 ± 0.332 ms, Memory: 3714.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 140.220 ± 0.822 ms, Memory: 3764.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         88.029 ± 0.332            140.220 ± 0.822           3714.51                   3764.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 82/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 178.671 ± 1.473 ms, Memory: 7412.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 283.418 ± 1.709 ms, Memory: 7502.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         178.671 ± 1.473           283.418 ± 1.709           7412.51                   7502.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 83/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.80 GiB is free. Process 4036 has 12.94 GiB memory in use. Of the allocated memory 12.46 GiB is allocated by PyTorch, and 317.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 84/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.80 GiB is free. Process 4036 has 10.94 GiB memory in use. Of the allocated memory 8.92 GiB is allocated by PyTorch, and 1.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 85/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.80 GiB is free. Process 4036 has 10.94 GiB memory in use. Of the allocated memory 9.83 GiB is allocated by PyTorch, and 965.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 86/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 63.484 ± 0.301 ms, Memory: 2672.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 70.893 ± 0.245 ms, Memory: 3724.76 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         63.484 ± 0.301            70.893 ± 0.245            2672.51                   3724.76                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 87/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 127.946 ± 0.794 ms, Memory: 5328.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 145.656 ± 1.181 ms, Memory: 7422.76 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         127.946 ± 0.794           145.656 ± 1.181           5328.51                   7422.76                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 88/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 263.630 ± 0.861 ms, Memory: 10640.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.80 GiB is free. Process 4036 has 12.94 GiB memory in use. Of the allocated memory 12.52 GiB is allocated by PyTorch, and 252.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 89/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.80 GiB is free. Process 4036 has 12.94 GiB memory in use. Of the allocated memory 12.77 GiB is allocated by PyTorch, and 5.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 90/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.80 GiB is free. Process 4036 has 12.94 GiB memory in use. Of the allocated memory 9.52 GiB is allocated by PyTorch, and 3.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 91/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 14.961 ± 0.178 ms, Memory: 465.77 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 23.512 ± 0.311 ms, Memory: 632.96 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         14.961 ± 0.178            23.512 ± 0.311            465.77                    632.96                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 92/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 30.110 ± 0.265 ms, Memory: 914.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 46.020 ± 0.718 ms, Memory: 1243.59 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         30.110 ± 0.265            46.020 ± 0.718            914.27                    1243.59                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 93/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 60.480 ± 0.431 ms, Memory: 1811.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 92.008 ± 0.617 ms, Memory: 2464.84 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         60.480 ± 0.431            92.008 ± 0.617            1811.27                   2464.84                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 94/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 117.439 ± 1.147 ms, Memory: 3605.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 183.926 ± 0.913 ms, Memory: 4907.34 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         117.439 ± 1.147           183.926 ± 0.913           3605.27                   4907.34                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 95/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 242.331 ± 0.678 ms, Memory: 7193.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 395.668 ± 2.589 ms, Memory: 9792.34 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         242.331 ± 0.678           395.668 ± 2.589           7193.27                   9792.34                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 96/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 11.717 ± 0.120 ms, Memory: 369.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 16.420 ± 0.228 ms, Memory: 566.96 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         11.717 ± 0.120            16.420 ± 0.228            369.27                    566.96                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 97/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 23.623 ± 0.361 ms, Memory: 721.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 31.312 ± 0.411 ms, Memory: 1111.59 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         23.623 ± 0.361            31.312 ± 0.411            721.27                    1111.59                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 98/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 47.509 ± 0.579 ms, Memory: 1425.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 62.869 ± 1.056 ms, Memory: 2200.84 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         47.509 ± 0.579            62.869 ± 1.056            1425.27                   2200.84                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 99/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 90.761 ± 0.574 ms, Memory: 2833.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 124.112 ± 1.699 ms, Memory: 4379.34 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         90.761 ± 0.574            124.112 ± 1.699           2833.27                   4379.34                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 100/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 181.446 ± 0.535 ms, Memory: 5649.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 273.229 ± 1.528 ms, Memory: 8736.34 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         181.446 ± 0.535           273.229 ± 1.528           5649.27                   8736.34                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 101/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 19.983 ± 0.164 ms, Memory: 689.79 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 31.786 ± 0.329 ms, Memory: 797.04 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         19.983 ± 0.164            31.786 ± 0.329            689.79                    797.04                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 102/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 40.197 ± 0.329 ms, Memory: 1362.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 62.636 ± 0.576 ms, Memory: 1567.67 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         40.197 ± 0.329            62.636 ± 0.576            1362.29                   1567.67                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 103/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 81.431 ± 0.849 ms, Memory: 2707.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 127.539 ± 1.104 ms, Memory: 3108.92 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         81.431 ± 0.849            127.539 ± 1.104           2707.29                   3108.92                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 104/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 161.650 ± 0.487 ms, Memory: 5397.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 256.351 ± 3.319 ms, Memory: 6191.42 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         161.650 ± 0.487           256.351 ± 3.319           5397.29                   6191.42                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 105/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 334.533 ± 3.738 ms, Memory: 10777.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 542.358 ± 2.085 ms, Memory: 12356.42 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         334.533 ± 3.738           542.358 ± 2.085           10777.29                  12356.42                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 106/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 15.639 ± 0.178 ms, Memory: 529.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 21.267 ± 0.320 ms, Memory: 731.04 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         15.639 ± 0.178            21.267 ± 0.320            529.29                    731.04                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 107/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 31.863 ± 0.411 ms, Memory: 1041.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 40.885 ± 0.900 ms, Memory: 1435.67 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         31.863 ± 0.411            40.885 ± 0.900            1041.29                   1435.67                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 108/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 63.666 ± 0.571 ms, Memory: 2065.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 81.988 ± 1.125 ms, Memory: 2844.92 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         63.666 ± 0.571            81.988 ± 1.125            2065.29                   2844.92                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 109/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 121.316 ± 0.436 ms, Memory: 4113.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 164.255 ± 1.697 ms, Memory: 5663.42 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         121.316 ± 0.436           164.255 ± 1.697           4113.29                   5663.42                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 110/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 252.661 ± 2.892 ms, Memory: 8209.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 359.815 ± 1.261 ms, Memory: 11300.42 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         252.661 ± 2.892           359.815 ± 1.261           8209.29                   11300.42                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 111/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 31.479 ± 0.455 ms, Memory: 1137.82 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 49.233 ± 0.344 ms, Memory: 1221.32 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         31.479 ± 0.455            49.233 ± 0.344            1137.82                   1221.32                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 112/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 63.260 ± 1.172 ms, Memory: 2258.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 99.464 ± 0.384 ms, Memory: 2407.82 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         63.260 ± 1.172            99.464 ± 0.384            2258.32                   2407.82                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 113/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 128.465 ± 0.834 ms, Memory: 4499.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 200.282 ± 1.748 ms, Memory: 4780.82 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         128.465 ± 0.834           200.282 ± 1.748           4499.32                   4780.82                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 114/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 260.437 ± 2.917 ms, Memory: 8981.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 402.895 ± 1.847 ms, Memory: 9526.82 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         260.437 ± 2.917           402.895 ± 1.847           8981.32                   9526.82                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 115/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 308.12 MiB is free. Process 4036 has 14.44 GiB memory in use. Of the allocated memory 13.53 GiB is allocated by PyTorch, and 756.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 116/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 24.108 ± 0.409 ms, Memory: 849.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 31.063 ± 0.657 ms, Memory: 1155.32 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         24.108 ± 0.409            31.063 ± 0.657            849.32                    1155.32                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 117/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 49.176 ± 0.692 ms, Memory: 1681.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 60.864 ± 0.754 ms, Memory: 2275.82 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         49.176 ± 0.692            60.864 ± 0.754            1681.32                   2275.82                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 118/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 100.965 ± 0.402 ms, Memory: 3345.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 124.749 ± 0.530 ms, Memory: 4516.82 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         100.965 ± 0.402           124.749 ± 0.530           3345.32                   4516.82                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 119/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 202.081 ± 0.445 ms, Memory: 6673.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 251.837 ± 0.795 ms, Memory: 8998.82 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         202.081 ± 0.445           251.837 ± 0.795           6673.32                   8998.82                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 120/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 406.317 ± 1.163 ms, Memory: 13329.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 121/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 20.868 ± 0.336 ms, Memory: 690.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 32.255 ± 0.257 ms, Memory: 795.65 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         20.868 ± 0.336            32.255 ± 0.257            690.26                    795.65                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 122/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 41.982 ± 0.633 ms, Memory: 1363.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 63.544 ± 0.333 ms, Memory: 1568.90 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         41.982 ± 0.633            63.544 ± 0.333            1363.26                   1568.90                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 123/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 84.858 ± 0.785 ms, Memory: 2709.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 126.922 ± 0.607 ms, Memory: 3115.40 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         84.858 ± 0.785            126.922 ± 0.607           2709.26                   3115.40                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 124/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 168.417 ± 1.233 ms, Memory: 5401.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 254.947 ± 0.929 ms, Memory: 6208.40 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         168.417 ± 1.233           254.947 ± 0.929           5401.26                   6208.40                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 125/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 344.523 ± 1.244 ms, Memory: 10785.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 562.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 12.10 GiB is allocated by PyTorch, and 1.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 126/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 15.248 ± 0.180 ms, Memory: 529.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 20.192 ± 0.263 ms, Memory: 727.64 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         15.248 ± 0.180            20.192 ± 0.263            529.26                    727.64                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 127/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 30.674 ± 0.368 ms, Memory: 1041.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 38.892 ± 0.590 ms, Memory: 1432.89 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         30.674 ± 0.368            38.892 ± 0.590            1041.26                   1432.89                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 128/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 61.163 ± 0.782 ms, Memory: 2065.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 78.029 ± 0.915 ms, Memory: 2843.39 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         61.163 ± 0.782            78.029 ± 0.915            2065.26                   2843.39                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 129/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 121.057 ± 0.357 ms, Memory: 4113.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 155.958 ± 1.736 ms, Memory: 5664.39 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         121.057 ± 0.357           155.958 ± 1.736           4113.26                   5664.39                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 130/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 244.530 ± 2.074 ms, Memory: 8209.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 337.939 ± 1.770 ms, Memory: 11306.39 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         244.530 ± 2.074           337.939 ± 1.770           8209.26                   11306.39                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 131/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 31.135 ± 0.348 ms, Memory: 1138.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 48.757 ± 0.372 ms, Memory: 1215.78 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         31.135 ± 0.348            48.757 ± 0.372            1138.27                   1215.78                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 132/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 62.779 ± 0.973 ms, Memory: 2259.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 97.005 ± 0.693 ms, Memory: 2404.78 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         62.779 ± 0.973            97.005 ± 0.693            2259.27                   2404.78                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 133/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 127.711 ± 0.786 ms, Memory: 4501.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 196.245 ± 0.726 ms, Memory: 4782.78 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         127.711 ± 0.786           196.245 ± 0.726           4501.27                   4782.78                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 134/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 256.942 ± 2.401 ms, Memory: 8985.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 392.081 ± 0.856 ms, Memory: 9538.78 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         256.942 ± 2.401           392.081 ± 0.856           8985.27                   9538.78                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 135/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 136/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 22.676 ± 0.382 ms, Memory: 849.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 28.560 ± 0.444 ms, Memory: 1147.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         22.676 ± 0.382            28.560 ± 0.444            849.27                    1147.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 137/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 46.159 ± 0.506 ms, Memory: 1681.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 55.664 ± 0.449 ms, Memory: 2268.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         46.159 ± 0.506            55.664 ± 0.449            1681.27                   2268.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 138/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 92.729 ± 1.060 ms, Memory: 3345.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 114.115 ± 0.867 ms, Memory: 4510.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         92.729 ± 1.060            114.115 ± 0.867           3345.27                   4510.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 139/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 181.724 ± 0.349 ms, Memory: 6673.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 228.970 ± 0.875 ms, Memory: 8994.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         181.724 ± 0.349           228.970 ± 0.875           6673.27                   8994.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 140/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 381.081 ± 0.913 ms, Memory: 13329.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 141/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 52.220 ± 0.385 ms, Memory: 2034.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 82.766 ± 1.099 ms, Memory: 2120.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         52.220 ± 0.385            82.766 ± 1.099            2034.29                   2120.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 142/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 105.505 ± 0.935 ms, Memory: 4051.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 167.123 ± 1.291 ms, Memory: 4205.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         105.505 ± 0.935           167.123 ± 1.291           4051.29                   4205.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 143/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 216.469 ± 2.589 ms, Memory: 8085.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 336.088 ± 0.966 ms, Memory: 8375.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         216.469 ± 2.589           336.088 ± 0.966           8085.29                   8375.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 144/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.78 GiB is allocated by PyTorch, and 2.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 145/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 146/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 38.738 ± 0.439 ms, Memory: 1489.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 46.431 ± 0.476 ms, Memory: 2052.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         38.738 ± 0.439            46.431 ± 0.476            1489.29                   2052.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 147/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 78.886 ± 0.907 ms, Memory: 2961.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 93.625 ± 1.051 ms, Memory: 4069.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         78.886 ± 0.907            93.625 ± 1.051            2961.29                   4069.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 148/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 161.787 ± 0.450 ms, Memory: 5905.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 189.041 ± 0.790 ms, Memory: 8103.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         161.787 ± 0.450           189.041 ± 0.790           5905.29                   8103.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 149/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 322.640 ± 0.885 ms, Memory: 11793.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 308.12 MiB is free. Process 4036 has 14.44 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 244.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 150/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 3.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 151/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 33.016 ± 0.225 ms, Memory: 1139.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 50.065 ± 0.320 ms, Memory: 1216.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         33.016 ± 0.225            50.065 ± 0.320            1139.26                   1216.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 152/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 66.212 ± 0.667 ms, Memory: 2261.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 99.577 ± 0.348 ms, Memory: 2410.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         66.212 ± 0.667            99.577 ± 0.348            2261.26                   2410.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 153/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 134.097 ± 0.985 ms, Memory: 4505.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 198.819 ± 0.642 ms, Memory: 4798.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         134.097 ± 0.985           198.819 ± 0.642           4505.26                   4798.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 154/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 268.517 ± 1.149 ms, Memory: 8993.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 397.309 ± 1.250 ms, Memory: 9574.77 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         268.517 ± 1.149           397.309 ± 1.250           8993.26                   9574.77                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 155/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.58 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 156/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 23.023 ± 0.310 ms, Memory: 849.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 28.157 ± 0.299 ms, Memory: 1144.76 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         23.023 ± 0.310            28.157 ± 0.299            849.26                    1144.76                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 157/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 46.448 ± 0.612 ms, Memory: 1681.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 55.062 ± 0.274 ms, Memory: 2266.76 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         46.448 ± 0.612            55.062 ± 0.274            1681.26                   2266.76                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 158/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 93.739 ± 0.748 ms, Memory: 3345.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 110.169 ± 0.727 ms, Memory: 4510.76 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         93.739 ± 0.748            110.169 ± 0.727           3345.26                   4510.76                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 159/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 184.415 ± 1.872 ms, Memory: 6673.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 221.893 ± 0.738 ms, Memory: 8998.76 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         184.415 ± 1.872           221.893 ± 0.738           6673.26                   8998.76                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 160/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 384.196 ± 0.946 ms, Memory: 13329.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 161/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 52.975 ± 0.314 ms, Memory: 2035.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 82.103 ± 0.519 ms, Memory: 2117.27 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         52.975 ± 0.314            82.103 ± 0.519            2035.27                   2117.27                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 162/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 107.334 ± 0.640 ms, Memory: 4053.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 165.692 ± 1.112 ms, Memory: 4207.27 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         107.334 ± 0.640           165.692 ± 1.112           4053.27                   4207.27                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 163/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 219.421 ± 2.022 ms, Memory: 8089.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 332.416 ± 0.919 ms, Memory: 8387.27 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         219.421 ± 2.022           332.416 ± 0.919           8089.27                   8387.27                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 164/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.80 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 165/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.58 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 166/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 37.988 ± 0.441 ms, Memory: 1489.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 44.547 ± 0.276 ms, Memory: 2045.27 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         37.988 ± 0.441            44.547 ± 0.276            1489.26                   2045.27                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 167/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 76.670 ± 0.612 ms, Memory: 2961.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 89.852 ± 0.656 ms, Memory: 4063.27 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         76.670 ± 0.612            89.852 ± 0.656            2961.26                   4063.27                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 168/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 155.050 ± 1.654 ms, Memory: 5905.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 180.534 ± 0.826 ms, Memory: 8099.27 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         155.050 ± 1.654           180.534 ± 0.826           5905.26                   8099.27                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 169/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 315.062 ± 1.144 ms, Memory: 11793.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 308.12 MiB is free. Process 4036 has 14.44 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 252.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 170/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 3.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 171/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 94.073 ± 0.426 ms, Memory: 3827.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 149.471 ± 0.561 ms, Memory: 3918.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         94.073 ± 0.426            149.471 ± 0.561           3827.27                   3918.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 172/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 191.933 ± 1.275 ms, Memory: 7637.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 301.351 ± 1.487 ms, Memory: 7800.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         191.933 ± 1.275           301.351 ± 1.487           7637.27                   7800.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 173/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 12.90 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 174/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 9.80 GiB is allocated by PyTorch, and 4.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 175/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.58 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 176/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 68.658 ± 0.456 ms, Memory: 2769.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 79.257 ± 0.708 ms, Memory: 3846.27 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         68.658 ± 0.456            79.257 ± 0.708            2769.27                   3846.27                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 177/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 141.511 ± 0.348 ms, Memory: 5521.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 162.714 ± 0.977 ms, Memory: 7656.27 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         141.511 ± 0.348           162.714 ± 0.977           5521.27                   7656.27                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 178/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 286.982 ± 1.352 ms, Memory: 11025.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 562.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 13.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 179/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 13.52 GiB is allocated by PyTorch, and 516.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 180/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 3.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 181/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 31.997 ± 0.335 ms, Memory: 692.79 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 49.803 ± 0.436 ms, Memory: 1090.99 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         31.997 ± 0.335            49.803 ± 0.436            692.79                    1090.99                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 182/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 63.942 ± 0.513 ms, Memory: 1365.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 97.746 ± 0.593 ms, Memory: 2149.61 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         63.942 ± 0.513            97.746 ± 0.593            1365.29                   2149.61                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 183/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 127.788 ± 0.819 ms, Memory: 2710.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 203.554 ± 1.342 ms, Memory: 4266.86 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         127.788 ± 0.819           203.554 ± 1.342           2710.29                   4266.86                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 184/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 255.944 ± 1.355 ms, Memory: 5400.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 418.085 ± 2.333 ms, Memory: 8501.36 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         255.944 ± 1.355           418.085 ± 2.333           5400.29                   8501.36                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 185/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 512.992 ± 2.764 ms, Memory: 10780.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 12.06 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 186/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 27.337 ± 0.376 ms, Memory: 596.79 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 39.673 ± 0.658 ms, Memory: 960.99 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         27.337 ± 0.376            39.673 ± 0.658            596.79                    960.99                   \n",
      "\n",
      "\n",
      "==================== COMBINATION 187/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 55.321 ± 0.586 ms, Memory: 1173.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 79.494 ± 1.136 ms, Memory: 1889.61 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         55.321 ± 0.586            79.494 ± 1.136            1173.29                   1889.61                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 188/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 110.916 ± 0.861 ms, Memory: 2326.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 163.697 ± 1.867 ms, Memory: 3746.86 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         110.916 ± 0.861           163.697 ± 1.867           2326.29                   3746.86                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 189/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 221.287 ± 0.708 ms, Memory: 4632.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 332.246 ± 2.162 ms, Memory: 7461.36 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         221.287 ± 0.708           332.246 ± 2.162           4632.29                   7461.36                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 190/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 443.034 ± 1.058 ms, Memory: 9244.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 12.54 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 191/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 37.916 ± 0.533 ms, Memory: 916.82 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 59.611 ± 0.703 ms, Memory: 1259.08 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         37.916 ± 0.533            59.611 ± 0.703            916.82                    1259.08                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 192/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 76.319 ± 0.676 ms, Memory: 1813.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 118.606 ± 0.888 ms, Memory: 2477.71 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         76.319 ± 0.676            118.606 ± 0.888           1813.32                   2477.71                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 193/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 153.479 ± 0.787 ms, Memory: 3606.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 245.730 ± 1.550 ms, Memory: 4914.96 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         153.479 ± 0.787           245.730 ± 1.550           3606.32                   4914.96                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 194/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 310.536 ± 1.344 ms, Memory: 7192.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 496.980 ± 1.791 ms, Memory: 9789.46 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         310.536 ± 1.344           496.980 ± 1.791           7192.32                   9789.46                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 195/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 13.03 GiB is allocated by PyTorch, and 1021.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 196/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 32.365 ± 0.414 ms, Memory: 724.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 46.170 ± 0.692 ms, Memory: 1129.08 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         32.365 ± 0.414            46.170 ± 0.692            724.32                    1129.08                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 197/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 66.042 ± 0.810 ms, Memory: 1428.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 93.166 ± 0.867 ms, Memory: 2217.70 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         66.042 ± 0.810            93.166 ± 0.867            1428.32                   2217.70                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 198/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 132.078 ± 1.041 ms, Memory: 2836.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 189.936 ± 0.901 ms, Memory: 4394.95 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         132.078 ± 1.041           189.936 ± 0.901           2836.32                   4394.95                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 199/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 265.104 ± 1.742 ms, Memory: 5652.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 386.813 ± 2.243 ms, Memory: 8749.45 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         265.104 ± 1.742           386.813 ± 2.243           5652.32                   8749.45                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 200/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 530.593 ± 3.266 ms, Memory: 11284.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 13.04 GiB is allocated by PyTorch, and 1013.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 201/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 50.695 ± 0.561 ms, Memory: 1364.88 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 80.367 ± 0.951 ms, Memory: 1595.27 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         50.695 ± 0.561            80.367 ± 0.951            1364.88                   1595.27                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 202/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 100.875 ± 0.472 ms, Memory: 2709.38 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 161.187 ± 0.603 ms, Memory: 3133.89 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         100.875 ± 0.472           161.187 ± 0.603           2709.38                   3133.89                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 203/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 207.508 ± 2.101 ms, Memory: 5398.38 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 330.998 ± 1.516 ms, Memory: 6211.14 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         207.508 ± 2.101           330.998 ± 1.516           5398.38                   6211.14                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 204/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 415.205 ± 1.353 ms, Memory: 10776.38 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 556.12 MiB is free. Process 4036 has 14.20 GiB memory in use. Of the allocated memory 12.04 GiB is allocated by PyTorch, and 1.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 205/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.04 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 206/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 42.719 ± 0.887 ms, Memory: 1044.38 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 58.341 ± 0.298 ms, Memory: 1465.27 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         42.719 ± 0.887            58.341 ± 0.298            1044.38                   1465.27                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 207/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 86.962 ± 0.743 ms, Memory: 2068.38 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 120.406 ± 1.207 ms, Memory: 2873.89 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         86.962 ± 0.743            120.406 ± 1.207           2068.38                   2873.89                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 208/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 177.583 ± 1.228 ms, Memory: 4116.38 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 246.095 ± 1.698 ms, Memory: 5691.14 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         177.583 ± 1.228           246.095 ± 1.698           4116.38                   5691.14                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 209/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 352.044 ± 1.847 ms, Memory: 8212.38 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 493.543 ± 1.828 ms, Memory: 11325.64 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         352.044 ± 1.847           493.543 ± 1.828           8212.38                   11325.64                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 210/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 5.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 211/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 37.342 ± 0.472 ms, Memory: 917.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 57.619 ± 0.522 ms, Memory: 1253.66 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         37.342 ± 0.472            57.619 ± 0.522            917.28                    1253.66                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 212/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 75.277 ± 0.495 ms, Memory: 1814.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 114.169 ± 1.256 ms, Memory: 2474.91 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         75.277 ± 0.495            114.169 ± 1.256           1814.28                   2474.91                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 213/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 151.110 ± 0.718 ms, Memory: 3608.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 237.946 ± 2.425 ms, Memory: 4917.41 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         151.110 ± 0.718           237.946 ± 2.425           3608.28                   4917.41                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 214/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 305.088 ± 1.336 ms, Memory: 7196.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 484.095 ± 2.889 ms, Memory: 9802.41 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         305.088 ± 1.336           484.095 ± 2.889           7196.28                   9802.41                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 215/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 13.04 GiB is allocated by PyTorch, and 1013.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 216/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 30.577 ± 0.377 ms, Memory: 724.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 42.969 ± 0.615 ms, Memory: 1121.66 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         30.577 ± 0.377            42.969 ± 0.615            724.27                    1121.66                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 217/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 61.693 ± 0.711 ms, Memory: 1428.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 86.336 ± 0.952 ms, Memory: 2210.91 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         61.693 ± 0.711            86.336 ± 0.952            1428.27                   2210.91                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 218/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 123.736 ± 1.195 ms, Memory: 2836.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 177.999 ± 0.791 ms, Memory: 4389.41 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         123.736 ± 1.195           177.999 ± 0.791           2836.27                   4389.41                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 219/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 249.542 ± 1.832 ms, Memory: 5652.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 360.505 ± 1.734 ms, Memory: 8746.41 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         249.542 ± 1.832           360.505 ± 1.734           5652.27                   8746.41                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 220/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 500.439 ± 2.242 ms, Memory: 11284.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 13.03 GiB is allocated by PyTorch, and 1021.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 221/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 48.065 ± 0.430 ms, Memory: 1365.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 74.432 ± 0.438 ms, Memory: 1581.80 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         48.065 ± 0.430            74.432 ± 0.438            1365.29                   1581.80                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 222/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 97.162 ± 0.889 ms, Memory: 2710.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 150.651 ± 1.256 ms, Memory: 3123.05 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         97.162 ± 0.889            150.651 ± 1.256           2710.29                   3123.05                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 223/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 197.650 ± 1.487 ms, Memory: 5400.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 307.410 ± 1.443 ms, Memory: 6205.55 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         197.650 ± 1.487           307.410 ± 1.443           5400.29                   6205.55                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 224/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 397.457 ± 2.027 ms, Memory: 10780.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 556.12 MiB is free. Process 4036 has 14.20 GiB memory in use. Of the allocated memory 12.06 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 225/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.05 GiB is allocated by PyTorch, and 2.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 226/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 39.263 ± 0.702 ms, Memory: 1044.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 52.472 ± 0.616 ms, Memory: 1449.80 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         39.263 ± 0.702            52.472 ± 0.616            1044.29                   1449.80                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 227/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 78.882 ± 0.907 ms, Memory: 2068.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 105.991 ± 1.610 ms, Memory: 2859.05 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         78.882 ± 0.907            105.991 ± 1.610           2068.29                   2859.05                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 228/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 161.456 ± 0.647 ms, Memory: 4116.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 219.083 ± 1.631 ms, Memory: 5677.55 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         161.456 ± 0.647           219.083 ± 1.631           4116.29                   5677.55                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 229/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 322.690 ± 1.318 ms, Memory: 8212.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 442.572 ± 2.818 ms, Memory: 11314.55 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         322.690 ± 1.318           442.572 ± 2.818           8212.29                   11314.55                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 230/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 5.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 231/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 70.509 ± 0.447 ms, Memory: 2261.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 111.785 ± 0.832 ms, Memory: 2430.33 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         70.509 ± 0.447            111.785 ± 0.832           2261.32                   2430.33                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 232/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 144.162 ± 0.838 ms, Memory: 4502.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 222.239 ± 1.057 ms, Memory: 4803.33 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         144.162 ± 0.838           222.239 ± 1.057           4502.32                   4803.33                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 233/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 292.388 ± 2.347 ms, Memory: 8984.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 458.279 ± 1.428 ms, Memory: 9549.33 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         292.388 ± 2.347           458.279 ± 1.428           8984.32                   9549.33                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 234/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.54 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 235/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.05 GiB is allocated by PyTorch, and 2.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 236/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 56.263 ± 0.694 ms, Memory: 1684.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 73.081 ± 1.091 ms, Memory: 2298.33 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         56.263 ± 0.694            73.081 ± 1.091            1684.32                   2298.33                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 237/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 114.850 ± 1.304 ms, Memory: 3348.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 148.869 ± 1.218 ms, Memory: 4539.33 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         114.850 ± 1.304           148.869 ± 1.218           3348.32                   4539.33                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 238/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 232.328 ± 2.493 ms, Memory: 6676.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 304.420 ± 2.197 ms, Memory: 9021.33 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         232.328 ± 2.493           304.420 ± 2.197           6676.32                   9021.33                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 239/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 469.457 ± 1.145 ms, Memory: 13332.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 240/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 5.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 241/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 49.756 ± 0.274 ms, Memory: 1366.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 74.961 ± 0.637 ms, Memory: 1579.03 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         49.756 ± 0.274            74.961 ± 0.637            1366.27                   1579.03                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 242/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 100.975 ± 0.623 ms, Memory: 2712.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 149.716 ± 1.122 ms, Memory: 3125.53 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         100.975 ± 0.623           149.716 ± 1.122           2712.27                   3125.53                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 243/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 201.997 ± 0.411 ms, Memory: 5404.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 312.948 ± 1.942 ms, Memory: 6218.53 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         201.997 ± 0.411           312.948 ± 1.942           5404.27                   6218.53                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 244/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 409.538 ± 3.622 ms, Memory: 10788.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 556.12 MiB is free. Process 4036 has 14.20 GiB memory in use. Of the allocated memory 12.10 GiB is allocated by PyTorch, and 1.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 245/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.08 GiB is allocated by PyTorch, and 2.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 246/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 38.203 ± 0.565 ms, Memory: 1044.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 50.562 ± 0.477 ms, Memory: 1443.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         38.203 ± 0.565            50.562 ± 0.477            1044.27                   1443.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 247/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 76.471 ± 0.583 ms, Memory: 2068.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 100.871 ± 0.764 ms, Memory: 2853.52 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         76.471 ± 0.583            100.871 ± 0.764           2068.27                   2853.52                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 248/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 153.206 ± 1.158 ms, Memory: 4116.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 207.297 ± 1.068 ms, Memory: 5674.52 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         153.206 ± 1.158           207.297 ± 1.068           4116.27                   5674.52                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 249/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 311.066 ± 2.885 ms, Memory: 8212.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 425.530 ± 1.566 ms, Memory: 11316.52 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         311.066 ± 2.885           425.530 ± 1.566           8212.27                   11316.52                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 250/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 5.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 251/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 70.618 ± 0.500 ms, Memory: 2262.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 108.437 ± 0.991 ms, Memory: 2419.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         70.618 ± 0.500            108.437 ± 0.991           2262.28                   2419.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 252/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 142.400 ± 0.887 ms, Memory: 4504.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 217.417 ± 1.652 ms, Memory: 4797.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         142.400 ± 0.887           217.417 ± 1.652           4504.28                   4797.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 253/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 289.986 ± 1.920 ms, Memory: 8988.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 448.466 ± 1.943 ms, Memory: 9553.29 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         289.986 ± 1.920           448.466 ± 1.943           8988.28                   9553.29                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 254/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 255/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.08 GiB is allocated by PyTorch, and 2.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 256/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 53.648 ± 0.477 ms, Memory: 1684.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 67.877 ± 0.441 ms, Memory: 2283.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         53.648 ± 0.477            67.877 ± 0.441            1684.27                   2283.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 257/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 108.156 ± 0.940 ms, Memory: 3348.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 137.359 ± 0.502 ms, Memory: 4525.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         108.156 ± 0.940           137.359 ± 0.502           3348.27                   4525.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 258/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 222.096 ± 0.446 ms, Memory: 6676.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 286.325 ± 0.551 ms, Memory: 9009.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         222.096 ± 0.446           286.325 ± 0.551           6676.27                   9009.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 259/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 444.793 ± 0.843 ms, Memory: 13332.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 260/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 5.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 261/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 112.777 ± 0.843 ms, Memory: 4054.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 178.455 ± 1.223 ms, Memory: 4228.31 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         112.777 ± 0.843           178.455 ± 1.223           4054.29                   4228.31                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 262/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 232.321 ± 1.359 ms, Memory: 8088.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 358.939 ± 0.816 ms, Memory: 8398.31 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         232.321 ± 1.359           358.939 ± 0.816           8088.29                   8398.31                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 263/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.79 GiB is allocated by PyTorch, and 2.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 264/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 265/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 7.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 266/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 85.522 ± 0.714 ms, Memory: 2964.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 105.676 ± 1.113 ms, Memory: 4092.30 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         85.522 ± 0.714            105.676 ± 1.113           2964.29                   4092.30                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 267/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 176.345 ± 2.510 ms, Memory: 5908.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 214.213 ± 1.073 ms, Memory: 8126.30 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         176.345 ± 2.510           214.213 ± 1.073           5908.29                   8126.30                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 268/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 357.253 ± 0.963 ms, Memory: 11796.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 304.12 MiB is free. Process 4036 has 14.44 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 228.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 269/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 3.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 270/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 5.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 271/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 91.181 ± 0.519 ms, Memory: 1152.83 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 137.008 ± 0.921 ms, Memory: 2019.03 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         91.181 ± 0.519            137.008 ± 0.921           1152.83                   2019.03                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 272/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 181.141 ± 0.574 ms, Memory: 2273.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 275.662 ± 1.325 ms, Memory: 3973.66 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         181.141 ± 0.574           275.662 ± 1.325           2273.33                   3973.66                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 273/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 361.173 ± 1.157 ms, Memory: 4514.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 545.863 ± 1.721 ms, Memory: 7882.91 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         361.173 ± 1.157           545.863 ± 1.721           4514.33                   7882.91                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 274/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 723.206 ± 2.189 ms, Memory: 8996.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 12.32 GiB is allocated by PyTorch, and 1.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 275/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 4036 has 13.19 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 276/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 84.979 ± 0.681 ms, Memory: 1120.83 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 122.073 ± 1.010 ms, Memory: 1761.03 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         84.979 ± 0.681            122.073 ± 1.010           1120.83                   1761.03                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 277/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 171.054 ± 0.983 ms, Memory: 2209.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 247.858 ± 1.377 ms, Memory: 3457.66 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         171.054 ± 0.983           247.858 ± 1.377           2209.33                   3457.66                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 278/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 342.068 ± 0.855 ms, Memory: 4386.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 485.014 ± 1.416 ms, Memory: 6850.91 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         342.068 ± 0.855           485.014 ± 1.416           4386.33                   6850.91                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 279/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 678.986 ± 2.949 ms, Memory: 8740.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 4036 has 14.19 GiB memory in use. Of the allocated memory 12.32 GiB is allocated by PyTorch, and 1.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 280/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 281/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 98.039 ± 0.607 ms, Memory: 1376.89 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 149.634 ± 0.539 ms, Memory: 2195.16 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         98.039 ± 0.607            149.634 ± 0.539           1376.89                   2195.16                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 282/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 197.620 ± 1.631 ms, Memory: 2721.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 303.064 ± 1.085 ms, Memory: 4309.78 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         197.620 ± 1.631           303.064 ± 1.085           2721.39                   4309.78                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 283/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 393.010 ± 1.170 ms, Memory: 5410.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 599.227 ± 2.337 ms, Memory: 8539.03 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         393.010 ± 1.170           599.227 ± 2.337           5410.39                   8539.03                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 284/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 789.340 ± 1.823 ms, Memory: 10788.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 13.08 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 285/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 286/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 91.869 ± 0.805 ms, Memory: 1184.89 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 133.055 ± 0.733 ms, Memory: 1937.16 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         91.869 ± 0.805            133.055 ± 0.733           1184.89                   1937.16                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 287/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 185.119 ± 1.607 ms, Memory: 2337.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 268.474 ± 1.081 ms, Memory: 3793.78 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         185.119 ± 1.607           268.474 ± 1.081           2337.39                   3793.78                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 288/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 366.977 ± 2.503 ms, Memory: 4642.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 524.503 ± 2.024 ms, Memory: 7507.03 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         366.977 ± 2.503           524.503 ± 2.024           4642.39                   7507.03                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 289/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 737.842 ± 1.514 ms, Memory: 9252.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 13.58 GiB is allocated by PyTorch, and 960.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 290/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 291/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 114.500 ± 0.749 ms, Memory: 1825.02 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 177.437 ± 0.572 ms, Memory: 2547.41 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         114.500 ± 0.749           177.437 ± 0.572           1825.02                   2547.41                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 292/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 231.626 ± 1.126 ms, Memory: 3617.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 357.620 ± 1.547 ms, Memory: 4982.03 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         231.626 ± 1.126           357.620 ± 1.547           3617.52                   4982.03                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 293/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 463.645 ± 1.180 ms, Memory: 7202.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 710.550 ± 2.491 ms, Memory: 9851.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         463.645 ± 1.180           710.550 ± 2.491           7202.52                   9851.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 294/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 925.413 ± 2.922 ms, Memory: 14372.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 481.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 295/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 296/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 106.908 ± 0.994 ms, Memory: 1440.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 153.604 ± 1.323 ms, Memory: 2289.41 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         106.908 ± 0.994           153.604 ± 1.323           1440.52                   2289.41                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 297/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 215.071 ± 1.870 ms, Memory: 2848.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 306.617 ± 2.014 ms, Memory: 4466.03 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         215.071 ± 1.870           306.617 ± 2.014           2848.52                   4466.03                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 298/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 427.175 ± 1.832 ms, Memory: 5664.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 603.619 ± 1.996 ms, Memory: 8819.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         427.175 ± 1.832           603.619 ± 1.996           5664.52                   8819.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 299/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 852.818 ± 2.262 ms, Memory: 11296.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 437.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 300/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 301/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 95.423 ± 0.653 ms, Memory: 1377.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 145.028 ± 0.636 ms, Memory: 2181.69 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         95.423 ± 0.653            145.028 ± 0.636           1377.30                   2181.69                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 302/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 192.775 ± 1.630 ms, Memory: 2722.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 292.939 ± 0.804 ms, Memory: 4298.94 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         192.775 ± 1.630           292.939 ± 0.804           2722.30                   4298.94                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 303/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 383.435 ± 1.573 ms, Memory: 5412.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 580.378 ± 1.846 ms, Memory: 8533.44 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         383.435 ± 1.573           580.378 ± 1.846           5412.30                   8533.44                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 304/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 767.882 ± 3.186 ms, Memory: 10792.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 13.09 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 305/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 306/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 87.459 ± 0.681 ms, Memory: 1185.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 125.178 ± 1.080 ms, Memory: 1921.69 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         87.459 ± 0.681            125.178 ± 1.080           1185.30                   1921.69                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 307/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 175.459 ± 1.649 ms, Memory: 2338.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 253.229 ± 1.376 ms, Memory: 3778.94 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         175.459 ± 1.649           253.229 ± 1.376           2338.30                   3778.94                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 308/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 350.340 ± 2.599 ms, Memory: 4644.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 497.082 ± 1.625 ms, Memory: 7493.44 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         350.340 ± 2.599           497.082 ± 1.625           4644.30                   7493.44                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 309/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 702.931 ± 2.598 ms, Memory: 9256.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 13.57 GiB is allocated by PyTorch, and 971.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 310/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 311/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 109.484 ± 0.751 ms, Memory: 1825.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 165.714 ± 1.155 ms, Memory: 2517.85 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         109.484 ± 0.751           165.714 ± 1.155           1825.33                   2517.85                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 312/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 220.040 ± 1.175 ms, Memory: 3618.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 334.024 ± 1.756 ms, Memory: 4955.10 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         220.040 ± 1.175           334.024 ± 1.756           3618.33                   4955.10                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 313/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 439.687 ± 2.112 ms, Memory: 7204.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 663.433 ± 1.829 ms, Memory: 9829.60 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         439.687 ± 2.112           663.433 ± 1.829           7204.33                   9829.60                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 314/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 881.257 ± 4.437 ms, Memory: 14376.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 461.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 315/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 316/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 99.262 ± 1.298 ms, Memory: 1440.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 139.279 ± 1.345 ms, Memory: 2257.84 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         99.262 ± 1.298            139.279 ± 1.345           1440.33                   2257.84                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 317/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 196.698 ± 1.630 ms, Memory: 2848.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 279.631 ± 1.125 ms, Memory: 4435.09 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         196.698 ± 1.630           279.631 ± 1.125           2848.33                   4435.09                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 318/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 394.065 ± 1.840 ms, Memory: 5664.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 549.388 ± 1.972 ms, Memory: 8789.59 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         394.065 ± 1.840           549.388 ± 1.972           5664.33                   8789.59                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 319/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 786.083 ± 1.639 ms, Memory: 11296.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 469.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 320/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 321/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 135.641 ± 0.897 ms, Memory: 2721.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 208.179 ± 0.812 ms, Memory: 3190.16 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         135.641 ± 0.897           208.179 ± 0.812           2721.39                   3190.16                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 322/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 274.844 ± 1.822 ms, Memory: 5410.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 417.052 ± 2.292 ms, Memory: 6267.41 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         274.844 ± 1.822           417.052 ± 2.292           5410.39                   6267.41                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 323/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 546.057 ± 1.693 ms, Memory: 10788.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 832.471 ± 2.799 ms, Memory: 12421.91 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         546.057 ± 1.693           832.471 ± 2.799           10788.39                  12421.91                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 324/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 13.05 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 325/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 326/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 120.733 ± 0.604 ms, Memory: 2080.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 166.328 ± 0.825 ms, Memory: 2930.16 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         120.733 ± 0.604           166.328 ± 0.825           2080.39                   2930.16                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 327/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 241.876 ± 0.721 ms, Memory: 4128.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 332.633 ± 2.949 ms, Memory: 5747.41 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         241.876 ± 0.721           332.633 ± 2.949           4128.39                   5747.41                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 328/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 482.196 ± 1.809 ms, Memory: 8224.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 658.589 ± 2.269 ms, Memory: 11381.91 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         482.196 ± 1.809           658.589 ± 2.269           8224.39                   11381.91                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 329/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 330/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 331/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 107.482 ± 0.702 ms, Memory: 1826.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 160.738 ± 1.631 ms, Memory: 2507.06 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         107.482 ± 0.702           160.738 ± 1.631           1826.29                   2507.06                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 332/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 219.647 ± 2.041 ms, Memory: 3620.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 328.200 ± 1.797 ms, Memory: 4949.56 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         219.647 ± 2.041           328.200 ± 1.797           3620.29                   4949.56                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 333/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 435.410 ± 2.042 ms, Memory: 7208.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 649.283 ± 3.020 ms, Memory: 9834.56 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         435.410 ± 2.042           649.283 ± 3.020           7208.29                   9834.56                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 334/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 875.697 ± 3.297 ms, Memory: 14384.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 421.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 335/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 336/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 94.488 ± 0.942 ms, Memory: 1440.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 132.093 ± 1.101 ms, Memory: 2243.05 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         94.488 ± 0.942            132.093 ± 1.101           1440.28                   2243.05                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 337/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 188.306 ± 1.091 ms, Memory: 2848.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 268.391 ± 1.157 ms, Memory: 4421.55 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         188.306 ± 1.091           268.391 ± 1.157           2848.28                   4421.55                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 338/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 377.942 ± 2.320 ms, Memory: 5664.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 527.431 ± 2.153 ms, Memory: 8778.55 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         377.942 ± 2.320           527.431 ± 2.153           5664.28                   8778.55                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 339/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 754.546 ± 3.149 ms, Memory: 11296.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 485.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 340/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 341/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 130.839 ± 0.980 ms, Memory: 2722.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 197.013 ± 1.325 ms, Memory: 3163.32 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         130.839 ± 0.980           197.013 ± 1.325           2722.30                   3163.32                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 342/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 261.796 ± 0.956 ms, Memory: 5412.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 400.054 ± 2.593 ms, Memory: 6245.82 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         261.796 ± 0.956           400.054 ± 2.593           5412.30                   6245.82                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 343/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 526.200 ± 3.090 ms, Memory: 10792.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 790.888 ± 1.684 ms, Memory: 12410.82 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         526.200 ± 3.090           790.888 ± 1.684           10792.30                  12410.82                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 344/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 13.06 GiB is allocated by PyTorch, and 1.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 345/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 346/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 111.087 ± 0.874 ms, Memory: 2080.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 153.552 ± 1.424 ms, Memory: 2899.31 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         111.087 ± 0.874           153.552 ± 1.424           2080.30                   2899.31                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 347/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 221.504 ± 0.908 ms, Memory: 4128.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 308.406 ± 1.839 ms, Memory: 5717.81 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         221.504 ± 0.908           308.406 ± 1.839           4128.30                   5717.81                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 348/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 443.246 ± 1.170 ms, Memory: 8224.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 610.993 ± 3.305 ms, Memory: 11354.81 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         443.246 ± 1.170           610.993 ± 3.305           8224.30                   11354.81                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 349/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 350/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 351/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 176.841 ± 1.185 ms, Memory: 4514.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 270.108 ± 1.414 ms, Memory: 4860.35 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         176.841 ± 1.185           270.108 ± 1.414           4514.33                   4860.35                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 352/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 360.477 ± 3.347 ms, Memory: 8996.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 543.060 ± 2.353 ms, Memory: 9606.35 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         360.477 ± 3.347           543.060 ± 2.353           8996.33                   9606.35                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 353/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 13.55 GiB is allocated by PyTorch, and 997.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 354/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 11.06 GiB is allocated by PyTorch, and 3.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 355/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 356/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 147.459 ± 1.583 ms, Memory: 3360.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 196.407 ± 2.031 ms, Memory: 4596.34 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         147.459 ± 1.583           196.407 ± 2.031           3360.33                   4596.34                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 357/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 302.621 ± 0.711 ms, Memory: 6688.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention backward: 393.537 ± 1.325 ms, Memory: 9078.34 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "RandMultiHeadAttention         302.621 ± 0.711           393.537 ± 1.325           6688.33                   9078.34                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 358/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "RandMultiHeadAttention forward: 598.186 ± 1.281 ms, Memory: 13344.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 13.03 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 359/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 360/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 4036 has 14.69 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "All benchmark results saved to attention_benchmark_results.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch._dynamo\n",
    "import torch._inductor.config as config\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# Configure torch\n",
    "config.max_autotune_gemm = False\n",
    "torch._dynamo.config.cache_size_limit = 2**16\n",
    "torch._dynamo.config.accumulated_cache_size_limit = 2**16\n",
    "\n",
    "\n",
    "def is_valid_params(embed_dim, num_heads, num_random_features):\n",
    "    """\n",
    "    Check if parameter combination is valid:\n",
    "    embed_dim must be divisible by num_heads\n",
    "    """\n",
    "    return embed_dim % num_heads == 0\n",
    "\n",
    "\n",
    "class BenchmarkParams:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim=256,\n",
    "        num_heads=8,\n",
    "        num_random_features=128,\n",
    "        batch_size=64,\n",
    "        seq_length=32,\n",
    "        num_runs=200,\n",
    "        warmup=15,\n",
    "        device=torch.device(\"cuda\"),\n",
    "        dtype=torch.float32,\n",
    "    ):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_random_features = num_random_features\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.num_runs = num_runs\n",
    "        self.warmup = warmup\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "\n",
    "def benchmark_model(model, inputs, model_name, params):\n",
    "    """\n",
    "    Generic benchmarking function for any PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to benchmark\n",
    "        inputs: Dictionary of input tensors\n",
    "        model_name: Name of the model for logging\n",
    "        params: Benchmark parameters\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with benchmark results\n",
    "    """\n",
    "    # Compile the model\n",
    "    # model_compiled = torch.compile(\n",
    "    #     model,\n",
    "    #     backend=\"inductor\",\n",
    "    #     fullgraph=True,\n",
    "    #     dynamic=False\n",
    "    # )\n",
    "    model_compiled = model\n",
    "\n",
    "    # Benchmark forward pass\n",
    "    print(f\"\\n=== {model_name} FORWARD PASS BENCHMARK ===\")\n",
    "\n",
    "    # Warmup runs for forward pass\n",
    "    model_compiled.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(params.warmup):\n",
    "            _ = model_compiled(**inputs)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Actual timed runs for forward\n",
    "    forward_times = []\n",
    "    forward_memories = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(params.num_runs):\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            _ = model_compiled(**inputs)\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "\n",
    "            forward_times.append((end - start) * 1000)  # Convert to ms\n",
    "            forward_memories.append(\n",
    "                torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
    "            )  # Convert to MB\n",
    "\n",
    "    mean_forward = np.mean(forward_times)\n",
    "    std_forward = np.std(forward_times)\n",
    "    mean_forward_memory = np.mean(forward_memories)\n",
    "    std_forward_memory = np.std(forward_memories)\n",
    "    print(\n",
    "        f\"{model_name} forward: {mean_forward:.3f} ± {std_forward:.3f} ms, Memory: {mean_forward_memory:.2f} ± {std_forward_memory:.2f} MB\"\n",
    "    )\n",
    "\n",
    "    # Benchmark backward pass\n",
    "    print(f\"\\n=== {model_name} BACKWARD PASS BENCHMARK ===\")\n",
    "\n",
    "    # Get query for backward\n",
    "    query = inputs[\"query\"]\n",
    "\n",
    "    # Warmup runs for backward pass\n",
    "    model_compiled.train()\n",
    "    for _ in range(params.warmup):\n",
    "        out = model_compiled(**inputs)[0]\n",
    "        loss = out.sum()\n",
    "        loss.backward()\n",
    "        query.grad.zero_()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Actual timed runs for backward\n",
    "    backward_times = []\n",
    "    backward_memories = []\n",
    "    for _ in range(params.num_runs):\n",
    "        out = model_compiled(**inputs)[0]\n",
    "        loss = out.sum()\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        loss.backward()\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        backward_times.append((end - start) * 1000)  # Convert to ms\n",
    "        backward_memories.append(\n",
    "            torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
    "        )  # Convert to MB\n",
    "        query.grad.zero_()\n",
    "\n",
    "    mean_backward = np.mean(backward_times)\n",
    "    std_backward = np.std(backward_times)\n",
    "    mean_backward_memory = np.mean(backward_memories)\n",
    "    std_backward_memory = np.std(backward_memories)\n",
    "    print(\n",
    "        f\"{model_name} backward: {mean_backward:.3f} ± {std_backward:.3f} ms, Memory: {mean_backward_memory:.2f} ± {std_backward_memory:.2f} MB\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"forward\": {\n",
    "            \"mean\": mean_forward,\n",
    "            \"std\": std_forward,\n",
    "            \"times\": forward_times,\n",
    "            \"memory_mb\": mean_forward_memory,\n",
    "            \"memory_std\": std_forward_memory,\n",
    "            \"memories\": forward_memories,\n",
    "        },\n",
    "        \"backward\": {\n",
    "            \"mean\": mean_backward,\n",
    "            \"std\": std_backward,\n",
    "            \"times\": backward_times,\n",
    "            \"memory_mb\": mean_backward_memory,\n",
    "            \"memory_std\": std_backward_memory,\n",
    "            \"memories\": backward_memories,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def benchmark_model_factory(model_factory, model_name, params):\n",
    "    """\n",
    "    Benchmark a model using a factory function.\n",
    "\n",
    "    Args:\n",
    "        model_factory: Function that creates the model\n",
    "        model_name: Name of the model for logging\n",
    "        params: Benchmark parameters\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with benchmark results\n",
    "    """\n",
    "    # Create the model\n",
    "    torch.manual_seed(42)\n",
    "    model = model_factory(params)\n",
    "\n",
    "    # Create input tensors for benchmarking\n",
    "    query = torch.randn(\n",
    "        params.batch_size,\n",
    "        params.seq_length,\n",
    "        params.embed_dim,\n",
    "        dtype=params.dtype,\n",
    "        device=params.device,\n",
    "        requires_grad=True,\n",
    "    )\n",
    "    key = torch.randn(\n",
    "        params.batch_size,\n",
    "        params.seq_length,\n",
    "        params.embed_dim,\n",
    "        dtype=params.dtype,\n",
    "        device=params.device,\n",
    "    )\n",
    "    value = torch.randn(\n",
    "        params.batch_size,\n",
    "        params.seq_length,\n",
    "        params.embed_dim,\n",
    "        dtype=params.dtype,\n",
    "        device=params.device,\n",
    "    )\n",
    "\n",
    "    inputs = {\"query\": query, \"key\": key, \"value\": value, \"attention_mask\": None}\n",
    "\n",
    "    return benchmark_model(model, inputs, model_name, params)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import torch.nn as nn\n",
    "    from panther.nn.attention import RandMultiHeadAttention\n",
    "\n",
    "    # Parameter combinations to test\n",
    "    embed_dims = [128, 256, 512, 1024]\n",
    "    num_heads_options = [4, 8, 16]\n",
    "    num_random_features_options = [64, 128, 256]\n",
    "    kernel_fn_options = [\"softmax\", \"relu\"]\n",
    "    causal_options = [False]\n",
    "    # causal_options = [False, True]\n",
    "    seq_lens = [512, 1024, 2048, 4096, 8192]\n",
    "\n",
    "    # Define model factories\n",
    "    def create_attention(p):\n",
    "        return RandMultiHeadAttention(\n",
    "            embed_dim=p.embed_dim,\n",
    "            num_heads=p.num_heads,\n",
    "            num_random_features=p.num_random_features,\n",
    "            dropout=0.0,\n",
    "            kernel_fn=p.kernel_fn if hasattr(p, \"kernel_fn\") else \"softmax\",\n",
    "            iscausal=p.iscausal if hasattr(p, \"iscausal\") else False,\n",
    "            device=p.device,\n",
    "            dtype=p.dtype,\n",
    "        )\n",
    "\n",
    "    models_to_benchmark = [(create_attention, \"RandMultiHeadAttention\")]\n",
    "\n",
    "    # Prepare data structure to store all results\n",
    "    results_data = []\n",
    "\n",
    "    # Iterate through all parameter combinations\n",
    "    total_combinations = (\n",
    "        len(embed_dims)\n",
    "        * len(num_heads_options)\n",
    "        * len(num_random_features_options)\n",
    "        * len(kernel_fn_options)\n",
    "        * len(causal_options)\n",
    "        * len(seq_lens)\n",
    "    )\n",
    "    current_combo = 0\n",
    "\n",
    "    for (\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        num_random_features,\n",
    "        kernel_fn,\n",
    "        iscausal,\n",
    "        seq_length,\n",
    "    ) in itertools.product(\n",
    "        embed_dims,\n",
    "        num_heads_options,\n",
    "        num_random_features_options,\n",
    "        kernel_fn_options,\n",
    "        causal_options,\n",
    "        seq_lens,\n",
    "    ):\n",
    "        current_combo += 1\n",
    "        print(f\"\\n\\n{'='*20} COMBINATION {current_combo}/{total_combinations} {'='*20}\")\n",
    "        print(\n",
    "            f\"Embed dimension: {embed_dim}, Num heads: {num_heads}, Num random features: {num_random_features}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Kernel function: {kernel_fn}, Causal: {iscausal}, Sequence length: {seq_length}\"\n",
    "        )\n",
    "\n",
    "        # Check if parameters are valid\n",
    "        is_valid = is_valid_params(embed_dim, num_heads, num_random_features)\n",
    "\n",
    "        if not is_valid:\n",
    "            print(f\"INVALID COMBINATION: {embed_dim} is not divisible by {num_heads}\")\n",
    "            print(\"Skipping benchmarks for this invalid combination\")\n",
    "\n",
    "            # Add invalid entry to results data\n",
    "            for model_name in [m[1] for m in models_to_benchmark]:\n",
    "                results_data.append(\n",
    "                    {\n",
    "                        \"model\": model_name,\n",
    "                        \"embed_dim\": embed_dim,\n",
    "                        \"num_heads\": num_heads,\n",
    "                        \"num_random_features\": num_random_features,\n",
    "                        \"kernel_fn\": kernel_fn,\n",
    "                        \"iscausal\": iscausal,\n",
    "                        \"seq_length\": seq_length,\n",
    "                        \"forward_mean_ms\": float(\"nan\"),\n",
    "                        \"forward_std_ms\": float(\"nan\"),\n",
    "                        \"backward_mean_ms\": float(\"nan\"),\n",
    "                        \"backward_std_ms\": float(\"nan\"),\n",
    "                        \"forward_memory_mb\": float(\"nan\"),\n",
    "                        \"backward_memory_mb\": float(\"nan\"),\n",
    "                        \"is_valid\": False,\n",
    "                        \"error\": \"Invalid parameter combination\",\n",
    "                    }\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        # Create parameter object for this combination\n",
    "        params = BenchmarkParams(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_random_features=num_random_features,\n",
    "            seq_length=seq_length,\n",
    "        )\n",
    "        # Add the new parameters\n",
    "        params.kernel_fn = kernel_fn\n",
    "        params.iscausal = iscausal\n",
    "\n",
    "        all_results = {}\n",
    "        for model_factory, model_name in models_to_benchmark:\n",
    "            print(f\"\\n{'='*20} Benchmarking {model_name} {'='*20}\")\n",
    "            try:\n",
    "                results = benchmark_model_factory(model_factory, model_name, params)\n",
    "                all_results[model_name] = results\n",
    "\n",
    "                # Add result to our data collection\n",
    "                results_data.append(\n",
    "                    {\n",
    "                        \"model\": model_name,\n",
    "                        \"embed_dim\": embed_dim,\n",
    "                        \"num_heads\": num_heads,\n",
    "                        \"num_random_features\": num_random_features,\n",
    "                        \"kernel_fn\": kernel_fn,\n",
    "                        \"iscausal\": iscausal,\n",
    "                        \"seq_length\": seq_length,\n",
    "                        \"forward_mean_ms\": results[\"forward\"][\"mean\"],\n",
    "                        \"forward_std_ms\": results[\"forward\"][\"std\"],\n",
    "                        \"backward_mean_ms\": results[\"backward\"][\"mean\"],\n",
    "                        \"backward_std_ms\": results[\"backward\"][\"std\"],\n",
    "                        \"forward_memory_mb\": results[\"forward\"][\"memory_mb\"],\n",
    "                        \"backward_memory_mb\": results[\"backward\"][\"memory_mb\"],\n",
    "                        \"is_valid\": True,\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error benchmarking {model_name}: {e}\")\n",
    "                # Add error entry to data\n",
    "                results_data.append(\n",
    "                    {\n",
    "                        \"model\": model_name,\n",
    "                        \"embed_dim\": embed_dim,\n",
    "                        \"num_heads\": num_heads,\n",
    "                        \"num_random_features\": num_random_features,\n",
    "                        \"kernel_fn\": kernel_fn,\n",
    "                        \"iscausal\": iscausal,\n",
    "                        \"seq_length\": seq_length,\n",
    "                        \"forward_mean_ms\": float(\"nan\"),\n",
    "                        \"forward_std_ms\": float(\"nan\"),\n",
    "                        \"backward_mean_ms\": float(\"nan\"),\n",
    "                        \"backward_std_ms\": float(\"nan\"),\n",
    "                        \"forward_memory_mb\": float(\"nan\"),\n",
    "                        \"backward_memory_mb\": float(\"nan\"),\n",
    "                        \"is_valid\": True,\n",
    "                        \"error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Print comparative summary for this combination\n",
    "        if all_results:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(f\"{'='*20} SUMMARY FOR CURRENT COMBINATION {'='*20}\")\n",
    "            print(\"=\" * 60)\n",
    "            print(\n",
    "                f\"{'Model':<30} {'Forward (ms)':<25} {'Backward (ms)':<25} {'Forward Memory (MB)':<25} {'Backward Memory (MB)':<25}\"\n",
    "            )\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "            for model_name, results in all_results.items():\n",
    "                fwd = f\"{results['forward']['mean']:.3f} ± {results['forward']['std']:.3f}\"\n",
    "                bwd = f\"{results['backward']['mean']:.3f} ± {results['backward']['std']:.3f}\"\n",
    "                fwd_mem = f\"{results['forward']['memory_mb']:.2f}\"\n",
    "                bwd_mem = f\"{results['backward']['memory_mb']:.2f}\"\n",
    "                print(\n",
    "                    f\"{model_name:<30} {fwd:<25} {bwd:<25} {fwd_mem:<25} {bwd_mem:<25}\"\n",
    "                )\n",
    "\n",
    "    # Create a DataFrame with all results\n",
    "    df = pd.DataFrame(results_data)\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_file = \"attention_benchmark_results.csv\"\n",
    "    df.to_csv(results_file, index=False)\n",
    "    print(f\"\\nAll benchmark results saved to {results_file}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29488.417788,
   "end_time": "2025-05-08T02:50:58.415248",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-07T18:39:29.997460",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
