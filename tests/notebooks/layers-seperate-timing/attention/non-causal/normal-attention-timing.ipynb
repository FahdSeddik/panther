{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e10bc09",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-07T20:20:44.653173Z",
     "iopub.status.busy": "2025-05-07T20:20:44.652883Z",
     "iopub.status.idle": "2025-05-07T20:20:44.819559Z",
     "shell.execute_reply": "2025-05-07T20:20:44.818919Z"
    },
    "papermill": {
     "duration": 0.172824,
     "end_time": "2025-05-07T20:20:44.820986",
     "exception": false,
     "start_time": "2025-05-07T20:20:44.648162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "token = user_secrets.get_secret(\"github_repos_wildcard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f95ae90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:20:44.833264Z",
     "iopub.status.busy": "2025-05-07T20:20:44.832564Z",
     "iopub.status.idle": "2025-05-07T20:20:44.836333Z",
     "shell.execute_reply": "2025-05-07T20:20:44.835523Z"
    },
    "papermill": {
     "duration": 0.008872,
     "end_time": "2025-05-07T20:20:44.837659",
     "exception": false,
     "start_time": "2025-05-07T20:20:44.828787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_url = f\"https://{token}@github.com/gaserSami/panther.git\"\n",
    "branch = \"autotuner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f40c6e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:20:44.844962Z",
     "iopub.status.busy": "2025-05-07T20:20:44.844636Z",
     "iopub.status.idle": "2025-05-07T20:20:48.697254Z",
     "shell.execute_reply": "2025-05-07T20:20:48.696433Z"
    },
    "papermill": {
     "duration": 3.858194,
     "end_time": "2025-05-07T20:20:48.698798",
     "exception": false,
     "start_time": "2025-05-07T20:20:44.840604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'panther'...\r\n",
      "remote: Enumerating objects: 1046, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (181/181), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (96/96), done.\u001b[K\r\n",
      "remote: Total 1046 (delta 125), reused 110 (delta 85), pack-reused 865 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (1046/1046), 27.77 MiB | 19.15 MiB/s, done.\r\n",
      "Resolving deltas: 100% (626/626), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone -b {branch} {repo_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf44f8d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:20:48.707636Z",
     "iopub.status.busy": "2025-05-07T20:20:48.707337Z",
     "iopub.status.idle": "2025-05-07T20:20:48.830127Z",
     "shell.execute_reply": "2025-05-07T20:20:48.829065Z"
    },
    "papermill": {
     "duration": 0.129128,
     "end_time": "2025-05-07T20:20:48.831989",
     "exception": false,
     "start_time": "2025-05-07T20:20:48.702861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv panther Panther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3da4595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:20:48.842009Z",
     "iopub.status.busy": "2025-05-07T20:20:48.841653Z",
     "iopub.status.idle": "2025-05-07T20:24:01.097272Z",
     "shell.execute_reply": "2025-05-07T20:24:01.096512Z"
    },
    "papermill": {
     "duration": 192.262422,
     "end_time": "2025-05-07T20:24:01.098851",
     "exception": false,
     "start_time": "2025-05-07T20:20:48.836429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.5.1+cu124\r\n",
      "Uninstalling torch-2.5.1+cu124:\r\n",
      "  Successfully uninstalled torch-2.5.1+cu124\r\n",
      "Found existing installation: torchvision 0.20.1+cu124\r\n",
      "Uninstalling torchvision-0.20.1+cu124:\r\n",
      "  Successfully uninstalled torchvision-0.20.1+cu124\r\n",
      "Found existing installation: torchaudio 2.5.1+cu124\r\n",
      "Uninstalling torchaudio-2.5.1+cu124:\r\n",
      "  Successfully uninstalled torchaudio-2.5.1+cu124\r\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu124\r\n",
      "Collecting torch==2.6.0+cu124\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (28 kB)\r\n",
      "Collecting torchvision==0.21.0+cu124\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\r\n",
      "Collecting torchaudio==2.6.0+cu124\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (4.13.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting triton==3.2.0 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (1.13.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (11.1.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0+cu124) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0+cu124) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0+cu124) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0+cu124) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.21.0+cu124) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.21.0+cu124) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.21.0+cu124) (2024.2.0)\r\n",
      "Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (768.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m885.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl (7.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchaudio, torchvision\r\n",
      "  Attempting uninstall: triton\r\n",
      "    Found existing installation: triton 3.1.0\r\n",
      "    Uninstalling triton-3.1.0:\r\n",
      "      Successfully uninstalled triton-3.1.0\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu124 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nvjitlink-cu12-12.4.127 torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124 triton-3.2.0\r\n"
     ]
    }
   ],
   "source": [
    "# First uninstall existing torch, torchvision, torchaudio\n",
    "!pip uninstall -y torch torchvision torchaudio\n",
    "\n",
    "# Install the specified versions from PyTorch's official CUDA 12.4 wheels\n",
    "!pip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e980f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:24:01.253914Z",
     "iopub.status.busy": "2025-05-07T20:24:01.253294Z",
     "iopub.status.idle": "2025-05-07T20:24:04.043755Z",
     "shell.execute_reply": "2025-05-07T20:24:04.042653Z"
    },
    "papermill": {
     "duration": 2.83315,
     "end_time": "2025-05-07T20:24:04.045339",
     "exception": false,
     "start_time": "2025-05-07T20:24:01.212189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import triton\n",
    "print(triton.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ab3506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:24:04.127057Z",
     "iopub.status.busy": "2025-05-07T20:24:04.126330Z",
     "iopub.status.idle": "2025-05-07T20:24:04.129942Z",
     "shell.execute_reply": "2025-05-07T20:24:04.129334Z"
    },
    "papermill": {
     "duration": 0.044853,
     "end_time": "2025-05-07T20:24:04.131051",
     "exception": false,
     "start_time": "2025-05-07T20:24:04.086198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !export LC_ALL=\"en_US.UTF-8\"\n",
    "# !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "# !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "# !ldconfig /usr/lib64-nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d3ce3e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:24:04.214676Z",
     "iopub.status.busy": "2025-05-07T20:24:04.213751Z",
     "iopub.status.idle": "2025-05-07T20:24:04.219431Z",
     "shell.execute_reply": "2025-05-07T20:24:04.218880Z"
    },
    "papermill": {
     "duration": 0.049491,
     "end_time": "2025-05-07T20:24:04.220638",
     "exception": false,
     "start_time": "2025-05-07T20:24:04.171147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/Panther/pawX/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/Panther/pawX/setup.py\n",
    "from setuptools import setup\n",
    "from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n",
    "\n",
    "setup(\n",
    "    name=\"pawX\",\n",
    "    ext_modules=[\n",
    "        CUDAExtension(\n",
    "            name=\"pawX\",\n",
    "            sources=[\n",
    "                \"skops.cpp\",\n",
    "                \"bindings.cpp\",\n",
    "                \"linear.cpp\",\n",
    "                \"linear_cuda.cu\",\n",
    "                \"cqrrpt.cpp\",\n",
    "                \"rsvd.cpp\",\n",
    "                \"attention.cpp\",\n",
    "                \"conv2d.cpp\"\n",
    "            ],\n",
    "            # Use system includes and libraries\n",
    "            include_dirs=[\"/usr/include/x86_64-linux-gnu\"],\n",
    "            library_dirs=[],\n",
    "            libraries=[\"openblas\"],\n",
    "            extra_compile_args={\"cxx\": [\"-O2\", \"-fopenmp\"], \"nvcc\": [\"-O2\"]},\n",
    "            extra_link_args=[\"-llapacke\", \"-lopenblas\"]\n",
    "        )\n",
    "    ],\n",
    "    cmdclass={\"build_ext\": BuildExtension},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00c84a1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:24:04.301709Z",
     "iopub.status.busy": "2025-05-07T20:24:04.300979Z",
     "iopub.status.idle": "2025-05-07T20:24:18.380967Z",
     "shell.execute_reply": "2025-05-07T20:24:18.379889Z"
    },
    "papermill": {
     "duration": 14.122334,
     "end_time": "2025-05-07T20:24:18.382535",
     "exception": false,
     "start_time": "2025-05-07T20:24:04.260201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "The following additional packages will be installed:\r\n",
      "  liblapacke libtmglib-dev libtmglib3\r\n",
      "Suggested packages:\r\n",
      "  liblapack-doc\r\n",
      "The following NEW packages will be installed:\r\n",
      "  liblapacke liblapacke-dev libtmglib-dev libtmglib3\r\n",
      "0 upgraded, 4 newly installed, 0 to remove and 122 not upgraded.\r\n",
      "Need to get 1,071 kB of archives.\r\n",
      "After this operation, 12.3 MB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtmglib3 amd64 3.10.0-2ubuntu1 [144 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblapacke amd64 3.10.0-2ubuntu1 [435 kB]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtmglib-dev amd64 3.10.0-2ubuntu1 [134 kB]\r\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblapacke-dev amd64 3.10.0-2ubuntu1 [358 kB]\r\n",
      "Fetched 1,071 kB in 0s (3,233 kB/s)\r\n",
      "debconf: unable to initialize frontend: Dialog\r\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\r\n",
      "debconf: falling back to frontend: Readline\r\n",
      "Selecting previously unselected package libtmglib3:amd64.\r\n",
      "(Reading database ... 128691 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libtmglib3_3.10.0-2ubuntu1_amd64.deb ...\r\n",
      "Unpacking libtmglib3:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Selecting previously unselected package liblapacke:amd64.\r\n",
      "Preparing to unpack .../liblapacke_3.10.0-2ubuntu1_amd64.deb ...\r\n",
      "Unpacking liblapacke:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Selecting previously unselected package libtmglib-dev:amd64.\r\n",
      "Preparing to unpack .../libtmglib-dev_3.10.0-2ubuntu1_amd64.deb ...\r\n",
      "Unpacking libtmglib-dev:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Selecting previously unselected package liblapacke-dev:amd64.\r\n",
      "Preparing to unpack .../liblapacke-dev_3.10.0-2ubuntu1_amd64.deb ...\r\n",
      "Unpacking liblapacke-dev:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Setting up libtmglib3:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Setting up liblapacke:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Setting up libtmglib-dev:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Setting up liblapacke-dev:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install liblapacke-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da4d3d4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:24:18.468632Z",
     "iopub.status.busy": "2025-05-07T20:24:18.468304Z",
     "iopub.status.idle": "2025-05-07T20:27:37.726748Z",
     "shell.execute_reply": "2025-05-07T20:27:37.725843Z"
    },
    "papermill": {
     "duration": 199.303234,
     "end_time": "2025-05-07T20:27:37.728281",
     "exception": false,
     "start_time": "2025-05-07T20:24:18.425047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\r\n",
      "!!\r\n",
      "\r\n",
      "        ********************************************************************************\r\n",
      "        Please avoid running ``setup.py`` directly.\r\n",
      "        Instead, use pypa/build, pypa/installer or other\r\n",
      "        standards-based tools.\r\n",
      "\r\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\r\n",
      "        ********************************************************************************\r\n",
      "\r\n",
      "!!\r\n",
      "  self.initialize_options()\r\n",
      "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\r\n",
      "!!\r\n",
      "\r\n",
      "        ********************************************************************************\r\n",
      "        Please avoid running ``setup.py`` and ``easy_install``.\r\n",
      "        Instead, use pypa/build, pypa/installer or other\r\n",
      "        standards-based tools.\r\n",
      "\r\n",
      "        See https://github.com/pypa/setuptools/issues/917 for details.\r\n",
      "        ********************************************************************************\r\n",
      "\r\n",
      "!!\r\n",
      "  self.initialize_options()\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\r\n",
      "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\r\n",
      "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\r\n",
      "  warnings.warn(\r\n",
      "Emitting ninja build file /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/build.ninja...\r\n",
      "Compiling objects...\r\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n",
      "[1/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/conv2d.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/conv2d.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/conv2d.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "/kaggle/working/Panther/pawX/conv2d.cpp: In function ‘std::vector<at::Tensor> sketched_conv2d_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const std::vector<long int>&, const std::vector<long int>&, const std::vector<long int>&, const at::Tensor&)’:\r\n",
      "/kaggle/working/Panther/pawX/conv2d.cpp:19:43: warning: unused variable ‘H’ [-Wunused-variable]\r\n",
      "   19 |     int64_t B = x.size(0), C = x.size(1), H = x.size(2), W = x.size(3);\r\n",
      "      |                                           ^\r\n",
      "/kaggle/working/Panther/pawX/conv2d.cpp:19:58: warning: unused variable ‘W’ [-Wunused-variable]\r\n",
      "   19 |     int64_t B = x.size(0), C = x.size(1), H = x.size(2), W = x.size(3);\r\n",
      "      |                                                          ^\r\n",
      "[2/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/linear.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "[3/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/cqrrpt.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "[4/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/attention.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "[5/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/bindings.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "In file included from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/Exceptions.h:12,\r\n",
      "                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/python.h:11,\r\n",
      "                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/extension.h:9,\r\n",
      "                 from /kaggle/working/Panther/pawX/attention.h:3,\r\n",
      "                 from /kaggle/working/Panther/pawX/bindings.cpp:1:\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘class pybind11::class_<DistributionFamily>’:\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2216:7:   required from ‘class pybind11::enum_<DistributionFamily>’\r\n",
      "/kaggle/working/Panther/pawX/bindings.cpp:24:58:   required from here\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1539:7: warning: ‘pybind11::class_<DistributionFamily>’ declared with greater visibility than its base ‘pybind11::detail::generic_type’ [-Wattributes]\r\n",
      " 1539 | class class_ : public detail::generic_type {\r\n",
      "      |       ^~~~~~\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘pybind11::class_< <template-parameter-1-1>, <template-parameter-1-2> >::class_(pybind11::handle, const char*, const Extra& ...) [with Extra = {}; type_ = DistributionFamily; options = {}]’:\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2232:67:   required from ‘pybind11::enum_<Type>::enum_(const pybind11::handle&, const char*, const Extra& ...) [with Extra = {}; Type = DistributionFamily]’\r\n",
      "/kaggle/working/Panther/pawX/bindings.cpp:24:58:   required from here\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1599:28: warning: ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>’ declared with greater visibility than the type of its field ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>::<record capture>’ [-Wattributes]\r\n",
      " 1599 |             with_internals([&](internals &internals) {\r\n",
      "      |                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1600 |                 auto &instances = record.module_local ? get_local_internals().registered_types_cpp\r\n",
      "      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1601 |                                                       : internals.registered_types_cpp;\r\n",
      "      |                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1602 |                 instances[std::type_index(typeid(type_alias))]\r\n",
      "      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1603 |                     = instances[std::type_index(typeid(type))];\r\n",
      "      |                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1604 |             });\r\n",
      "      |             ~               \r\n",
      "[6/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/rsvd.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "[7/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/skops.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "[8/8] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o.d -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/linear_cuda.cu -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\r\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pawX-0.0.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\r\n",
      "\u001b[0mObtaining file:///kaggle/working/Panther/pawX\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Installing collected packages: pawX\r\n",
      "  Attempting uninstall: pawX\r\n",
      "    Found existing installation: pawX 0.0.0\r\n",
      "    Uninstalling pawX-0.0.0:\r\n",
      "      Successfully uninstalled pawX-0.0.0\r\n",
      "  Running setup.py develop for pawX\r\n",
      "Successfully installed pawX-0.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!cd /kaggle/working/Panther/pawX; python setup.py install\n",
    "!cd /kaggle/working/Panther/pawX; pip install --no-build-isolation -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f4da75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:27:37.819354Z",
     "iopub.status.busy": "2025-05-07T20:27:37.819044Z",
     "iopub.status.idle": "2025-05-07T20:27:37.823296Z",
     "shell.execute_reply": "2025-05-07T20:27:37.822621Z"
    },
    "papermill": {
     "duration": 0.051389,
     "end_time": "2025-05-07T20:27:37.824606",
     "exception": false,
     "start_time": "2025-05-07T20:27:37.773217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54df21e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:27:37.915086Z",
     "iopub.status.busy": "2025-05-07T20:27:37.914809Z",
     "iopub.status.idle": "2025-05-07T20:27:37.918846Z",
     "shell.execute_reply": "2025-05-07T20:27:37.918122Z"
    },
    "papermill": {
     "duration": 0.050021,
     "end_time": "2025-05-07T20:27:37.920119",
     "exception": false,
     "start_time": "2025-05-07T20:27:37.870098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/kaggle/working/Panther/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b4768b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:27:38.010529Z",
     "iopub.status.busy": "2025-05-07T20:27:38.009396Z",
     "iopub.status.idle": "2025-05-07T20:27:38.141088Z",
     "shell.execute_reply": "2025-05-07T20:27:38.139997Z"
    },
    "papermill": {
     "duration": 0.178812,
     "end_time": "2025-05-07T20:27:38.142724",
     "exception": false,
     "start_time": "2025-05-07T20:27:37.963912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Panther\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5060317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:27:38.234022Z",
     "iopub.status.busy": "2025-05-07T20:27:38.233707Z",
     "iopub.status.idle": "2025-05-07T23:21:10.583754Z",
     "shell.execute_reply": "2025-05-07T23:21:10.582718Z"
    },
    "papermill": {
     "duration": 10412.397026,
     "end_time": "2025-05-07T23:21:10.585097",
     "exception": false,
     "start_time": "2025-05-07T20:27:38.188071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: botorch is not available. Install with: pip install botorch\n",
      "\n",
      "\n",
      "==================== COMBINATION 1/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 10.375 ± 0.382 ms, Memory: 632.38 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 16.641 ± 0.105 ms, Memory: 1216.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      10.375 ± 0.382            16.641 ± 0.105            632.38                    1216.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 2/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 35.850 ± 0.343 ms, Memory: 2288.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 60.072 ± 0.780 ms, Memory: 4464.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      35.850 ± 0.343            60.072 ± 0.780            2288.50                   4464.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 3/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 154.719 ± 2.410 ms, Memory: 8656.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.84 GiB is free. Process 5709 has 12.90 GiB memory in use. Of the allocated memory 12.64 GiB is allocated by PyTorch, and 133.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 4/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.84 GiB is free. Process 5709 has 4.90 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 5/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.84 GiB is free. Process 5709 has 4.90 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 3.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 6/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 10.641 ± 0.077 ms, Memory: 640.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 17.436 ± 0.112 ms, Memory: 1216.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      10.641 ± 0.077            17.436 ± 0.112            640.50                    1216.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 7/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 40.153 ± 0.536 ms, Memory: 2288.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 63.640 ± 0.433 ms, Memory: 4464.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      40.153 ± 0.536            63.640 ± 0.433            2288.50                   4464.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 8/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 173.479 ± 2.262 ms, Memory: 8656.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.84 GiB is free. Process 5709 has 12.90 GiB memory in use. Of the allocated memory 12.64 GiB is allocated by PyTorch, and 133.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 9/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.84 GiB is free. Process 5709 has 4.90 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 10/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.84 GiB is free. Process 5709 has 4.90 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 3.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 11/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 11.093 ± 0.311 ms, Memory: 640.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 17.936 ± 0.235 ms, Memory: 1216.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      11.093 ± 0.311            17.936 ± 0.235            640.50                    1216.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 12/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 40.147 ± 0.295 ms, Memory: 2288.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 63.474 ± 0.525 ms, Memory: 4464.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      40.147 ± 0.295            63.474 ± 0.525            2288.50                   4464.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 13/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 170.199 ± 1.482 ms, Memory: 8656.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.84 GiB is free. Process 5709 has 12.90 GiB memory in use. Of the allocated memory 12.64 GiB is allocated by PyTorch, and 133.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 14/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.84 GiB is free. Process 5709 has 4.90 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 15/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.84 GiB is free. Process 5709 has 4.90 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 3.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 16/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 11.129 ± 0.287 ms, Memory: 640.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 18.062 ± 0.231 ms, Memory: 1216.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      11.129 ± 0.287            18.062 ± 0.231            640.50                    1216.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 17/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 40.101 ± 0.319 ms, Memory: 2288.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 64.199 ± 1.674 ms, Memory: 4464.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      40.101 ± 0.319            64.199 ± 1.674            2288.50                   4464.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 18/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 170.391 ± 1.145 ms, Memory: 8656.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.84 GiB is free. Process 5709 has 12.90 GiB memory in use. Of the allocated memory 12.64 GiB is allocated by PyTorch, and 133.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 19/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.84 GiB is free. Process 5709 has 4.90 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 20/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.84 GiB is free. Process 5709 has 4.90 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 3.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 21/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 11.090 ± 0.290 ms, Memory: 640.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 17.922 ± 0.212 ms, Memory: 1216.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      11.090 ± 0.290            17.922 ± 0.212            640.50                    1216.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 22/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 40.165 ± 0.322 ms, Memory: 2288.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 63.653 ± 0.639 ms, Memory: 4464.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      40.165 ± 0.322            63.653 ± 0.639            2288.50                   4464.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 23/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 169.967 ± 1.020 ms, Memory: 8656.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.84 GiB is free. Process 5709 has 12.90 GiB memory in use. Of the allocated memory 12.64 GiB is allocated by PyTorch, and 133.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 24/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.84 GiB is free. Process 5709 has 4.90 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 25/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.84 GiB is free. Process 5709 has 4.90 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 3.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 26/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 11.023 ± 0.246 ms, Memory: 640.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 17.943 ± 0.197 ms, Memory: 1216.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      11.023 ± 0.246            17.943 ± 0.197            640.50                    1216.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 27/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 40.181 ± 0.360 ms, Memory: 2288.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 63.829 ± 1.241 ms, Memory: 4464.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      40.181 ± 0.360            63.829 ± 1.241            2288.50                   4464.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 28/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 170.200 ± 0.912 ms, Memory: 8656.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.84 GiB is free. Process 5709 has 12.90 GiB memory in use. Of the allocated memory 12.64 GiB is allocated by PyTorch, and 133.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 29/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.84 GiB is free. Process 5709 has 4.90 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 30/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.84 GiB is free. Process 5709 has 4.90 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 3.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 31/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 18.032 ± 0.198 ms, Memory: 1152.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 31.423 ± 0.245 ms, Memory: 2240.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      18.032 ± 0.198            31.423 ± 0.245            1152.50                   2240.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 32/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 68.050 ± 0.995 ms, Memory: 4336.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 118.170 ± 0.735 ms, Memory: 8560.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      68.050 ± 0.995            118.170 ± 0.735           4336.50                   8560.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 33/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 8.45 GiB is allocated by PyTorch, and 69.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 34/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 7.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 35/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 6.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 36/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 18.123 ± 0.611 ms, Memory: 1152.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 31.509 ± 0.276 ms, Memory: 2240.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      18.123 ± 0.611            31.509 ± 0.276            1152.50                   2240.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 37/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 68.575 ± 1.057 ms, Memory: 4336.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 118.576 ± 0.621 ms, Memory: 8560.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      68.575 ± 1.057            118.576 ± 0.621           4336.50                   8560.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 38/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 8.45 GiB is allocated by PyTorch, and 69.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 39/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 7.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 40/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 6.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 41/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 18.065 ± 0.188 ms, Memory: 1152.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 31.548 ± 0.274 ms, Memory: 2240.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      18.065 ± 0.188            31.548 ± 0.274            1152.50                   2240.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 42/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 68.649 ± 1.127 ms, Memory: 4336.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 118.829 ± 0.779 ms, Memory: 8560.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      68.649 ± 1.127            118.829 ± 0.779           4336.50                   8560.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 43/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 8.45 GiB is allocated by PyTorch, and 69.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 44/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 7.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 45/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 6.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 46/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 18.072 ± 0.195 ms, Memory: 1152.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 31.567 ± 0.283 ms, Memory: 2240.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      18.072 ± 0.195            31.567 ± 0.283            1152.50                   2240.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 47/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 68.594 ± 1.057 ms, Memory: 4336.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 118.753 ± 0.814 ms, Memory: 8560.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      68.594 ± 1.057            118.753 ± 0.814           4336.50                   8560.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 48/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 8.45 GiB is allocated by PyTorch, and 69.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 49/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 7.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 50/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 6.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 51/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 18.067 ± 0.196 ms, Memory: 1152.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 31.529 ± 0.285 ms, Memory: 2240.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      18.067 ± 0.196            31.529 ± 0.285            1152.50                   2240.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 52/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 68.711 ± 1.017 ms, Memory: 4336.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 118.570 ± 0.686 ms, Memory: 8560.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      68.711 ± 1.017            118.570 ± 0.686           4336.50                   8560.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 53/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 8.45 GiB is allocated by PyTorch, and 69.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 54/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 7.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 55/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 6.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 56/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 18.091 ± 0.208 ms, Memory: 1152.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 31.535 ± 0.273 ms, Memory: 2240.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      18.091 ± 0.208            31.535 ± 0.273            1152.50                   2240.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 57/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 68.736 ± 1.051 ms, Memory: 4336.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 118.614 ± 0.753 ms, Memory: 8560.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      68.736 ± 1.051            118.614 ± 0.753           4336.50                   8560.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 58/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 8.45 GiB is allocated by PyTorch, and 69.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 59/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 7.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 60/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.09 GiB is free. Process 5709 has 8.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 6.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 61/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 31.972 ± 0.246 ms, Memory: 2176.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 58.448 ± 0.400 ms, Memory: 4288.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      31.972 ± 0.246            58.448 ± 0.400            2176.50                   4288.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 62/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 124.959 ± 0.535 ms, Memory: 8432.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.09 GiB is free. Process 5709 has 12.65 GiB memory in use. Of the allocated memory 12.33 GiB is allocated by PyTorch, and 197.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 63/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.09 GiB is free. Process 5709 has 668.00 MiB memory in use. Of the allocated memory 464.50 MiB is allocated by PyTorch, and 69.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 64/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.71 GiB is free. Process 5709 has 1.03 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 5.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 65/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.84 GiB is free. Process 5709 has 1.90 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 5.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 66/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 31.965 ± 0.257 ms, Memory: 2176.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 58.364 ± 0.430 ms, Memory: 4288.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      31.965 ± 0.257            58.364 ± 0.430            2176.50                   4288.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 67/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 124.925 ± 0.531 ms, Memory: 8432.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.09 GiB is free. Process 5709 has 12.65 GiB memory in use. Of the allocated memory 12.33 GiB is allocated by PyTorch, and 197.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 68/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.09 GiB is free. Process 5709 has 668.00 MiB memory in use. Of the allocated memory 464.50 MiB is allocated by PyTorch, and 69.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 69/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.71 GiB is free. Process 5709 has 1.03 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 5.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 70/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.84 GiB is free. Process 5709 has 1.90 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 5.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 71/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 31.935 ± 0.243 ms, Memory: 2176.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 58.565 ± 0.299 ms, Memory: 4288.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      31.935 ± 0.243            58.565 ± 0.299            2176.50                   4288.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 72/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 124.871 ± 0.619 ms, Memory: 8432.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.09 GiB is free. Process 5709 has 12.65 GiB memory in use. Of the allocated memory 12.33 GiB is allocated by PyTorch, and 197.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 73/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.09 GiB is free. Process 5709 has 668.00 MiB memory in use. Of the allocated memory 464.50 MiB is allocated by PyTorch, and 69.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 74/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.71 GiB is free. Process 5709 has 1.03 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 5.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 75/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.84 GiB is free. Process 5709 has 1.90 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 5.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 76/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 31.921 ± 0.245 ms, Memory: 2176.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 58.446 ± 0.356 ms, Memory: 4288.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      31.921 ± 0.245            58.446 ± 0.356            2176.50                   4288.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 77/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 124.721 ± 0.652 ms, Memory: 8432.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.09 GiB is free. Process 5709 has 12.65 GiB memory in use. Of the allocated memory 12.33 GiB is allocated by PyTorch, and 197.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 78/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.09 GiB is free. Process 5709 has 668.00 MiB memory in use. Of the allocated memory 464.50 MiB is allocated by PyTorch, and 69.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 79/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.71 GiB is free. Process 5709 has 1.03 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 5.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 80/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.84 GiB is free. Process 5709 has 1.90 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 5.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 81/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 31.966 ± 0.251 ms, Memory: 2176.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 58.452 ± 0.460 ms, Memory: 4288.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      31.966 ± 0.251            58.452 ± 0.460            2176.50                   4288.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 82/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 124.830 ± 0.619 ms, Memory: 8432.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.09 GiB is free. Process 5709 has 12.65 GiB memory in use. Of the allocated memory 12.33 GiB is allocated by PyTorch, and 197.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 83/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.09 GiB is free. Process 5709 has 668.00 MiB memory in use. Of the allocated memory 464.50 MiB is allocated by PyTorch, and 69.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 84/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.71 GiB is free. Process 5709 has 1.03 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 5.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 85/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.84 GiB is free. Process 5709 has 1.90 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 5.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 86/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 31.903 ± 0.240 ms, Memory: 2176.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 58.465 ± 0.368 ms, Memory: 4288.75 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      31.903 ± 0.240            58.465 ± 0.368            2176.50                   4288.75                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 87/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 124.864 ± 0.575 ms, Memory: 8432.50 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.09 GiB is free. Process 5709 has 12.65 GiB memory in use. Of the allocated memory 12.33 GiB is allocated by PyTorch, and 197.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 88/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.09 GiB is free. Process 5709 has 668.00 MiB memory in use. Of the allocated memory 464.50 MiB is allocated by PyTorch, and 69.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 89/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.71 GiB is free. Process 5709 has 1.03 GiB memory in use. Of the allocated memory 912.50 MiB is allocated by PyTorch, and 5.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 90/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.84 GiB is free. Process 5709 has 1.90 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 5.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 91/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 17.408 ± 0.548 ms, Memory: 753.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 27.196 ± 0.512 ms, Memory: 1394.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      17.408 ± 0.548            27.196 ± 0.512            753.25                    1394.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 92/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 58.297 ± 1.827 ms, Memory: 2513.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 87.677 ± 0.517 ms, Memory: 4818.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      58.297 ± 1.827            87.677 ± 0.517            2513.25                   4818.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 93/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 214.719 ± 1.435 ms, Memory: 9105.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.09 GiB is free. Process 5709 has 13.65 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 260.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 94/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.09 GiB is free. Process 5709 has 5.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 3.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 95/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.59 GiB is free. Process 5709 has 4.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 516.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 96/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 17.311 ± 0.473 ms, Memory: 753.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 26.989 ± 0.435 ms, Memory: 1394.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      17.311 ± 0.473            26.989 ± 0.435            753.25                    1394.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 97/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 57.308 ± 1.697 ms, Memory: 2513.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 87.822 ± 0.568 ms, Memory: 4818.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      57.308 ± 1.697            87.822 ± 0.568            2513.25                   4818.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 98/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 214.127 ± 1.258 ms, Memory: 9105.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 600.12 MiB is free. Process 5709 has 14.15 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 772.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 99/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.59 GiB is free. Process 5709 has 2.15 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 260.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 100/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 11.09 GiB is free. Process 5709 has 3.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 4.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 101/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 17.311 ± 0.575 ms, Memory: 753.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 27.004 ± 0.469 ms, Memory: 1394.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      17.311 ± 0.575            27.004 ± 0.469            753.25                    1394.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 102/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 57.407 ± 1.688 ms, Memory: 2513.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 87.614 ± 0.543 ms, Memory: 4818.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      57.407 ± 1.688            87.614 ± 0.543            2513.25                   4818.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 103/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 214.482 ± 1.380 ms, Memory: 9105.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.09 GiB is free. Process 5709 has 13.65 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 260.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 104/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.09 GiB is free. Process 5709 has 5.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 3.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 105/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.09 GiB is free. Process 5709 has 5.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 106/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 17.416 ± 0.779 ms, Memory: 753.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 27.039 ± 0.461 ms, Memory: 1394.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      17.416 ± 0.779            27.039 ± 0.461            753.25                    1394.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 107/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 57.512 ± 1.693 ms, Memory: 2513.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 87.753 ± 0.565 ms, Memory: 4818.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      57.512 ± 1.693            87.753 ± 0.565            2513.25                   4818.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 108/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 214.436 ± 1.389 ms, Memory: 9105.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.09 GiB is free. Process 5709 has 13.65 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 260.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 109/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.09 GiB is free. Process 5709 has 5.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 3.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 110/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.09 GiB is free. Process 5709 has 5.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 111/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 17.355 ± 0.678 ms, Memory: 753.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 27.007 ± 0.507 ms, Memory: 1394.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      17.355 ± 0.678            27.007 ± 0.507            753.25                    1394.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 112/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 57.485 ± 1.727 ms, Memory: 2513.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 87.692 ± 0.491 ms, Memory: 4818.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      57.485 ± 1.727            87.692 ± 0.491            2513.25                   4818.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 113/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 214.370 ± 1.450 ms, Memory: 9105.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.09 GiB is free. Process 5709 has 13.65 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 260.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 114/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.09 GiB is free. Process 5709 has 5.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 3.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 115/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.09 GiB is free. Process 5709 has 5.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 116/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 17.320 ± 0.525 ms, Memory: 753.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 27.003 ± 0.469 ms, Memory: 1394.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      17.320 ± 0.525            27.003 ± 0.469            753.25                    1394.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 117/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 57.473 ± 1.709 ms, Memory: 2513.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 87.737 ± 0.625 ms, Memory: 4818.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      57.473 ± 1.709            87.737 ± 0.625            2513.25                   4818.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 118/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 214.336 ± 1.367 ms, Memory: 9105.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.09 GiB is free. Process 5709 has 13.65 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 260.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 119/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.09 GiB is free. Process 5709 has 5.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 3.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 120/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.09 GiB is free. Process 5709 has 5.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 121/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 23.998 ± 0.685 ms, Memory: 1265.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 38.994 ± 1.094 ms, Memory: 2418.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      23.998 ± 0.685            38.994 ± 1.094            1265.25                   2418.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 122/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 84.956 ± 1.114 ms, Memory: 4561.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 140.524 ± 1.531 ms, Memory: 8914.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      84.956 ± 1.114            140.524 ± 1.531           4561.25                   8914.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 123/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 8.89 GiB is allocated by PyTorch, and 132.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 124/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 7.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 125/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 5.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 126/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 24.215 ± 0.898 ms, Memory: 1265.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 39.325 ± 1.192 ms, Memory: 2418.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      24.215 ± 0.898            39.325 ± 1.192            1265.25                   2418.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 127/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 85.194 ± 1.074 ms, Memory: 4561.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 139.776 ± 1.250 ms, Memory: 8914.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      85.194 ± 1.074            139.776 ± 1.250           4561.25                   8914.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 128/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 8.89 GiB is allocated by PyTorch, and 132.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 129/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 7.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 130/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 5.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 131/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 24.106 ± 0.495 ms, Memory: 1265.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 39.333 ± 1.173 ms, Memory: 2418.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      24.106 ± 0.495            39.333 ± 1.173            1265.25                   2418.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 132/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 85.736 ± 1.177 ms, Memory: 4561.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 140.868 ± 1.594 ms, Memory: 8914.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      85.736 ± 1.177            140.868 ± 1.594           4561.25                   8914.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 133/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 8.89 GiB is allocated by PyTorch, and 132.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 134/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 7.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 135/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 5.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 136/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 23.972 ± 0.458 ms, Memory: 1265.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 39.150 ± 1.069 ms, Memory: 2418.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      23.972 ± 0.458            39.150 ± 1.069            1265.25                   2418.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 137/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 85.233 ± 1.142 ms, Memory: 4561.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 140.078 ± 1.423 ms, Memory: 8914.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      85.233 ± 1.142            140.078 ± 1.423           4561.25                   8914.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 138/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 8.89 GiB is allocated by PyTorch, and 132.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 139/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 7.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 140/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 5.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 141/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 23.977 ± 0.469 ms, Memory: 1265.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 39.076 ± 1.093 ms, Memory: 2418.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      23.977 ± 0.469            39.076 ± 1.093            1265.25                   2418.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 142/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 85.245 ± 1.118 ms, Memory: 4561.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 140.050 ± 1.320 ms, Memory: 8914.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      85.245 ± 1.118            140.050 ± 1.320           4561.25                   8914.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 143/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 8.89 GiB is allocated by PyTorch, and 132.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 144/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 7.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 145/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 5.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 146/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 24.049 ± 0.487 ms, Memory: 1265.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 39.150 ± 1.144 ms, Memory: 2418.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      24.049 ± 0.487            39.150 ± 1.144            1265.25                   2418.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 147/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 85.515 ± 1.166 ms, Memory: 4561.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 140.967 ± 1.544 ms, Memory: 8914.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      85.515 ± 1.166            140.967 ± 1.544           4561.25                   8914.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 148/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 8.89 GiB is allocated by PyTorch, and 132.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 149/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 7.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 150/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 5.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 151/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 37.975 ± 0.776 ms, Memory: 2289.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 65.629 ± 1.310 ms, Memory: 4466.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      37.975 ± 0.776            65.629 ± 1.310            2289.25                   4466.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 152/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 140.525 ± 0.498 ms, Memory: 8657.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.64 GiB is allocated by PyTorch, and 388.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 153/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.59 GiB is free. Process 5709 has 1.15 GiB memory in use. Of the allocated memory 913.25 MiB is allocated by PyTorch, and 132.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 154/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.84 GiB is free. Process 5709 has 1.90 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 4.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 155/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 11.09 GiB is free. Process 5709 has 3.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 4.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 156/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 37.909 ± 0.745 ms, Memory: 2289.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 65.309 ± 1.177 ms, Memory: 4466.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      37.909 ± 0.745            65.309 ± 1.177            2289.25                   4466.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 157/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 140.357 ± 0.558 ms, Memory: 8657.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.09 GiB is free. Process 5709 has 12.65 GiB memory in use. Of the allocated memory 8.64 GiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 158/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 913.25 MiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 159/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 2.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 160/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 161/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 38.094 ± 0.859 ms, Memory: 2289.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 65.619 ± 1.305 ms, Memory: 4466.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      38.094 ± 0.859            65.619 ± 1.305            2289.25                   4466.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 162/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 140.212 ± 0.584 ms, Memory: 8657.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.09 GiB is free. Process 5709 has 12.65 GiB memory in use. Of the allocated memory 8.64 GiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 163/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 913.25 MiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 164/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 2.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 165/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 166/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 38.059 ± 1.133 ms, Memory: 2289.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 65.667 ± 1.352 ms, Memory: 4466.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      38.059 ± 1.133            65.667 ± 1.352            2289.25                   4466.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 167/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 140.842 ± 0.598 ms, Memory: 8657.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.09 GiB is free. Process 5709 has 12.65 GiB memory in use. Of the allocated memory 8.64 GiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 168/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 913.25 MiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 169/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 2.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 170/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 171/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 38.265 ± 1.089 ms, Memory: 2289.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 65.804 ± 1.393 ms, Memory: 4466.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      38.265 ± 1.089            65.804 ± 1.393            2289.25                   4466.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 172/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 140.448 ± 0.578 ms, Memory: 8657.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.09 GiB is free. Process 5709 has 12.65 GiB memory in use. Of the allocated memory 8.64 GiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 173/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 913.25 MiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 174/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 2.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 175/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 176/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 37.972 ± 1.100 ms, Memory: 2289.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 65.497 ± 1.257 ms, Memory: 4466.26 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      37.972 ± 1.100            65.497 ± 1.257            2289.25                   4466.26                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 177/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 140.016 ± 0.517 ms, Memory: 8657.25 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.09 GiB is free. Process 5709 has 12.65 GiB memory in use. Of the allocated memory 8.64 GiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 178/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 913.25 MiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 179/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 2.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 180/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.09 GiB is free. Process 5709 has 4.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 181/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 37.456 ± 1.016 ms, Memory: 981.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 58.021 ± 1.570 ms, Memory: 1753.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      37.456 ± 1.016            58.021 ± 1.570            981.01                    1753.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 182/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 104.472 ± 1.206 ms, Memory: 2965.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 164.963 ± 1.342 ms, Memory: 5529.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      104.472 ± 1.206           164.963 ± 1.342           2965.01                   5529.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 183/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 338.358 ± 0.878 ms, Memory: 10005.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 86.12 MiB is free. Process 5709 has 14.65 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 184/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.09 GiB is free. Process 5709 has 6.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 3.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 185/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 186/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 37.213 ± 1.041 ms, Memory: 981.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 57.119 ± 1.401 ms, Memory: 1753.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      37.213 ± 1.041            57.119 ± 1.401            981.01                    1753.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 187/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 104.406 ± 1.179 ms, Memory: 2965.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 166.353 ± 1.651 ms, Memory: 5529.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      104.406 ± 1.179           166.353 ± 1.651           2965.01                   5529.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 188/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 338.561 ± 0.869 ms, Memory: 10005.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.58 GiB is free. Process 5709 has 11.15 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 513.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 189/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 3.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 190/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 191/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 37.309 ± 1.043 ms, Memory: 981.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 57.272 ± 1.489 ms, Memory: 1753.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      37.309 ± 1.043            57.272 ± 1.489            981.01                    1753.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 192/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 104.072 ± 1.146 ms, Memory: 2965.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 165.223 ± 1.394 ms, Memory: 5529.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      104.072 ± 1.146           165.223 ± 1.394           2965.01                   5529.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 193/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 339.311 ± 0.828 ms, Memory: 10005.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.58 GiB is free. Process 5709 has 11.15 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 513.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 194/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 3.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 195/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 196/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 37.439 ± 1.135 ms, Memory: 981.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 57.562 ± 1.550 ms, Memory: 1753.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      37.439 ± 1.135            57.562 ± 1.550            981.01                    1753.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 197/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 104.574 ± 1.263 ms, Memory: 2965.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 165.618 ± 1.466 ms, Memory: 5529.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      104.574 ± 1.263           165.618 ± 1.466           2965.01                   5529.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 198/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 338.020 ± 0.814 ms, Memory: 10005.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.58 GiB is free. Process 5709 has 11.15 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 513.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 199/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 3.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 200/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 201/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 37.245 ± 1.031 ms, Memory: 981.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 57.339 ± 1.467 ms, Memory: 1753.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      37.245 ± 1.031            57.339 ± 1.467            981.01                    1753.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 202/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 104.282 ± 1.199 ms, Memory: 2965.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 165.427 ± 1.451 ms, Memory: 5529.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      104.282 ± 1.199           165.427 ± 1.451           2965.01                   5529.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 203/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 337.954 ± 0.834 ms, Memory: 10005.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.58 GiB is free. Process 5709 has 11.15 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 513.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 204/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 3.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 205/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 206/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 37.259 ± 1.011 ms, Memory: 981.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 57.323 ± 1.422 ms, Memory: 1753.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      37.259 ± 1.011            57.323 ± 1.422            981.01                    1753.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 207/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 104.289 ± 1.165 ms, Memory: 2965.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 165.489 ± 1.415 ms, Memory: 5529.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      104.289 ± 1.165           165.489 ± 1.415           2965.01                   5529.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 208/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 338.377 ± 0.863 ms, Memory: 10005.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.58 GiB is free. Process 5709 has 11.15 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 513.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 209/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 3.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 210/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 211/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 42.288 ± 1.150 ms, Memory: 1493.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 65.932 ± 1.371 ms, Memory: 2777.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      42.288 ± 1.150            65.932 ± 1.371            1493.01                   2777.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 212/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 128.586 ± 2.403 ms, Memory: 5013.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 209.528 ± 2.446 ms, Memory: 9625.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      128.586 ± 2.403           209.528 ± 2.446           5013.01                   9625.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 213/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 9.77 GiB is allocated by PyTorch, and 256.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 214/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 6.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 215/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 3.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 216/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 42.679 ± 1.155 ms, Memory: 1493.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 66.455 ± 1.201 ms, Memory: 2777.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      42.679 ± 1.155            66.455 ± 1.201            1493.01                   2777.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 217/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 128.927 ± 2.389 ms, Memory: 5013.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 209.690 ± 2.472 ms, Memory: 9625.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      128.927 ± 2.389           209.690 ± 2.472           5013.01                   9625.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 218/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 9.77 GiB is allocated by PyTorch, and 256.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 219/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 6.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 220/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 3.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 221/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 42.561 ± 1.161 ms, Memory: 1493.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 66.454 ± 1.285 ms, Memory: 2777.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      42.561 ± 1.161            66.454 ± 1.285            1493.01                   2777.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 222/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 128.778 ± 2.435 ms, Memory: 5013.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 210.033 ± 2.385 ms, Memory: 9625.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      128.778 ± 2.435           210.033 ± 2.385           5013.01                   9625.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 223/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 9.77 GiB is allocated by PyTorch, and 256.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 224/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 6.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 225/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 3.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 226/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 42.634 ± 1.160 ms, Memory: 1493.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 66.375 ± 1.198 ms, Memory: 2777.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      42.634 ± 1.160            66.375 ± 1.198            1493.01                   2777.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 227/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 128.809 ± 2.319 ms, Memory: 5013.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 209.717 ± 2.439 ms, Memory: 9625.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      128.809 ± 2.319           209.717 ± 2.439           5013.01                   9625.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 228/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 9.77 GiB is allocated by PyTorch, and 256.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 229/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 6.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 230/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 3.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 231/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 42.593 ± 1.143 ms, Memory: 1493.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 66.362 ± 1.203 ms, Memory: 2777.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      42.593 ± 1.143            66.362 ± 1.203            1493.01                   2777.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 232/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 128.787 ± 2.370 ms, Memory: 5013.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 209.558 ± 2.351 ms, Memory: 9625.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      128.787 ± 2.370           209.558 ± 2.351           5013.01                   9625.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 233/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 9.77 GiB is allocated by PyTorch, and 256.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 234/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 6.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 235/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 3.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 236/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 42.590 ± 1.161 ms, Memory: 1493.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 66.386 ± 1.191 ms, Memory: 2777.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      42.590 ± 1.161            66.386 ± 1.191            1493.01                   2777.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 237/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 128.691 ± 2.404 ms, Memory: 5013.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 210.078 ± 2.300 ms, Memory: 9625.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      128.691 ± 2.404           210.078 ± 2.300           5013.01                   9625.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 238/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 9.77 GiB is allocated by PyTorch, and 256.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 239/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 6.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 240/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.59 GiB is free. Process 5709 has 10.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 3.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 241/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 57.038 ± 1.488 ms, Memory: 2517.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 92.486 ± 2.255 ms, Memory: 4825.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      57.038 ± 1.488            92.486 ± 2.255            2517.01                   4825.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 242/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 185.097 ± 0.846 ms, Memory: 9109.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 598.12 MiB is free. Process 5709 has 14.15 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 769.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 243/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.59 GiB is free. Process 5709 has 2.15 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 256.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 244/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 11.09 GiB is free. Process 5709 has 3.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 245/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 246/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 56.872 ± 1.563 ms, Memory: 2517.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 92.203 ± 2.085 ms, Memory: 4825.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      56.872 ± 1.563            92.203 ± 2.085            2517.01                   4825.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 247/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 184.949 ± 0.769 ms, Memory: 9109.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.58 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 9.27 GiB is allocated by PyTorch, and 3.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 248/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.59 GiB is free. Process 5709 has 5.15 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 3.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 249/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.59 GiB is free. Process 5709 has 5.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 250/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 251/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 56.849 ± 1.511 ms, Memory: 2517.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 92.351 ± 2.219 ms, Memory: 4825.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      56.849 ± 1.511            92.351 ± 2.219            2517.01                   4825.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 252/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 184.940 ± 0.805 ms, Memory: 9109.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 598.12 MiB is free. Process 5709 has 14.15 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 769.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 253/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.59 GiB is free. Process 5709 has 2.15 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 256.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 254/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 11.09 GiB is free. Process 5709 has 3.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 255/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 256/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 57.031 ± 1.552 ms, Memory: 2517.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 92.789 ± 2.202 ms, Memory: 4825.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      57.031 ± 1.552            92.789 ± 2.202            2517.01                   4825.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 257/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 185.868 ± 0.766 ms, Memory: 9109.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.58 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 9.27 GiB is allocated by PyTorch, and 3.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 258/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.59 GiB is free. Process 5709 has 5.15 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 3.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 259/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.59 GiB is free. Process 5709 has 5.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 260/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 261/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 57.224 ± 1.599 ms, Memory: 2517.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 93.068 ± 2.171 ms, Memory: 4825.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      57.224 ± 1.599            93.068 ± 2.171            2517.01                   4825.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 262/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 186.018 ± 0.759 ms, Memory: 9109.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 598.12 MiB is free. Process 5709 has 14.15 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 769.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 263/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.59 GiB is free. Process 5709 has 2.15 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 256.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 264/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 11.09 GiB is free. Process 5709 has 3.65 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 265/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 266/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 57.285 ± 1.567 ms, Memory: 2517.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 92.994 ± 2.219 ms, Memory: 4825.02 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      57.285 ± 1.567            92.994 ± 2.219            2517.01                   4825.02                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 267/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 185.755 ± 0.674 ms, Memory: 9109.01 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.58 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 9.27 GiB is allocated by PyTorch, and 3.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 268/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.59 GiB is free. Process 5709 has 5.15 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 3.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 269/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.59 GiB is free. Process 5709 has 5.15 GiB memory in use. Of the allocated memory 3.52 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 270/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 1016.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 271/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 102.225 ± 0.832 ms, Memory: 1441.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 161.573 ± 0.662 ms, Memory: 2480.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      102.225 ± 0.832           161.573 ± 0.662           1441.27                   2480.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 272/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 249.174 ± 0.903 ms, Memory: 3872.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 405.162 ± 1.665 ms, Memory: 6960.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      249.174 ± 0.903           405.162 ± 1.665           3872.27                   6960.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 273/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 695.745 ± 1.977 ms, Memory: 11808.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.59 GiB is free. Process 5709 has 12.15 GiB memory in use. Of the allocated memory 9.03 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 274/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.59 GiB is free. Process 5709 has 8.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 275/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 276/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 100.725 ± 0.380 ms, Memory: 1441.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 161.762 ± 0.694 ms, Memory: 2480.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      100.725 ± 0.380           161.762 ± 0.694           1441.27                   2480.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 277/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 249.486 ± 0.908 ms, Memory: 3872.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 405.142 ± 1.446 ms, Memory: 6960.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      249.486 ± 0.908           405.142 ± 1.446           3872.27                   6960.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 278/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 696.191 ± 1.588 ms, Memory: 11808.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 9.03 GiB is allocated by PyTorch, and 3.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 279/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 1.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 280/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 281/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 100.527 ± 0.258 ms, Memory: 1441.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 161.509 ± 0.801 ms, Memory: 2480.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      100.527 ± 0.258           161.509 ± 0.801           1441.27                   2480.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 282/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 249.606 ± 0.904 ms, Memory: 3872.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 404.606 ± 1.581 ms, Memory: 6960.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      249.606 ± 0.904           404.606 ± 1.581           3872.27                   6960.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 283/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 695.884 ± 1.807 ms, Memory: 11808.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 9.03 GiB is allocated by PyTorch, and 3.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 284/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 1.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 285/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 286/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 100.557 ± 0.264 ms, Memory: 1441.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 161.763 ± 0.598 ms, Memory: 2480.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      100.557 ± 0.264           161.763 ± 0.598           1441.27                   2480.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 287/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 249.637 ± 1.054 ms, Memory: 3872.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 404.730 ± 1.544 ms, Memory: 6960.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      249.637 ± 1.054           404.730 ± 1.544           3872.27                   6960.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 288/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 695.341 ± 1.727 ms, Memory: 11808.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 9.03 GiB is allocated by PyTorch, and 3.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 289/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 1.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 290/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 291/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 100.629 ± 0.248 ms, Memory: 1441.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 161.519 ± 0.715 ms, Memory: 2480.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      100.629 ± 0.248           161.519 ± 0.715           1441.27                   2480.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 292/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 249.522 ± 0.928 ms, Memory: 3872.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 404.645 ± 1.432 ms, Memory: 6960.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      249.522 ± 0.928           404.645 ± 1.432           3872.27                   6960.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 293/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 696.718 ± 1.480 ms, Memory: 11808.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 9.03 GiB is allocated by PyTorch, and 3.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 294/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 1.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 295/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 296/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 100.799 ± 0.490 ms, Memory: 1441.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 161.518 ± 0.806 ms, Memory: 2480.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      100.799 ± 0.490           161.518 ± 0.806           1441.27                   2480.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 297/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 249.899 ± 0.922 ms, Memory: 3872.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 404.966 ± 1.866 ms, Memory: 6960.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      249.899 ± 0.922           404.966 ± 1.866           3872.27                   6960.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 298/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 696.040 ± 1.294 ms, Memory: 11808.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 9.03 GiB is allocated by PyTorch, and 3.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 299/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.59 GiB is free. Process 5709 has 9.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 1.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 300/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 301/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 107.097 ± 0.638 ms, Memory: 1952.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 169.560 ± 1.361 ms, Memory: 3504.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      107.097 ± 0.638           169.560 ± 1.361           1952.27                   3504.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 302/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 273.613 ± 1.427 ms, Memory: 5920.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 443.799 ± 1.632 ms, Memory: 11056.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      273.613 ± 1.427           443.799 ± 1.632           5920.27                   11056.28                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 303/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 11.53 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 304/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 5.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 305/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 306/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 107.120 ± 0.633 ms, Memory: 1952.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 169.493 ± 1.319 ms, Memory: 3504.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      107.120 ± 0.633           169.493 ± 1.319           1952.27                   3504.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 307/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 272.433 ± 1.649 ms, Memory: 5920.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 443.590 ± 1.530 ms, Memory: 11056.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      272.433 ± 1.649           443.590 ± 1.530           5920.27                   11056.28                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 308/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 11.53 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 309/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 5.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 310/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 311/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 107.519 ± 0.548 ms, Memory: 1952.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 169.548 ± 1.390 ms, Memory: 3504.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      107.519 ± 0.548           169.548 ± 1.390           1952.27                   3504.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 312/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 273.729 ± 1.325 ms, Memory: 5920.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 444.141 ± 1.824 ms, Memory: 11056.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      273.729 ± 1.325           444.141 ± 1.824           5920.27                   11056.28                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 313/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 11.53 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 314/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 5.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 315/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 316/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 107.460 ± 0.543 ms, Memory: 1952.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 169.935 ± 1.378 ms, Memory: 3504.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      107.460 ± 0.543           169.935 ± 1.378           1952.27                   3504.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 317/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 272.969 ± 1.340 ms, Memory: 5920.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 443.874 ± 1.615 ms, Memory: 11056.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      272.969 ± 1.340           443.874 ± 1.615           5920.27                   11056.28                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 318/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 11.53 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 319/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 5.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 320/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 321/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 107.522 ± 0.561 ms, Memory: 1952.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 169.718 ± 1.435 ms, Memory: 3504.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      107.522 ± 0.561           169.718 ± 1.435           1952.27                   3504.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 322/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 272.862 ± 1.490 ms, Memory: 5920.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 443.663 ± 1.620 ms, Memory: 11056.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      272.862 ± 1.490           443.663 ± 1.620           5920.27                   11056.28                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 323/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 11.53 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 324/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 5.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 325/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 326/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 107.471 ± 0.499 ms, Memory: 1952.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 169.656 ± 1.439 ms, Memory: 3504.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      107.471 ± 0.499           169.656 ± 1.439           1952.27                   3504.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 327/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 273.329 ± 1.392 ms, Memory: 5920.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 443.877 ± 1.518 ms, Memory: 11056.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      273.329 ± 1.392           443.877 ± 1.518           5920.27                   11056.28                 \n",
      "\n",
      "\n",
      "==================== COMBINATION 328/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 11.53 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 329/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 5.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 330/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 331/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 121.111 ± 0.509 ms, Memory: 2976.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 186.614 ± 0.693 ms, Memory: 5552.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      121.111 ± 0.509           186.614 ± 0.693           2976.27                   5552.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 332/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 322.891 ± 0.869 ms, Memory: 10016.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 333/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.59 GiB is free. Process 5709 has 5.15 GiB memory in use. Of the allocated memory 3.53 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 334/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.59 GiB is free. Process 5709 has 8.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 335/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 336/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 121.014 ± 0.457 ms, Memory: 2976.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 186.863 ± 1.007 ms, Memory: 5552.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      121.014 ± 0.457           186.863 ± 1.007           2976.27                   5552.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 337/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 322.911 ± 0.977 ms, Memory: 10016.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.59 GiB is free. Process 5709 has 11.15 GiB memory in use. Of the allocated memory 7.04 GiB is allocated by PyTorch, and 3.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 338/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 3.53 GiB is allocated by PyTorch, and 3.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 339/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.59 GiB is free. Process 5709 has 8.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 340/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 341/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 121.186 ± 0.513 ms, Memory: 2976.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 186.819 ± 0.928 ms, Memory: 5552.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      121.186 ± 0.513           186.819 ± 0.928           2976.27                   5552.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 342/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 322.829 ± 0.942 ms, Memory: 10016.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.59 GiB is free. Process 5709 has 11.15 GiB memory in use. Of the allocated memory 7.04 GiB is allocated by PyTorch, and 3.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 343/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 3.53 GiB is allocated by PyTorch, and 3.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 344/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.59 GiB is free. Process 5709 has 8.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 345/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 346/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 121.026 ± 0.353 ms, Memory: 2976.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 187.245 ± 0.898 ms, Memory: 5552.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      121.026 ± 0.353           187.245 ± 0.898           2976.27                   5552.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 347/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 322.826 ± 1.003 ms, Memory: 10016.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.59 GiB is free. Process 5709 has 11.15 GiB memory in use. Of the allocated memory 7.04 GiB is allocated by PyTorch, and 3.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 348/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 3.53 GiB is allocated by PyTorch, and 3.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 349/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.59 GiB is free. Process 5709 has 8.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 350/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 351/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 121.116 ± 0.482 ms, Memory: 2976.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 187.126 ± 0.951 ms, Memory: 5552.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      121.116 ± 0.482           187.126 ± 0.951           2976.27                   5552.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 352/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 322.846 ± 0.961 ms, Memory: 10016.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.59 GiB is free. Process 5709 has 11.15 GiB memory in use. Of the allocated memory 7.04 GiB is allocated by PyTorch, and 3.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 353/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 3.53 GiB is allocated by PyTorch, and 3.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 354/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.59 GiB is free. Process 5709 has 8.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 355/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 356/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 121.051 ± 0.431 ms, Memory: 2976.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "attention backward: 187.120 ± 0.850 ms, Memory: 5552.28 ± 0.00 MB\n",
      "\n",
      "============================================================\n",
      "==================== SUMMARY FOR CURRENT COMBINATION ====================\n",
      "============================================================\n",
      "Model                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n",
      "------------------------------------------------------------\n",
      "attention                      121.051 ± 0.431           187.120 ± 0.850           2976.27                   5552.28                  \n",
      "\n",
      "\n",
      "==================== COMBINATION 357/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "attention forward: 322.894 ± 0.961 ms, Memory: 10016.27 ± 0.00 MB\n",
      "\n",
      "=== attention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.59 GiB is free. Process 5709 has 11.15 GiB memory in use. Of the allocated memory 7.04 GiB is allocated by PyTorch, and 3.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 358/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.59 GiB is free. Process 5709 has 7.15 GiB memory in use. Of the allocated memory 3.53 GiB is allocated by PyTorch, and 3.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 359/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.59 GiB is free. Process 5709 has 8.15 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 360/360 ====================\n",
      "Embed dimension: 1024, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: False, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking attention ====================\n",
      "\n",
      "=== attention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.59 GiB is free. Process 5709 has 13.15 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "All benchmark results saved to attention_benchmark_results.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch._dynamo\n",
    "import torch._inductor.config as config\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# Configure torch\n",
    "config.max_autotune_gemm = False\n",
    "torch._dynamo.config.cache_size_limit = 2**16\n",
    "torch._dynamo.config.accumulated_cache_size_limit = 2**16\n",
    "\n",
    "def is_valid_params(embed_dim, num_heads, num_random_features):\n",
    "    \"\"\"\n",
    "    Check if parameter combination is valid:\n",
    "    embed_dim must be divisible by num_heads\n",
    "    \"\"\"\n",
    "    return embed_dim % num_heads == 0\n",
    "\n",
    "class BenchmarkParams:\n",
    "    def __init__(self, \n",
    "                 embed_dim=256,\n",
    "                 num_heads=8,\n",
    "                 num_random_features=128,\n",
    "                 batch_size=64, \n",
    "                 seq_length=32,\n",
    "                 num_runs=200, \n",
    "                 warmup=15, \n",
    "                 device=torch.device(\"cuda\"),\n",
    "                 dtype=torch.float32):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_random_features = num_random_features\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.num_runs = num_runs\n",
    "        self.warmup = warmup\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "def benchmark_model(model, inputs, model_name, params):\n",
    "    \"\"\"\n",
    "    Generic benchmarking function for any PyTorch model.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to benchmark\n",
    "        inputs: Dictionary of input tensors\n",
    "        model_name: Name of the model for logging\n",
    "        params: Benchmark parameters\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with benchmark results\n",
    "    \"\"\"\n",
    "    # Compile the model\n",
    "    # model_compiled = torch.compile(\n",
    "    #     model,\n",
    "    #     backend=\"inductor\",\n",
    "    #     fullgraph=True,\n",
    "    #     dynamic=False\n",
    "    # )\n",
    "    model_compiled = model\n",
    "    \n",
    "    # Benchmark forward pass\n",
    "    print(f\"\\n=== {model_name} FORWARD PASS BENCHMARK ===\")\n",
    "    \n",
    "    # Warmup runs for forward pass\n",
    "    model_compiled.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(params.warmup):\n",
    "            _ = model_compiled(**inputs)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Actual timed runs for forward\n",
    "    forward_times = []\n",
    "    forward_memories = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(params.num_runs):\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            _ = model_compiled(**inputs)\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "            \n",
    "            forward_times.append((end - start) * 1000)  # Convert to ms\n",
    "            forward_memories.append(torch.cuda.max_memory_allocated() / (1024 * 1024))  # Convert to MB\n",
    "    \n",
    "    mean_forward = np.mean(forward_times)\n",
    "    std_forward = np.std(forward_times)\n",
    "    mean_forward_memory = np.mean(forward_memories)\n",
    "    std_forward_memory = np.std(forward_memories)\n",
    "    print(f\"{model_name} forward: {mean_forward:.3f} ± {std_forward:.3f} ms, Memory: {mean_forward_memory:.2f} ± {std_forward_memory:.2f} MB\")\n",
    "    \n",
    "    # Benchmark backward pass\n",
    "    print(f\"\\n=== {model_name} BACKWARD PASS BENCHMARK ===\")\n",
    "    \n",
    "    # Get query for backward\n",
    "    query = inputs['query']\n",
    "    \n",
    "    # Warmup runs for backward pass\n",
    "    model_compiled.train()\n",
    "    for _ in range(params.warmup):\n",
    "        out = model_compiled(**inputs)[0]\n",
    "        loss = out.sum()\n",
    "        loss.backward()\n",
    "        query.grad.zero_()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Actual timed runs for backward\n",
    "    backward_times = []\n",
    "    backward_memories = []\n",
    "    for _ in range(params.num_runs):\n",
    "        out = model_compiled(**inputs)[0]\n",
    "        loss = out.sum()\n",
    "        \n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        loss.backward()\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        backward_times.append((end - start) * 1000)  # Convert to ms\n",
    "        backward_memories.append(torch.cuda.max_memory_allocated() / (1024 * 1024))  # Convert to MB\n",
    "        query.grad.zero_()\n",
    "    \n",
    "    mean_backward = np.mean(backward_times)\n",
    "    std_backward = np.std(backward_times)\n",
    "    mean_backward_memory = np.mean(backward_memories)\n",
    "    std_backward_memory = np.std(backward_memories)\n",
    "    print(f\"{model_name} backward: {mean_backward:.3f} ± {std_backward:.3f} ms, Memory: {mean_backward_memory:.2f} ± {std_backward_memory:.2f} MB\")\n",
    "    \n",
    "    return {\n",
    "        \"forward\": {\n",
    "            \"mean\": mean_forward,\n",
    "            \"std\": std_forward,\n",
    "            \"times\": forward_times,\n",
    "            \"memory_mb\": mean_forward_memory,\n",
    "            \"memory_std\": std_forward_memory,\n",
    "            \"memories\": forward_memories\n",
    "        },\n",
    "        \"backward\": {\n",
    "            \"mean\": mean_backward,\n",
    "            \"std\": std_backward,\n",
    "            \"times\": backward_times,\n",
    "            \"memory_mb\": mean_backward_memory,\n",
    "            \"memory_std\": std_backward_memory,\n",
    "            \"memories\": backward_memories\n",
    "        }\n",
    "    }\n",
    "\n",
    "def benchmark_model_factory(model_factory, model_name, params):\n",
    "    \"\"\"\n",
    "    Benchmark a model using a factory function.\n",
    "    \n",
    "    Args:\n",
    "        model_factory: Function that creates the model\n",
    "        model_name: Name of the model for logging\n",
    "        params: Benchmark parameters\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with benchmark results\n",
    "    \"\"\"\n",
    "    # Create the model\n",
    "    torch.manual_seed(42)\n",
    "    model = model_factory(params)\n",
    "    \n",
    "    # Create input tensors for benchmarking\n",
    "    query = torch.randn(params.batch_size, params.seq_length, params.embed_dim, \n",
    "                      dtype=params.dtype, device=params.device, requires_grad=True)\n",
    "    key = torch.randn(params.batch_size, params.seq_length, params.embed_dim, \n",
    "                     dtype=params.dtype, device=params.device)\n",
    "    value = torch.randn(params.batch_size, params.seq_length, params.embed_dim, \n",
    "                       dtype=params.dtype, device=params.device)\n",
    "    \n",
    "    inputs = {\n",
    "        'query': query,\n",
    "        'key': key,\n",
    "        'value': value,\n",
    "        # 'attention_mask': None\n",
    "    }\n",
    "    \n",
    "    return benchmark_model(model, inputs, model_name, params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import torch.nn as nn\n",
    "    from panther.nn.attention import RandMultiHeadAttention\n",
    "    \n",
    "    # Parameter combinations to test\n",
    "    embed_dims = [128, 256, 512, 1024]\n",
    "    num_heads_options = [4, 8, 16]\n",
    "    num_random_features_options = [64, 128, 256]\n",
    "    kernel_fn_options = [\"softmax\", \"relu\"]\n",
    "    causal_options = [False]\n",
    "    # causal_options = [False, True]\n",
    "    seq_lens = [512, 1024, 2048, 4096, 8192]\n",
    "    \n",
    "    # Define model factories\n",
    "    def create_attention(p):\n",
    "        return RandMultiHeadAttention(\n",
    "            embed_dim=p.embed_dim,\n",
    "            num_heads=p.num_heads,\n",
    "            num_random_features=p.num_random_features,\n",
    "            dropout=0.0,\n",
    "            kernel_fn=p.kernel_fn if hasattr(p, 'kernel_fn') else \"softmax\",\n",
    "            iscausal=p.iscausal if hasattr(p, 'iscausal') else False,\n",
    "            device=p.device,\n",
    "            dtype=p.dtype\n",
    "        )\n",
    "    \n",
    "    def create_torch_attention(p):\n",
    "        return torch.nn.MultiheadAttention(\n",
    "            embed_dim=p.embed_dim,\n",
    "            num_heads=p.num_heads,\n",
    "            dropout=0.0,\n",
    "            batch_first=True,  # Since your inputs are [batch, seq, dim]\n",
    "            device=p.device,\n",
    "            dtype=p.dtype\n",
    "        )\n",
    "    \n",
    "    models_to_benchmark = [\n",
    "        (create_torch_attention, \"attention\")\n",
    "    ]\n",
    "    \n",
    "    # Prepare data structure to store all results\n",
    "    results_data = []\n",
    "    \n",
    "    # Iterate through all parameter combinations\n",
    "    total_combinations = len(embed_dims) * len(num_heads_options) * len(num_random_features_options) * len(kernel_fn_options) * len(causal_options) * len(seq_lens)\n",
    "    current_combo = 0\n",
    "    \n",
    "    for embed_dim, num_heads, num_random_features, kernel_fn, iscausal, seq_length in itertools.product(\n",
    "        embed_dims, num_heads_options, num_random_features_options, kernel_fn_options, causal_options, seq_lens\n",
    "    ):\n",
    "        current_combo += 1\n",
    "        print(f\"\\n\\n{'='*20} COMBINATION {current_combo}/{total_combinations} {'='*20}\")\n",
    "        print(f\"Embed dimension: {embed_dim}, Num heads: {num_heads}, Num random features: {num_random_features}\")\n",
    "        print(f\"Kernel function: {kernel_fn}, Causal: {iscausal}, Sequence length: {seq_length}\")\n",
    "        \n",
    "        # Check if parameters are valid\n",
    "        is_valid = is_valid_params(embed_dim, num_heads, num_random_features)\n",
    "        \n",
    "        if not is_valid:\n",
    "            print(f\"INVALID COMBINATION: {embed_dim} is not divisible by {num_heads}\")\n",
    "            print(\"Skipping benchmarks for this invalid combination\")\n",
    "            \n",
    "            # Add invalid entry to results data\n",
    "            for model_name in [m[1] for m in models_to_benchmark]:\n",
    "                results_data.append({\n",
    "                    'model': model_name,\n",
    "                    'embed_dim': embed_dim,\n",
    "                    'num_heads': num_heads,\n",
    "                    'num_random_features': num_random_features,\n",
    "                    'kernel_fn': kernel_fn,\n",
    "                    'iscausal': iscausal,\n",
    "                    'seq_length': seq_length,\n",
    "                    'forward_mean_ms': float('nan'),\n",
    "                    'forward_std_ms': float('nan'),\n",
    "                    'backward_mean_ms': float('nan'),\n",
    "                    'backward_std_ms': float('nan'),\n",
    "                    'forward_memory_mb': float('nan'),\n",
    "                    'backward_memory_mb': float('nan'),\n",
    "                    'is_valid': False,\n",
    "                    'error': \"Invalid parameter combination\"\n",
    "                })\n",
    "            continue\n",
    "        \n",
    "        # Create parameter object for this combination\n",
    "        params = BenchmarkParams(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_random_features=num_random_features,\n",
    "            seq_length=seq_length\n",
    "        )\n",
    "        # Add the new parameters\n",
    "        params.kernel_fn = kernel_fn\n",
    "        params.iscausal = iscausal\n",
    "        \n",
    "        all_results = {}\n",
    "        for model_factory, model_name in models_to_benchmark:\n",
    "            print(f\"\\n{'='*20} Benchmarking {model_name} {'='*20}\")\n",
    "            try:\n",
    "                results = benchmark_model_factory(model_factory, model_name, params)\n",
    "                all_results[model_name] = results\n",
    "                \n",
    "                # Add result to our data collection\n",
    "                results_data.append({\n",
    "                    'model': model_name,\n",
    "                    'embed_dim': embed_dim,\n",
    "                    'num_heads': num_heads,\n",
    "                    'num_random_features': num_random_features,\n",
    "                    'kernel_fn': kernel_fn,\n",
    "                    'iscausal': iscausal,\n",
    "                    'seq_length': seq_length,\n",
    "                    'forward_mean_ms': results['forward']['mean'],\n",
    "                    'forward_std_ms': results['forward']['std'],\n",
    "                    'backward_mean_ms': results['backward']['mean'],\n",
    "                    'backward_std_ms': results['backward']['std'],\n",
    "                    'forward_memory_mb': results['forward']['memory_mb'],\n",
    "                    'backward_memory_mb': results['backward']['memory_mb'],\n",
    "                    'is_valid': True\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error benchmarking {model_name}: {e}\")\n",
    "                # Add error entry to data\n",
    "                results_data.append({\n",
    "                    'model': model_name,\n",
    "                    'embed_dim': embed_dim,\n",
    "                    'num_heads': num_heads,\n",
    "                    'num_random_features': num_random_features,\n",
    "                    'kernel_fn': kernel_fn, \n",
    "                    'iscausal': iscausal,\n",
    "                    'seq_length': seq_length,\n",
    "                    'forward_mean_ms': float('nan'),\n",
    "                    'forward_std_ms': float('nan'),\n",
    "                    'backward_mean_ms': float('nan'),\n",
    "                    'backward_std_ms': float('nan'),\n",
    "                    'forward_memory_mb': float('nan'),\n",
    "                    'backward_memory_mb': float('nan'),\n",
    "                    'is_valid': True,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        # Print comparative summary for this combination\n",
    "        if all_results:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(f\"{'='*20} SUMMARY FOR CURRENT COMBINATION {'='*20}\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"{'Model':<30} {'Forward (ms)':<25} {'Backward (ms)':<25} {'Forward Memory (MB)':<25} {'Backward Memory (MB)':<25}\")\n",
    "            print(\"-\"*60)\n",
    "            \n",
    "            for model_name, results in all_results.items():\n",
    "                fwd = f\"{results['forward']['mean']:.3f} ± {results['forward']['std']:.3f}\"\n",
    "                bwd = f\"{results['backward']['mean']:.3f} ± {results['backward']['std']:.3f}\"\n",
    "                fwd_mem = f\"{results['forward']['memory_mb']:.2f}\"\n",
    "                bwd_mem = f\"{results['backward']['memory_mb']:.2f}\"\n",
    "                print(f\"{model_name:<30} {fwd:<25} {bwd:<25} {fwd_mem:<25} {bwd_mem:<25}\")\n",
    "    \n",
    "    # Create a DataFrame with all results\n",
    "    df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_file = \"attention_benchmark_results.csv\"\n",
    "    df.to_csv(results_file, index=False)\n",
    "    print(f\"\\nAll benchmark results saved to {results_file}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10835.686765,
   "end_time": "2025-05-07T23:21:13.322332",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-07T20:20:37.635567",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
