{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a525d682",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-13T20:13:48.340489Z",
     "iopub.status.busy": "2025-05-13T20:13:48.340228Z",
     "iopub.status.idle": "2025-05-13T20:13:48.447499Z",
     "shell.execute_reply": "2025-05-13T20:13:48.446705Z"
    },
    "papermill": {
     "duration": 0.112898,
     "end_time": "2025-05-13T20:13:48.448901",
     "exception": false,
     "start_time": "2025-05-13T20:13:48.336003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "token = user_secrets.get_secret(\"github_repos_wildcard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95023696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T20:13:48.457028Z",
     "iopub.status.busy": "2025-05-13T20:13:48.456826Z",
     "iopub.status.idle": "2025-05-13T20:13:48.460489Z",
     "shell.execute_reply": "2025-05-13T20:13:48.459822Z"
    },
    "papermill": {
     "duration": 0.008939,
     "end_time": "2025-05-13T20:13:48.461560",
     "exception": false,
     "start_time": "2025-05-13T20:13:48.452621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_url = f\"https://{token}@github.com/gaserSami/panther.git\"\n",
    "branch = \"pawX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e9109e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T20:13:48.467632Z",
     "iopub.status.busy": "2025-05-13T20:13:48.467404Z",
     "iopub.status.idle": "2025-05-13T20:13:51.877707Z",
     "shell.execute_reply": "2025-05-13T20:13:51.876947Z"
    },
    "papermill": {
     "duration": 3.415126,
     "end_time": "2025-05-13T20:13:51.879302",
     "exception": false,
     "start_time": "2025-05-13T20:13:48.464176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'panther'...\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 1473, done.\u001b[K\r\n",
      "remote: Counting objects:   0% (1/378)\u001b[K\r",
      "remote: Counting objects:   1% (4/378)\u001b[K\r",
      "remote: Counting objects:   2% (8/378)\u001b[K\r",
      "remote: Counting objects:   3% (12/378)\u001b[K\r",
      "remote: Counting objects:   4% (16/378)\u001b[K\r",
      "remote: Counting objects:   5% (19/378)\u001b[K\r",
      "remote: Counting objects:   6% (23/378)\u001b[K\r",
      "remote: Counting objects:   7% (27/378)\u001b[K\r",
      "remote: Counting objects:   8% (31/378)\u001b[K\r",
      "remote: Counting objects:   9% (35/378)\u001b[K\r",
      "remote: Counting objects:  10% (38/378)\u001b[K\r",
      "remote: Counting objects:  11% (42/378)\u001b[K\r",
      "remote: Counting objects:  12% (46/378)\u001b[K\r",
      "remote: Counting objects:  13% (50/378)\u001b[K\r",
      "remote: Counting objects:  14% (53/378)\u001b[K\r",
      "remote: Counting objects:  15% (57/378)\u001b[K\r",
      "remote: Counting objects:  16% (61/378)\u001b[K\r",
      "remote: Counting objects:  17% (65/378)\u001b[K\r",
      "remote: Counting objects:  18% (69/378)\u001b[K\r",
      "remote: Counting objects:  19% (72/378)\u001b[K\r",
      "remote: Counting objects:  20% (76/378)\u001b[K\r",
      "remote: Counting objects:  21% (80/378)\u001b[K\r",
      "remote: Counting objects:  22% (84/378)\u001b[K\r",
      "remote: Counting objects:  23% (87/378)\u001b[K\r",
      "remote: Counting objects:  24% (91/378)\u001b[K\r",
      "remote: Counting objects:  25% (95/378)\u001b[K\r",
      "remote: Counting objects:  26% (99/378)\u001b[K\r",
      "remote: Counting objects:  27% (103/378)\u001b[K\r",
      "remote: Counting objects:  28% (106/378)\u001b[K\r",
      "remote: Counting objects:  29% (110/378)\u001b[K\r",
      "remote: Counting objects:  30% (114/378)\u001b[K\r",
      "remote: Counting objects:  31% (118/378)\u001b[K\r",
      "remote: Counting objects:  32% (121/378)\u001b[K\r",
      "remote: Counting objects:  33% (125/378)\u001b[K\r",
      "remote: Counting objects:  34% (129/378)\u001b[K\r",
      "remote: Counting objects:  35% (133/378)\u001b[K\r",
      "remote: Counting objects:  36% (137/378)\u001b[K\r",
      "remote: Counting objects:  37% (140/378)\u001b[K\r",
      "remote: Counting objects:  38% (144/378)\u001b[K\r",
      "remote: Counting objects:  39% (148/378)\u001b[K\r",
      "remote: Counting objects:  40% (152/378)\u001b[K\r",
      "remote: Counting objects:  41% (155/378)\u001b[K\r",
      "remote: Counting objects:  42% (159/378)\u001b[K\r",
      "remote: Counting objects:  43% (163/378)\u001b[K\r",
      "remote: Counting objects:  44% (167/378)\u001b[K\r",
      "remote: Counting objects:  45% (171/378)\u001b[K\r",
      "remote: Counting objects:  46% (174/378)\u001b[K\r",
      "remote: Counting objects:  47% (178/378)\u001b[K\r",
      "remote: Counting objects:  48% (182/378)\u001b[K\r",
      "remote: Counting objects:  49% (186/378)\u001b[K\r",
      "remote: Counting objects:  50% (189/378)\u001b[K\r",
      "remote: Counting objects:  51% (193/378)\u001b[K\r",
      "remote: Counting objects:  52% (197/378)\u001b[K\r",
      "remote: Counting objects:  53% (201/378)\u001b[K\r",
      "remote: Counting objects:  54% (205/378)\u001b[K\r",
      "remote: Counting objects:  55% (208/378)\u001b[K\r",
      "remote: Counting objects:  56% (212/378)\u001b[K\r",
      "remote: Counting objects:  57% (216/378)\u001b[K\r",
      "remote: Counting objects:  58% (220/378)\u001b[K\r",
      "remote: Counting objects:  59% (224/378)\u001b[K\r",
      "remote: Counting objects:  60% (227/378)\u001b[K\r",
      "remote: Counting objects:  61% (231/378)\u001b[K\r",
      "remote: Counting objects:  62% (235/378)\u001b[K\r",
      "remote: Counting objects:  63% (239/378)\u001b[K\r",
      "remote: Counting objects:  64% (242/378)\u001b[K\r",
      "remote: Counting objects:  65% (246/378)\u001b[K\r",
      "remote: Counting objects:  66% (250/378)\u001b[K\r",
      "remote: Counting objects:  67% (254/378)\u001b[K\r",
      "remote: Counting objects:  68% (258/378)\u001b[K\r",
      "remote: Counting objects:  69% (261/378)\u001b[K\r",
      "remote: Counting objects:  70% (265/378)\u001b[K\r",
      "remote: Counting objects:  71% (269/378)\u001b[K\r",
      "remote: Counting objects:  72% (273/378)\u001b[K\r",
      "remote: Counting objects:  73% (276/378)\u001b[K\r",
      "remote: Counting objects:  74% (280/378)\u001b[K\r",
      "remote: Counting objects:  75% (284/378)\u001b[K\r",
      "remote: Counting objects:  76% (288/378)\u001b[K\r",
      "remote: Counting objects:  77% (292/378)\u001b[K\r",
      "remote: Counting objects:  78% (295/378)\u001b[K\r",
      "remote: Counting objects:  79% (299/378)\u001b[K\r",
      "remote: Counting objects:  80% (303/378)\u001b[K\r",
      "remote: Counting objects:  81% (307/378)\u001b[K\r",
      "remote: Counting objects:  82% (310/378)\u001b[K\r",
      "remote: Counting objects:  83% (314/378)\u001b[K\r",
      "remote: Counting objects:  84% (318/378)\u001b[K\r",
      "remote: Counting objects:  85% (322/378)\u001b[K\r",
      "remote: Counting objects:  86% (326/378)\u001b[K\r",
      "remote: Counting objects:  87% (329/378)\u001b[K\r",
      "remote: Counting objects:  88% (333/378)\u001b[K\r",
      "remote: Counting objects:  89% (337/378)\u001b[K\r",
      "remote: Counting objects:  90% (341/378)\u001b[K\r",
      "remote: Counting objects:  91% (344/378)\u001b[K\r",
      "remote: Counting objects:  92% (348/378)\u001b[K\r",
      "remote: Counting objects:  93% (352/378)\u001b[K\r",
      "remote: Counting objects:  94% (356/378)\u001b[K\r",
      "remote: Counting objects:  95% (360/378)\u001b[K\r",
      "remote: Counting objects:  96% (363/378)\u001b[K\r",
      "remote: Counting objects:  97% (367/378)\u001b[K\r",
      "remote: Counting objects:  98% (371/378)\u001b[K\r",
      "remote: Counting objects:  99% (375/378)\u001b[K\r",
      "remote: Counting objects: 100% (378/378)\u001b[K\r",
      "remote: Counting objects: 100% (378/378), done.\u001b[K\r\n",
      "remote: Compressing objects:   0% (1/111)\u001b[K\r",
      "remote: Compressing objects:   1% (2/111)\u001b[K\r",
      "remote: Compressing objects:   2% (3/111)\u001b[K\r",
      "remote: Compressing objects:   3% (4/111)\u001b[K\r",
      "remote: Compressing objects:   4% (5/111)\u001b[K\r",
      "remote: Compressing objects:   5% (6/111)\u001b[K\r",
      "remote: Compressing objects:   6% (7/111)\u001b[K\r",
      "remote: Compressing objects:   7% (8/111)\u001b[K\r",
      "remote: Compressing objects:   8% (9/111)\u001b[K\r",
      "remote: Compressing objects:   9% (10/111)\u001b[K\r",
      "remote: Compressing objects:  10% (12/111)\u001b[K\r",
      "remote: Compressing objects:  11% (13/111)\u001b[K\r",
      "remote: Compressing objects:  12% (14/111)\u001b[K\r",
      "remote: Compressing objects:  13% (15/111)\u001b[K\r",
      "remote: Compressing objects:  14% (16/111)\u001b[K\r",
      "remote: Compressing objects:  15% (17/111)\u001b[K\r",
      "remote: Compressing objects:  16% (18/111)\u001b[K\r",
      "remote: Compressing objects:  17% (19/111)\u001b[K\r",
      "remote: Compressing objects:  18% (20/111)\u001b[K\r",
      "remote: Compressing objects:  19% (22/111)\u001b[K\r",
      "remote: Compressing objects:  20% (23/111)\u001b[K\r",
      "remote: Compressing objects:  21% (24/111)\u001b[K\r",
      "remote: Compressing objects:  22% (25/111)\u001b[K\r",
      "remote: Compressing objects:  23% (26/111)\u001b[K\r",
      "remote: Compressing objects:  24% (27/111)\u001b[K\r",
      "remote: Compressing objects:  25% (28/111)\u001b[K\r",
      "remote: Compressing objects:  26% (29/111)\u001b[K\r",
      "remote: Compressing objects:  27% (30/111)\u001b[K\r",
      "remote: Compressing objects:  28% (32/111)\u001b[K\r",
      "remote: Compressing objects:  29% (33/111)\u001b[K\r",
      "remote: Compressing objects:  30% (34/111)\u001b[K\r",
      "remote: Compressing objects:  31% (35/111)\u001b[K\r",
      "remote: Compressing objects:  32% (36/111)\u001b[K\r",
      "remote: Compressing objects:  33% (37/111)\u001b[K\r",
      "remote: Compressing objects:  34% (38/111)\u001b[K\r",
      "remote: Compressing objects:  35% (39/111)\u001b[K\r",
      "remote: Compressing objects:  36% (40/111)\u001b[K\r",
      "remote: Compressing objects:  37% (42/111)\u001b[K\r",
      "remote: Compressing objects:  38% (43/111)\u001b[K\r",
      "remote: Compressing objects:  39% (44/111)\u001b[K\r",
      "remote: Compressing objects:  40% (45/111)\u001b[K\r",
      "remote: Compressing objects:  41% (46/111)\u001b[K\r",
      "remote: Compressing objects:  42% (47/111)\u001b[K\r",
      "remote: Compressing objects:  43% (48/111)\u001b[K\r",
      "remote: Compressing objects:  44% (49/111)\u001b[K\r",
      "remote: Compressing objects:  45% (50/111)\u001b[K\r",
      "remote: Compressing objects:  46% (52/111)\u001b[K\r",
      "remote: Compressing objects:  47% (53/111)\u001b[K\r",
      "remote: Compressing objects:  48% (54/111)\u001b[K\r",
      "remote: Compressing objects:  49% (55/111)\u001b[K\r",
      "remote: Compressing objects:  50% (56/111)\u001b[K\r",
      "remote: Compressing objects:  51% (57/111)\u001b[K\r",
      "remote: Compressing objects:  52% (58/111)\u001b[K\r",
      "remote: Compressing objects:  53% (59/111)\u001b[K\r",
      "remote: Compressing objects:  54% (60/111)\u001b[K\r",
      "remote: Compressing objects:  55% (62/111)\u001b[K\r",
      "remote: Compressing objects:  56% (63/111)\u001b[K\r",
      "remote: Compressing objects:  57% (64/111)\u001b[K\r",
      "remote: Compressing objects:  58% (65/111)\u001b[K\r",
      "remote: Compressing objects:  59% (66/111)\u001b[K\r",
      "remote: Compressing objects:  60% (67/111)\u001b[K\r",
      "remote: Compressing objects:  61% (68/111)\u001b[K\r",
      "remote: Compressing objects:  62% (69/111)\u001b[K\r",
      "remote: Compressing objects:  63% (70/111)\u001b[K\r",
      "remote: Compressing objects:  64% (72/111)\u001b[K\r",
      "remote: Compressing objects:  65% (73/111)\u001b[K\r",
      "remote: Compressing objects:  66% (74/111)\u001b[K\r",
      "remote: Compressing objects:  67% (75/111)\u001b[K\r",
      "remote: Compressing objects:  68% (76/111)\u001b[K\r",
      "remote: Compressing objects:  69% (77/111)\u001b[K\r",
      "remote: Compressing objects:  70% (78/111)\u001b[K\r",
      "remote: Compressing objects:  71% (79/111)\u001b[K\r",
      "remote: Compressing objects:  72% (80/111)\u001b[K\r",
      "remote: Compressing objects:  73% (82/111)\u001b[K\r",
      "remote: Compressing objects:  74% (83/111)\u001b[K\r",
      "remote: Compressing objects:  75% (84/111)\u001b[K\r",
      "remote: Compressing objects:  76% (85/111)\u001b[K\r",
      "remote: Compressing objects:  77% (86/111)\u001b[K\r",
      "remote: Compressing objects:  78% (87/111)\u001b[K\r",
      "remote: Compressing objects:  79% (88/111)\u001b[K\r",
      "remote: Compressing objects:  80% (89/111)\u001b[K\r",
      "remote: Compressing objects:  81% (90/111)\u001b[K\r",
      "remote: Compressing objects:  82% (92/111)\u001b[K\r",
      "remote: Compressing objects:  83% (93/111)\u001b[K\r",
      "remote: Compressing objects:  84% (94/111)\u001b[K\r",
      "remote: Compressing objects:  85% (95/111)\u001b[K\r",
      "remote: Compressing objects:  86% (96/111)\u001b[K\r",
      "remote: Compressing objects:  87% (97/111)\u001b[K\r",
      "remote: Compressing objects:  88% (98/111)\u001b[K\r",
      "remote: Compressing objects:  89% (99/111)\u001b[K\r",
      "remote: Compressing objects:  90% (100/111)\u001b[K\r",
      "remote: Compressing objects:  91% (102/111)\u001b[K\r",
      "remote: Compressing objects:  92% (103/111)\u001b[K\r",
      "remote: Compressing objects:  93% (104/111)\u001b[K\r",
      "remote: Compressing objects:  94% (105/111)\u001b[K\r",
      "remote: Compressing objects:  95% (106/111)\u001b[K\r",
      "remote: Compressing objects:  96% (107/111)\u001b[K\r",
      "remote: Compressing objects:  97% (108/111)\u001b[K\r",
      "remote: Compressing objects:  98% (109/111)\u001b[K\r",
      "remote: Compressing objects:  99% (110/111)\u001b[K\r",
      "remote: Compressing objects: 100% (111/111)\u001b[K\r",
      "remote: Compressing objects: 100% (111/111), done.\u001b[K\r\n",
      "Receiving objects:   0% (1/1473)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   1% (15/1473)\r",
      "Receiving objects:   2% (30/1473)\r",
      "Receiving objects:   3% (45/1473)\r",
      "Receiving objects:   4% (59/1473)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   5% (74/1473)\r",
      "Receiving objects:   6% (89/1473)\r",
      "Receiving objects:   7% (104/1473)\r",
      "Receiving objects:   8% (118/1473)\r",
      "Receiving objects:   9% (133/1473)\r",
      "Receiving objects:  10% (148/1473)\r",
      "Receiving objects:  11% (163/1473)\r",
      "Receiving objects:  12% (177/1473)\r",
      "Receiving objects:  13% (192/1473)\r",
      "Receiving objects:  14% (207/1473)\r",
      "Receiving objects:  15% (221/1473)\r",
      "Receiving objects:  16% (236/1473)\r",
      "Receiving objects:  17% (251/1473)\r",
      "Receiving objects:  18% (266/1473)\r",
      "Receiving objects:  19% (280/1473)\r",
      "Receiving objects:  20% (295/1473)\r",
      "Receiving objects:  21% (310/1473)\r",
      "Receiving objects:  22% (325/1473)\r",
      "Receiving objects:  23% (339/1473)\r",
      "Receiving objects:  24% (354/1473)\r",
      "Receiving objects:  25% (369/1473)\r",
      "Receiving objects:  26% (383/1473)\r",
      "Receiving objects:  27% (398/1473)\r",
      "Receiving objects:  28% (413/1473)\r",
      "Receiving objects:  29% (428/1473)\r",
      "Receiving objects:  30% (442/1473)\r",
      "Receiving objects:  31% (457/1473)\r",
      "Receiving objects:  32% (472/1473)\r",
      "Receiving objects:  33% (487/1473)\r",
      "Receiving objects:  34% (501/1473)\r",
      "Receiving objects:  35% (516/1473)\r",
      "Receiving objects:  36% (531/1473)\r",
      "Receiving objects:  37% (546/1473)\r",
      "Receiving objects:  38% (560/1473)\r",
      "Receiving objects:  39% (575/1473)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  40% (590/1473), 9.89 MiB | 19.76 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  40% (603/1473), 19.60 MiB | 19.59 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  41% (604/1473), 19.60 MiB | 19.59 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  42% (619/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  43% (634/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  44% (649/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  45% (663/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  46% (678/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  47% (693/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  48% (708/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  49% (722/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  50% (737/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  51% (752/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  52% (766/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  53% (781/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  54% (796/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  55% (811/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  56% (825/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  57% (840/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  58% (855/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  59% (870/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  60% (884/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  61% (899/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  62% (914/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  63% (928/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  64% (943/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  65% (958/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  66% (973/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  67% (987/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  68% (1002/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  69% (1017/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  70% (1032/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  71% (1046/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  72% (1061/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  73% (1076/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  74% (1091/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  75% (1105/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  76% (1120/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  77% (1135/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  78% (1149/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  79% (1164/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  80% (1179/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  81% (1194/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  82% (1208/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  83% (1223/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  84% (1238/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  85% (1253/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  86% (1267/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  87% (1282/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  88% (1297/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  89% (1311/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  90% (1326/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  91% (1341/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  92% (1356/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  93% (1370/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  94% (1385/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  95% (1400/1473), 19.60 MiB | 19.59 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Total 1473 (delta 307), reused 306 (delta 267), pack-reused 1095 (from 1)\u001b[K\r\n",
      "Receiving objects:  96% (1415/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  97% (1429/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  98% (1444/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects:  99% (1459/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects: 100% (1473/1473), 19.60 MiB | 19.59 MiB/s\r",
      "Receiving objects: 100% (1473/1473), 31.55 MiB | 21.65 MiB/s, done.\r\n",
      "Resolving deltas:   0% (0/957)\r",
      "Resolving deltas:   1% (10/957)\r",
      "Resolving deltas:   2% (20/957)\r",
      "Resolving deltas:   3% (29/957)\r",
      "Resolving deltas:   4% (39/957)\r",
      "Resolving deltas:   5% (48/957)\r",
      "Resolving deltas:   6% (58/957)\r",
      "Resolving deltas:   7% (67/957)\r",
      "Resolving deltas:   8% (77/957)\r",
      "Resolving deltas:   9% (87/957)\r",
      "Resolving deltas:  10% (96/957)\r",
      "Resolving deltas:  11% (106/957)\r",
      "Resolving deltas:  12% (115/957)\r",
      "Resolving deltas:  13% (125/957)\r",
      "Resolving deltas:  14% (134/957)\r",
      "Resolving deltas:  15% (144/957)\r",
      "Resolving deltas:  16% (154/957)\r",
      "Resolving deltas:  17% (163/957)\r",
      "Resolving deltas:  18% (173/957)\r",
      "Resolving deltas:  19% (182/957)\r",
      "Resolving deltas:  20% (192/957)\r",
      "Resolving deltas:  21% (201/957)\r",
      "Resolving deltas:  22% (211/957)\r",
      "Resolving deltas:  23% (221/957)\r",
      "Resolving deltas:  24% (230/957)\r",
      "Resolving deltas:  25% (240/957)\r",
      "Resolving deltas:  26% (249/957)\r",
      "Resolving deltas:  27% (259/957)\r",
      "Resolving deltas:  28% (268/957)\r",
      "Resolving deltas:  29% (278/957)\r",
      "Resolving deltas:  30% (288/957)\r",
      "Resolving deltas:  31% (297/957)\r",
      "Resolving deltas:  32% (307/957)\r",
      "Resolving deltas:  33% (316/957)\r",
      "Resolving deltas:  34% (326/957)\r",
      "Resolving deltas:  35% (335/957)\r",
      "Resolving deltas:  36% (345/957)\r",
      "Resolving deltas:  37% (355/957)\r",
      "Resolving deltas:  38% (364/957)\r",
      "Resolving deltas:  39% (374/957)\r",
      "Resolving deltas:  40% (383/957)\r",
      "Resolving deltas:  41% (393/957)\r",
      "Resolving deltas:  42% (402/957)\r",
      "Resolving deltas:  43% (412/957)\r",
      "Resolving deltas:  44% (422/957)\r",
      "Resolving deltas:  45% (431/957)\r",
      "Resolving deltas:  46% (441/957)\r",
      "Resolving deltas:  47% (450/957)\r",
      "Resolving deltas:  48% (460/957)\r",
      "Resolving deltas:  49% (469/957)\r",
      "Resolving deltas:  50% (479/957)\r",
      "Resolving deltas:  51% (489/957)\r",
      "Resolving deltas:  52% (498/957)\r",
      "Resolving deltas:  53% (508/957)\r",
      "Resolving deltas:  54% (517/957)\r",
      "Resolving deltas:  55% (527/957)\r",
      "Resolving deltas:  56% (536/957)\r",
      "Resolving deltas:  57% (546/957)\r",
      "Resolving deltas:  58% (556/957)\r",
      "Resolving deltas:  59% (565/957)\r",
      "Resolving deltas:  60% (575/957)\r",
      "Resolving deltas:  61% (584/957)\r",
      "Resolving deltas:  62% (594/957)\r",
      "Resolving deltas:  63% (603/957)\r",
      "Resolving deltas:  64% (613/957)\r",
      "Resolving deltas:  65% (623/957)\r",
      "Resolving deltas:  66% (632/957)\r",
      "Resolving deltas:  67% (642/957)\r",
      "Resolving deltas:  68% (651/957)\r",
      "Resolving deltas:  69% (661/957)\r",
      "Resolving deltas:  70% (670/957)\r",
      "Resolving deltas:  71% (680/957)\r",
      "Resolving deltas:  72% (690/957)\r",
      "Resolving deltas:  73% (699/957)\r",
      "Resolving deltas:  74% (709/957)\r",
      "Resolving deltas:  75% (718/957)\r",
      "Resolving deltas:  76% (728/957)\r",
      "Resolving deltas:  77% (737/957)\r",
      "Resolving deltas:  78% (747/957)\r",
      "Resolving deltas:  79% (757/957)\r",
      "Resolving deltas:  80% (766/957)\r",
      "Resolving deltas:  81% (776/957)\r",
      "Resolving deltas:  82% (785/957)\r",
      "Resolving deltas:  83% (795/957)\r",
      "Resolving deltas:  84% (804/957)\r",
      "Resolving deltas:  85% (814/957)\r",
      "Resolving deltas:  86% (824/957)\r",
      "Resolving deltas:  87% (833/957)\r",
      "Resolving deltas:  88% (843/957)\r",
      "Resolving deltas:  89% (852/957)\r",
      "Resolving deltas:  90% (862/957)\r",
      "Resolving deltas:  91% (871/957)\r",
      "Resolving deltas:  92% (881/957)\r",
      "Resolving deltas:  93% (891/957)\r",
      "Resolving deltas:  94% (900/957)\r",
      "Resolving deltas:  95% (910/957)\r",
      "Resolving deltas:  96% (919/957)\r",
      "Resolving deltas:  97% (929/957)\r",
      "Resolving deltas:  98% (938/957)\r",
      "Resolving deltas:  99% (948/957)\r",
      "Resolving deltas: 100% (957/957)\r",
      "Resolving deltas: 100% (957/957), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone -b {branch} {repo_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f369fc7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T20:13:51.887717Z",
     "iopub.status.busy": "2025-05-13T20:13:51.887494Z",
     "iopub.status.idle": "2025-05-13T20:13:52.006297Z",
     "shell.execute_reply": "2025-05-13T20:13:52.005419Z"
    },
    "papermill": {
     "duration": 0.124395,
     "end_time": "2025-05-13T20:13:52.007705",
     "exception": false,
     "start_time": "2025-05-13T20:13:51.883310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv panther Panther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f56934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T20:13:52.016077Z",
     "iopub.status.busy": "2025-05-13T20:13:52.015574Z",
     "iopub.status.idle": "2025-05-13T20:16:17.256060Z",
     "shell.execute_reply": "2025-05-13T20:16:17.255321Z"
    },
    "papermill": {
     "duration": 145.246114,
     "end_time": "2025-05-13T20:16:17.257562",
     "exception": false,
     "start_time": "2025-05-13T20:13:52.011448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.5.1+cu124\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling torch-2.5.1+cu124:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully uninstalled torch-2.5.1+cu124\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torchvision 0.20.1+cu124\r\n",
      "Uninstalling torchvision-0.20.1+cu124:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully uninstalled torchvision-0.20.1+cu124\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torchaudio 2.5.1+cu124\r\n",
      "Uninstalling torchaudio-2.5.1+cu124:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully uninstalled torchaudio-2.5.1+cu124\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.6.0+cu124\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (28 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision==0.21.0+cu124\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchaudio==2.6.0+cu124\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (4.13.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/664.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/664.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:31\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/664.8 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/664.8 MB\u001b[0m \u001b[31m155.9 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.5/664.8 MB\u001b[0m \u001b[31m187.7 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/664.8 MB\u001b[0m \u001b[31m187.1 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r",
      "\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/664.8 MB\u001b[0m \u001b[31m202.6 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.5/664.8 MB\u001b[0m \u001b[31m206.7 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/664.8 MB\u001b[0m \u001b[31m211.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/664.8 MB\u001b[0m \u001b[31m209.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/664.8 MB\u001b[0m \u001b[31m205.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.1/664.8 MB\u001b[0m \u001b[31m188.0 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/664.8 MB\u001b[0m \u001b[31m175.8 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/664.8 MB\u001b[0m \u001b[31m191.0 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.3/664.8 MB\u001b[0m \u001b[31m194.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/664.8 MB\u001b[0m \u001b[31m194.7 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.5/664.8 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/664.8 MB\u001b[0m \u001b[31m208.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.0/664.8 MB\u001b[0m \u001b[31m212.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.2/664.8 MB\u001b[0m \u001b[31m205.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/664.8 MB\u001b[0m \u001b[31m208.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.6/664.8 MB\u001b[0m \u001b[31m208.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/664.8 MB\u001b[0m \u001b[31m204.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/664.8 MB\u001b[0m \u001b[31m198.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.5/664.8 MB\u001b[0m \u001b[31m197.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.5/664.8 MB\u001b[0m \u001b[31m200.7 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.7/664.8 MB\u001b[0m \u001b[31m204.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.8/664.8 MB\u001b[0m \u001b[31m206.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.1/664.8 MB\u001b[0m \u001b[31m208.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/664.8 MB\u001b[0m \u001b[31m207.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.4/664.8 MB\u001b[0m \u001b[31m205.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/664.8 MB\u001b[0m \u001b[31m204.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.9/664.8 MB\u001b[0m \u001b[31m209.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.0/664.8 MB\u001b[0m \u001b[31m209.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.1/664.8 MB\u001b[0m \u001b[31m206.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.5/664.8 MB\u001b[0m \u001b[31m207.7 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.7/664.8 MB\u001b[0m \u001b[31m209.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.7/664.8 MB\u001b[0m \u001b[31m206.7 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.0/664.8 MB\u001b[0m \u001b[31m207.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.1/664.8 MB\u001b[0m \u001b[31m206.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.0/664.8 MB\u001b[0m \u001b[31m204.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.4/664.8 MB\u001b[0m \u001b[31m203.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.7/664.8 MB\u001b[0m \u001b[31m210.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.9/664.8 MB\u001b[0m \u001b[31m210.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.3/664.8 MB\u001b[0m \u001b[31m213.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.7/664.8 MB\u001b[0m \u001b[31m211.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/664.8 MB\u001b[0m \u001b[31m209.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.0/664.8 MB\u001b[0m \u001b[31m209.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.4/664.8 MB\u001b[0m \u001b[31m210.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.6/664.8 MB\u001b[0m \u001b[31m209.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.6/664.8 MB\u001b[0m \u001b[31m206.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.0/664.8 MB\u001b[0m \u001b[31m209.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.1/664.8 MB\u001b[0m \u001b[31m207.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.1/664.8 MB\u001b[0m \u001b[31m204.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.0/664.8 MB\u001b[0m \u001b[31m200.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/664.8 MB\u001b[0m \u001b[31m203.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.2/664.8 MB\u001b[0m \u001b[31m206.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.3/664.8 MB\u001b[0m \u001b[31m204.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.6/664.8 MB\u001b[0m \u001b[31m210.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.8/664.8 MB\u001b[0m \u001b[31m210.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.9/664.8 MB\u001b[0m \u001b[31m209.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.0/664.8 MB\u001b[0m \u001b[31m203.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m430.2/664.8 MB\u001b[0m \u001b[31m205.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m437.4/664.8 MB\u001b[0m \u001b[31m209.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m444.7/664.8 MB\u001b[0m \u001b[31m206.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m451.7/664.8 MB\u001b[0m \u001b[31m204.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m457.2/664.8 MB\u001b[0m \u001b[31m178.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m463.2/664.8 MB\u001b[0m \u001b[31m160.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m470.1/664.8 MB\u001b[0m \u001b[31m183.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m477.1/664.8 MB\u001b[0m \u001b[31m204.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m483.4/664.8 MB\u001b[0m \u001b[31m190.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m488.3/664.8 MB\u001b[0m \u001b[31m155.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m494.4/664.8 MB\u001b[0m \u001b[31m161.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m501.4/664.8 MB\u001b[0m \u001b[31m206.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m508.4/664.8 MB\u001b[0m \u001b[31m199.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m515.2/664.8 MB\u001b[0m \u001b[31m199.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m522.0/664.8 MB\u001b[0m \u001b[31m197.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m528.9/664.8 MB\u001b[0m \u001b[31m197.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m535.6/664.8 MB\u001b[0m \u001b[31m195.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m542.4/664.8 MB\u001b[0m \u001b[31m199.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m548.0/664.8 MB\u001b[0m \u001b[31m175.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m553.6/664.8 MB\u001b[0m \u001b[31m162.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m559.2/664.8 MB\u001b[0m \u001b[31m166.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m564.9/664.8 MB\u001b[0m \u001b[31m164.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m570.5/664.8 MB\u001b[0m \u001b[31m164.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m576.1/664.8 MB\u001b[0m \u001b[31m164.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m581.9/664.8 MB\u001b[0m \u001b[31m165.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m589.1/664.8 MB\u001b[0m \u001b[31m191.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m596.2/664.8 MB\u001b[0m \u001b[31m206.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m603.3/664.8 MB\u001b[0m \u001b[31m207.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m610.6/664.8 MB\u001b[0m \u001b[31m207.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m617.6/664.8 MB\u001b[0m \u001b[31m203.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m624.6/664.8 MB\u001b[0m \u001b[31m202.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m631.9/664.8 MB\u001b[0m \u001b[31m208.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m639.0/664.8 MB\u001b[0m \u001b[31m206.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m646.0/664.8 MB\u001b[0m \u001b[31m205.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m653.2/664.8 MB\u001b[0m \u001b[31m203.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m660.0/664.8 MB\u001b[0m \u001b[31m200.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/363.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/363.4 MB\u001b[0m \u001b[31m204.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/363.4 MB\u001b[0m \u001b[31m198.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.7/363.4 MB\u001b[0m \u001b[31m205.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.7/363.4 MB\u001b[0m \u001b[31m206.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/363.4 MB\u001b[0m \u001b[31m206.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/363.4 MB\u001b[0m \u001b[31m203.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/363.4 MB\u001b[0m \u001b[31m198.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/363.4 MB\u001b[0m \u001b[31m203.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/363.4 MB\u001b[0m \u001b[31m205.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/363.4 MB\u001b[0m \u001b[31m206.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/363.4 MB\u001b[0m \u001b[31m206.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/363.4 MB\u001b[0m \u001b[31m199.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/363.4 MB\u001b[0m \u001b[31m203.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/363.4 MB\u001b[0m \u001b[31m206.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/363.4 MB\u001b[0m \u001b[31m207.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.8/363.4 MB\u001b[0m \u001b[31m209.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.1/363.4 MB\u001b[0m \u001b[31m211.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/363.4 MB\u001b[0m \u001b[31m207.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.4/363.4 MB\u001b[0m \u001b[31m206.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.4/363.4 MB\u001b[0m \u001b[31m204.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.7/363.4 MB\u001b[0m \u001b[31m208.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/363.4 MB\u001b[0m \u001b[31m166.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.0/363.4 MB\u001b[0m \u001b[31m136.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/363.4 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/363.4 MB\u001b[0m \u001b[31m126.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.6/363.4 MB\u001b[0m \u001b[31m142.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.1/363.4 MB\u001b[0m \u001b[31m161.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/363.4 MB\u001b[0m \u001b[31m140.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/363.4 MB\u001b[0m \u001b[31m151.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/363.4 MB\u001b[0m \u001b[31m174.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/363.4 MB\u001b[0m \u001b[31m173.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.3/363.4 MB\u001b[0m \u001b[31m156.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.2/363.4 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.8/363.4 MB\u001b[0m \u001b[31m171.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.7/363.4 MB\u001b[0m \u001b[31m152.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.9/363.4 MB\u001b[0m \u001b[31m142.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.7/363.4 MB\u001b[0m \u001b[31m160.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m238.2/363.4 MB\u001b[0m \u001b[31m162.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m243.8/363.4 MB\u001b[0m \u001b[31m163.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m249.4/363.4 MB\u001b[0m \u001b[31m163.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m255.2/363.4 MB\u001b[0m \u001b[31m166.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m260.9/363.4 MB\u001b[0m \u001b[31m168.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m265.6/363.4 MB\u001b[0m \u001b[31m151.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m269.1/363.4 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m273.7/363.4 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m278.5/363.4 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m283.7/363.4 MB\u001b[0m \u001b[31m132.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m288.2/363.4 MB\u001b[0m \u001b[31m132.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m293.2/363.4 MB\u001b[0m \u001b[31m133.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m299.1/363.4 MB\u001b[0m \u001b[31m162.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m304.9/363.4 MB\u001b[0m \u001b[31m170.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m310.7/363.4 MB\u001b[0m \u001b[31m170.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m316.6/363.4 MB\u001b[0m \u001b[31m169.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m322.9/363.4 MB\u001b[0m \u001b[31m175.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m330.0/363.4 MB\u001b[0m \u001b[31m202.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m337.0/363.4 MB\u001b[0m \u001b[31m201.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m344.4/363.4 MB\u001b[0m \u001b[31m201.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m351.4/363.4 MB\u001b[0m \u001b[31m203.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m358.5/363.4 MB\u001b[0m \u001b[31m203.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0+cu124)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/211.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/211.5 MB\u001b[0m \u001b[31m149.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/211.5 MB\u001b[0m \u001b[31m156.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/211.5 MB\u001b[0m \u001b[31m159.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.2/211.5 MB\u001b[0m \u001b[31m155.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/211.5 MB\u001b[0m \u001b[31m153.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.1/211.5 MB\u001b[0m \u001b[31m162.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/211.5 MB\u001b[0m \u001b[31m176.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/211.5 MB\u001b[0m \u001b[31m179.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/211.5 MB\u001b[0m \u001b[31m171.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/211.5 MB\u001b[0m \u001b[31m167.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/211.5 MB\u001b[0m \u001b[31m165.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/211.5 MB\u001b[0m \u001b[31m165.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/211.5 MB\u001b[0m \u001b[31m163.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.0/211.5 MB\u001b[0m \u001b[31m174.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/211.5 MB\u001b[0m \u001b[31m183.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.3/211.5 MB\u001b[0m \u001b[31m179.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/211.5 MB\u001b[0m \u001b[31m181.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.8/211.5 MB\u001b[0m \u001b[31m183.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.1/211.5 MB\u001b[0m \u001b[31m180.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/211.5 MB\u001b[0m \u001b[31m179.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.3/211.5 MB\u001b[0m \u001b[31m176.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/211.5 MB\u001b[0m \u001b[31m177.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/211.5 MB\u001b[0m \u001b[31m179.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m142.0/211.5 MB\u001b[0m \u001b[31m198.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m149.2/211.5 MB\u001b[0m \u001b[31m204.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m156.3/211.5 MB\u001b[0m \u001b[31m206.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m163.8/211.5 MB\u001b[0m \u001b[31m208.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m170.8/211.5 MB\u001b[0m \u001b[31m209.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m177.9/211.5 MB\u001b[0m \u001b[31m204.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m185.1/211.5 MB\u001b[0m \u001b[31m205.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m192.1/211.5 MB\u001b[0m \u001b[31m203.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m197.0/211.5 MB\u001b[0m \u001b[31m170.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m202.5/211.5 MB\u001b[0m \u001b[31m149.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m208.5/211.5 MB\u001b[0m \u001b[31m171.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/56.3 MB\u001b[0m \u001b[31m157.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/56.3 MB\u001b[0m \u001b[31m165.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.8/56.3 MB\u001b[0m \u001b[31m184.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.9/56.3 MB\u001b[0m \u001b[31m183.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.1/56.3 MB\u001b[0m \u001b[31m181.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.5/56.3 MB\u001b[0m \u001b[31m183.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m42.9/56.3 MB\u001b[0m \u001b[31m185.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m49.2/56.3 MB\u001b[0m \u001b[31m185.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m195.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/127.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/127.9 MB\u001b[0m \u001b[31m144.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/127.9 MB\u001b[0m \u001b[31m151.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/127.9 MB\u001b[0m \u001b[31m164.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/127.9 MB\u001b[0m \u001b[31m167.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.4/127.9 MB\u001b[0m \u001b[31m167.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/127.9 MB\u001b[0m \u001b[31m166.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/127.9 MB\u001b[0m \u001b[31m169.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/127.9 MB\u001b[0m \u001b[31m168.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/127.9 MB\u001b[0m \u001b[31m168.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/127.9 MB\u001b[0m \u001b[31m164.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/127.9 MB\u001b[0m \u001b[31m164.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/127.9 MB\u001b[0m \u001b[31m166.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/127.9 MB\u001b[0m \u001b[31m167.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/127.9 MB\u001b[0m \u001b[31m165.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/127.9 MB\u001b[0m \u001b[31m165.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m89.9/127.9 MB\u001b[0m \u001b[31m167.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m95.4/127.9 MB\u001b[0m \u001b[31m162.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m100.9/127.9 MB\u001b[0m \u001b[31m161.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m106.4/127.9 MB\u001b[0m \u001b[31m161.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m112.0/127.9 MB\u001b[0m \u001b[31m161.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m117.7/127.9 MB\u001b[0m \u001b[31m165.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m123.3/127.9 MB\u001b[0m \u001b[31m166.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/207.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/207.5 MB\u001b[0m \u001b[31m158.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/207.5 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/207.5 MB\u001b[0m \u001b[31m162.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.1/207.5 MB\u001b[0m \u001b[31m169.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.0/207.5 MB\u001b[0m \u001b[31m171.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.5/207.5 MB\u001b[0m \u001b[31m201.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/207.5 MB\u001b[0m \u001b[31m211.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/207.5 MB\u001b[0m \u001b[31m207.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/207.5 MB\u001b[0m \u001b[31m210.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/207.5 MB\u001b[0m \u001b[31m211.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/207.5 MB\u001b[0m \u001b[31m206.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/207.5 MB\u001b[0m \u001b[31m213.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.4/207.5 MB\u001b[0m \u001b[31m208.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.7/207.5 MB\u001b[0m \u001b[31m212.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.1/207.5 MB\u001b[0m \u001b[31m210.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.4/207.5 MB\u001b[0m \u001b[31m209.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.8/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.0/207.5 MB\u001b[0m \u001b[31m212.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/207.5 MB\u001b[0m \u001b[31m208.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/207.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m144.6/207.5 MB\u001b[0m \u001b[31m209.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m151.7/207.5 MB\u001b[0m \u001b[31m205.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m158.6/207.5 MB\u001b[0m \u001b[31m199.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m165.8/207.5 MB\u001b[0m \u001b[31m208.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m173.0/207.5 MB\u001b[0m \u001b[31m206.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m180.3/207.5 MB\u001b[0m \u001b[31m210.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m184.2/207.5 MB\u001b[0m \u001b[31m216.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m184.2/207.5 MB\u001b[0m \u001b[31m134.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m184.3/207.5 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m184.3/207.5 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m184.3/207.5 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m184.3/207.5 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m189.1/207.5 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m196.2/207.5 MB\u001b[0m \u001b[31m203.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m203.4/207.5 MB\u001b[0m \u001b[31m209.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/21.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/21.1 MB\u001b[0m \u001b[31m208.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/21.1 MB\u001b[0m \u001b[31m205.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m216.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m216.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting triton==3.2.0 (from torch==2.6.0+cu124)\r\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (1.13.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (11.1.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0+cu124) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0+cu124) (3.0.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0+cu124) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0+cu124) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.21.0+cu124) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.21.0+cu124) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.21.0+cu124) (2024.2.0)\r\n",
      "Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (768.5 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/768.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/768.5 MB\u001b[0m \u001b[31m158.0 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/768.5 MB\u001b[0m \u001b[31m168.9 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/768.5 MB\u001b[0m \u001b[31m174.8 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/768.5 MB\u001b[0m \u001b[31m174.8 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/768.5 MB\u001b[0m \u001b[31m171.1 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.8/768.5 MB\u001b[0m \u001b[31m168.2 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/768.5 MB\u001b[0m \u001b[31m169.7 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/768.5 MB\u001b[0m \u001b[31m168.2 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/768.5 MB\u001b[0m \u001b[31m165.8 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/768.5 MB\u001b[0m \u001b[31m168.5 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/768.5 MB\u001b[0m \u001b[31m167.2 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.4/768.5 MB\u001b[0m \u001b[31m167.3 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/768.5 MB\u001b[0m \u001b[31m169.7 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/768.5 MB\u001b[0m \u001b[31m169.2 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/768.5 MB\u001b[0m \u001b[31m171.2 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/768.5 MB\u001b[0m \u001b[31m170.3 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/768.5 MB\u001b[0m \u001b[31m201.4 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/768.5 MB\u001b[0m \u001b[31m214.4 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/768.5 MB\u001b[0m \u001b[31m208.7 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.2/768.5 MB\u001b[0m \u001b[31m211.3 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/768.5 MB\u001b[0m \u001b[31m208.7 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.6/768.5 MB\u001b[0m \u001b[31m215.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.0/768.5 MB\u001b[0m \u001b[31m210.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/768.5 MB\u001b[0m \u001b[31m219.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.1/768.5 MB\u001b[0m \u001b[31m216.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.5/768.5 MB\u001b[0m \u001b[31m209.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.6/768.5 MB\u001b[0m \u001b[31m211.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.1/768.5 MB\u001b[0m \u001b[31m215.9 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.8/768.5 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.6/768.5 MB\u001b[0m \u001b[31m204.9 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.7/768.5 MB\u001b[0m \u001b[31m199.2 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.9/768.5 MB\u001b[0m \u001b[31m203.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.4/768.5 MB\u001b[0m \u001b[31m175.3 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/768.5 MB\u001b[0m \u001b[31m171.3 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/768.5 MB\u001b[0m \u001b[31m169.3 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.8/768.5 MB\u001b[0m \u001b[31m166.7 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.5/768.5 MB\u001b[0m \u001b[31m166.9 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/768.5 MB\u001b[0m \u001b[31m169.5 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.1/768.5 MB\u001b[0m \u001b[31m167.6 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/768.5 MB\u001b[0m \u001b[31m168.0 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.7/768.5 MB\u001b[0m \u001b[31m170.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.6/768.5 MB\u001b[0m \u001b[31m171.9 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.4/768.5 MB\u001b[0m \u001b[31m169.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.1/768.5 MB\u001b[0m \u001b[31m169.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.9/768.5 MB\u001b[0m \u001b[31m169.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.8/768.5 MB\u001b[0m \u001b[31m171.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.2/768.5 MB\u001b[0m \u001b[31m166.7 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.1/768.5 MB\u001b[0m \u001b[31m164.9 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.8/768.5 MB\u001b[0m \u001b[31m167.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.0/768.5 MB\u001b[0m \u001b[31m193.2 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/768.5 MB\u001b[0m \u001b[31m205.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.0/768.5 MB\u001b[0m \u001b[31m178.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.2/768.5 MB\u001b[0m \u001b[31m214.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.2/768.5 MB\u001b[0m \u001b[31m206.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.9/768.5 MB\u001b[0m \u001b[31m211.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.3/768.5 MB\u001b[0m \u001b[31m216.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.6/768.5 MB\u001b[0m \u001b[31m214.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m372.2/768.5 MB\u001b[0m \u001b[31m215.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.6/768.5 MB\u001b[0m \u001b[31m215.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.0/768.5 MB\u001b[0m \u001b[31m216.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.7/768.5 MB\u001b[0m \u001b[31m219.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.9/768.5 MB\u001b[0m \u001b[31m210.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/768.5 MB\u001b[0m \u001b[31m211.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/768.5 MB\u001b[0m \u001b[31m214.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.7/768.5 MB\u001b[0m \u001b[31m173.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.9/768.5 MB\u001b[0m \u001b[31m135.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.2/768.5 MB\u001b[0m \u001b[31m130.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.9/768.5 MB\u001b[0m \u001b[31m137.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.4/768.5 MB\u001b[0m \u001b[31m169.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.5/768.5 MB\u001b[0m \u001b[31m205.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.1/768.5 MB\u001b[0m \u001b[31m211.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.2/768.5 MB\u001b[0m \u001b[31m208.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/768.5 MB\u001b[0m \u001b[31m214.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.0/768.5 MB\u001b[0m \u001b[31m216.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.1/768.5 MB\u001b[0m \u001b[31m206.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.0/768.5 MB\u001b[0m \u001b[31m196.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m500.4/768.5 MB\u001b[0m \u001b[31m210.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m507.8/768.5 MB\u001b[0m \u001b[31m213.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m515.1/768.5 MB\u001b[0m \u001b[31m213.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m522.5/768.5 MB\u001b[0m \u001b[31m212.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m530.0/768.5 MB\u001b[0m \u001b[31m212.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m537.4/768.5 MB\u001b[0m \u001b[31m212.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m544.6/768.5 MB\u001b[0m \u001b[31m209.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m552.0/768.5 MB\u001b[0m \u001b[31m210.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m559.3/768.5 MB\u001b[0m \u001b[31m208.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m566.5/768.5 MB\u001b[0m \u001b[31m208.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m573.7/768.5 MB\u001b[0m \u001b[31m209.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m581.0/768.5 MB\u001b[0m \u001b[31m208.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m588.2/768.5 MB\u001b[0m \u001b[31m206.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m595.1/768.5 MB\u001b[0m \u001b[31m206.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m602.9/768.5 MB\u001b[0m \u001b[31m214.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m610.1/768.5 MB\u001b[0m \u001b[31m210.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m616.8/768.5 MB\u001b[0m \u001b[31m200.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m624.0/768.5 MB\u001b[0m \u001b[31m206.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m631.5/768.5 MB\u001b[0m \u001b[31m214.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m638.8/768.5 MB\u001b[0m \u001b[31m211.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m646.4/768.5 MB\u001b[0m \u001b[31m217.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m653.9/768.5 MB\u001b[0m \u001b[31m217.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m660.4/768.5 MB\u001b[0m \u001b[31m198.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m667.4/768.5 MB\u001b[0m \u001b[31m194.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m675.1/768.5 MB\u001b[0m \u001b[31m214.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m682.4/768.5 MB\u001b[0m \u001b[31m212.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m688.7/768.5 MB\u001b[0m \u001b[31m192.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m694.1/768.5 MB\u001b[0m \u001b[31m161.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m699.6/768.5 MB\u001b[0m \u001b[31m155.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m706.8/768.5 MB\u001b[0m \u001b[31m198.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m714.3/768.5 MB\u001b[0m \u001b[31m211.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m721.0/768.5 MB\u001b[0m \u001b[31m200.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m727.7/768.5 MB\u001b[0m \u001b[31m192.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m734.8/768.5 MB\u001b[0m \u001b[31m201.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m741.9/768.5 MB\u001b[0m \u001b[31m205.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m749.3/768.5 MB\u001b[0m \u001b[31m210.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m756.7/768.5 MB\u001b[0m \u001b[31m215.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m764.2/768.5 MB\u001b[0m \u001b[31m214.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m217.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl (7.3 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/7.3 MB\u001b[0m \u001b[31m132.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/150.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/150.1 MB\u001b[0m \u001b[31m158.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/150.1 MB\u001b[0m \u001b[31m166.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/150.1 MB\u001b[0m \u001b[31m158.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/150.1 MB\u001b[0m \u001b[31m151.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/150.1 MB\u001b[0m \u001b[31m140.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/150.1 MB\u001b[0m \u001b[31m130.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/150.1 MB\u001b[0m \u001b[31m133.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/150.1 MB\u001b[0m \u001b[31m125.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/150.1 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/150.1 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/150.1 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/150.1 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/150.1 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/150.1 MB\u001b[0m \u001b[31m165.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/150.1 MB\u001b[0m \u001b[31m212.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.5/150.1 MB\u001b[0m \u001b[31m190.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/150.1 MB\u001b[0m \u001b[31m181.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/150.1 MB\u001b[0m \u001b[31m205.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/150.1 MB\u001b[0m \u001b[31m209.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m108.1/150.1 MB\u001b[0m \u001b[31m190.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m112.8/150.1 MB\u001b[0m \u001b[31m159.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m117.2/150.1 MB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m121.3/150.1 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m126.3/150.1 MB\u001b[0m \u001b[31m128.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m132.5/150.1 MB\u001b[0m \u001b[31m168.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m138.6/150.1 MB\u001b[0m \u001b[31m177.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m144.4/150.1 MB\u001b[0m \u001b[31m170.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.7 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/166.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/166.7 MB\u001b[0m \u001b[31m210.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/166.7 MB\u001b[0m \u001b[31m209.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/166.7 MB\u001b[0m \u001b[31m216.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.2/166.7 MB\u001b[0m \u001b[31m211.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.6/166.7 MB\u001b[0m \u001b[31m215.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/166.7 MB\u001b[0m \u001b[31m215.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/166.7 MB\u001b[0m \u001b[31m211.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.7/166.7 MB\u001b[0m \u001b[31m213.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.1/166.7 MB\u001b[0m \u001b[31m215.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/166.7 MB\u001b[0m \u001b[31m214.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/166.7 MB\u001b[0m \u001b[31m207.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/166.7 MB\u001b[0m \u001b[31m199.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/166.7 MB\u001b[0m \u001b[31m203.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/166.7 MB\u001b[0m \u001b[31m214.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/166.7 MB\u001b[0m \u001b[31m214.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m117.2/166.7 MB\u001b[0m \u001b[31m214.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m124.5/166.7 MB\u001b[0m \u001b[31m214.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m131.9/166.7 MB\u001b[0m \u001b[31m213.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m139.0/166.7 MB\u001b[0m \u001b[31m207.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m146.4/166.7 MB\u001b[0m \u001b[31m214.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m153.5/166.7 MB\u001b[0m \u001b[31m209.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m160.8/166.7 MB\u001b[0m \u001b[31m211.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: triton, nvidia-cusparselt-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchaudio, torchvision\r\n",
      "  Attempting uninstall: triton\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: triton 3.1.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling triton-3.1.0:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled triton-3.1.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-curand-cu12\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-cufft-cu12\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-cublas-cu12\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu124 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nvjitlink-cu12-12.4.127 torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124 triton-3.2.0\r\n"
     ]
    }
   ],
   "source": [
    "# First uninstall existing torch, torchvision, torchaudio\n",
    "!pip uninstall -y torch torchvision torchaudio\n",
    "\n",
    "# Install the specified versions from PyTorch's official CUDA 12.4 wheels\n",
    "!pip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a676633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T20:16:17.379516Z",
     "iopub.status.busy": "2025-05-13T20:16:17.379239Z",
     "iopub.status.idle": "2025-05-13T20:16:19.626884Z",
     "shell.execute_reply": "2025-05-13T20:16:19.626065Z"
    },
    "papermill": {
     "duration": 2.332654,
     "end_time": "2025-05-13T20:16:19.628197",
     "exception": false,
     "start_time": "2025-05-13T20:16:17.295543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "import triton\n",
    "\n",
    "print(triton.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d83dafd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T20:16:19.705171Z",
     "iopub.status.busy": "2025-05-13T20:16:19.704611Z",
     "iopub.status.idle": "2025-05-13T20:16:19.707549Z",
     "shell.execute_reply": "2025-05-13T20:16:19.707031Z"
    },
    "papermill": {
     "duration": 0.042112,
     "end_time": "2025-05-13T20:16:19.708603",
     "exception": false,
     "start_time": "2025-05-13T20:16:19.666491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !export LC_ALL=\"en_US.UTF-8\"\n",
    "# !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "# !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "# !ldconfig /usr/lib64-nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e38651bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T20:16:19.782615Z",
     "iopub.status.busy": "2025-05-13T20:16:19.782397Z",
     "iopub.status.idle": "2025-05-13T20:16:19.787172Z",
     "shell.execute_reply": "2025-05-13T20:16:19.786575Z"
    },
    "papermill": {
     "duration": 0.043183,
     "end_time": "2025-05-13T20:16:19.788125",
     "exception": false,
     "start_time": "2025-05-13T20:16:19.744942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/Panther/pawX/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/Panther/pawX/setup.py\n",
    "from setuptools import setup\n",
    "from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n",
    "\n",
    "setup(\n",
    "    name=\"pawX\",\n",
    "    ext_modules=[\n",
    "        CUDAExtension(\n",
    "            name=\"pawX\",\n",
    "            sources=[\n",
    "                \"skops.cpp\",\n",
    "                \"bindings.cpp\",\n",
    "                \"linear.cpp\",\n",
    "                \"linear_cuda.cu\",\n",
    "                \"cqrrpt.cpp\",\n",
    "                \"rsvd.cpp\",\n",
    "                \"attention.cpp\",\n",
    "                \"conv2d.cpp\"\n",
    "            ],\n",
    "            # Use system includes and libraries\n",
    "            include_dirs=[\"/usr/include/x86_64-linux-gnu\"],\n",
    "            library_dirs=[],\n",
    "            libraries=[\"openblas\"],\n",
    "            extra_compile_args={\"cxx\": [\"-O2\", \"-fopenmp\"], \"nvcc\": [\"-O2\"]},\n",
    "            extra_link_args=[\"-llapacke\", \"-lopenblas\"]\n",
    "        )\n",
    "    ],\n",
    "    cmdclass={\"build_ext\": BuildExtension},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bb1531d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T20:16:19.862130Z",
     "iopub.status.busy": "2025-05-13T20:16:19.861902Z",
     "iopub.status.idle": "2025-05-13T20:16:28.875473Z",
     "shell.execute_reply": "2025-05-13T20:16:28.874179Z"
    },
    "papermill": {
     "duration": 9.052573,
     "end_time": "2025-05-13T20:16:28.877492",
     "exception": false,
     "start_time": "2025-05-13T20:16:19.824919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 0%\r",
      "\r",
      "Reading package lists... 0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 3%\r",
      "\r",
      "Reading package lists... 3%\r",
      "\r",
      "Reading package lists... 3%\r",
      "\r",
      "Reading package lists... 3%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 37%\r",
      "\r",
      "Reading package lists... 37%\r",
      "\r",
      "Reading package lists... 37%\r",
      "\r",
      "Reading package lists... 37%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 44%\r",
      "\r",
      "Reading package lists... 44%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 48%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 54%\r",
      "\r",
      "Reading package lists... 54%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 58%\r",
      "\r",
      "Reading package lists... 58%\r",
      "\r",
      "Reading package lists... 58%\r",
      "\r",
      "Reading package lists... 58%\r",
      "\r",
      "Reading package lists... 58%\r",
      "\r",
      "Reading package lists... 58%\r",
      "\r",
      "Reading package lists... 58%\r",
      "\r",
      "Reading package lists... 58%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 65%\r",
      "\r",
      "Reading package lists... 65%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 74%\r",
      "\r",
      "Reading package lists... 74%\r",
      "\r",
      "Reading package lists... 77%\r",
      "\r",
      "Reading package lists... 77%\r",
      "\r",
      "Reading package lists... 77%\r",
      "\r",
      "Reading package lists... 77%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 78%\r",
      "\r",
      "Reading package lists... 78%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 82%\r",
      "\r",
      "Reading package lists... 82%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 95%\r",
      "\r",
      "Reading package lists... 95%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 98%\r",
      "\r",
      "Reading package lists... 98%\r",
      "\r",
      "Reading package lists... 98%\r",
      "\r",
      "Reading package lists... 98%\r",
      "\r",
      "Reading package lists... 99%\r",
      "\r",
      "Reading package lists... 99%\r",
      "\r",
      "Reading package lists... 99%\r",
      "\r",
      "Reading package lists... 99%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... Done\r",
      "\r\n",
      "\r",
      "Building dependency tree... 0%\r",
      "\r",
      "Building dependency tree... 0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Building dependency tree... 50%\r",
      "\r",
      "Building dependency tree... 50%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Building dependency tree... 54%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Building dependency tree... Done\r",
      "\r\n",
      "\r",
      "Reading state information... 0% \r",
      "\r",
      "Reading state information... 0%\r",
      "\r",
      "Reading state information... Done\r",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following additional packages will be installed:\r\n",
      "  liblapacke libtmglib-dev libtmglib3\r\n",
      "Suggested packages:\r\n",
      "  liblapack-doc\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following NEW packages will be installed:\r\n",
      "  liblapacke liblapacke-dev libtmglib-dev libtmglib3\r\n",
      "0 upgraded, 4 newly installed, 0 to remove and 122 not upgraded.\r\n",
      "Need to get 1,071 kB of archives.\r\n",
      "After this operation, 12.3 MB of additional disk space will be used.\r\n",
      "\r",
      "0% [Working]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "            \r",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtmglib3 amd64 3.10.0-2ubuntu1 [144 kB]\r\n",
      "\r",
      "1% [1 libtmglib3 12.7 kB/144 kB 9%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                   \r",
      "16% [Working]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "             \r",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblapacke amd64 3.10.0-2ubuntu1 [435 kB]\r\n",
      "\r",
      "16% [2 liblapacke 2,604 B/435 kB 1%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                    \r",
      "53% [Waiting for headers]\r",
      "                         \r",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtmglib-dev amd64 3.10.0-2ubuntu1 [134 kB]\r\n",
      "\r",
      "54% [3 libtmglib-dev 3,208 B/134 kB 2%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                       \r",
      "68% [Waiting for headers]\r",
      "                         \r",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblapacke-dev amd64 3.10.0-2ubuntu1 [358 kB]\r\n",
      "\r",
      "68% [4 liblapacke-dev 2,572 B/358 kB 1%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                        \r",
      "100% [Working]\r",
      "              \r",
      "Fetched 1,071 kB in 1s (1,014 kB/s)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debconf: unable to initialize frontend: Dialog\r\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\r\n",
      "debconf: falling back to frontend: Readline\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libtmglib3:amd64.\r\n",
      "(Reading database ... \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Reading database ... 5%\r",
      "(Reading database ... 10%\r",
      "(Reading database ... 15%\r",
      "(Reading database ... 20%\r",
      "(Reading database ... 25%\r",
      "(Reading database ... 30%\r",
      "(Reading database ... 35%\r",
      "(Reading database ... 40%\r",
      "(Reading database ... 45%\r",
      "(Reading database ... 50%\r",
      "(Reading database ... 55%\r",
      "(Reading database ... 60%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Reading database ... 65%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Reading database ... 70%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Reading database ... 75%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Reading database ... 80%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Reading database ... 85%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Reading database ... 90%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Reading database ... 95%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Reading database ... 100%\r",
      "(Reading database ... 128691 files and directories currently installed.)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to unpack .../libtmglib3_3.10.0-2ubuntu1_amd64.deb ...\r\n",
      "Unpacking libtmglib3:amd64 (3.10.0-2ubuntu1) ...\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package liblapacke:amd64.\r\n",
      "Preparing to unpack .../liblapacke_3.10.0-2ubuntu1_amd64.deb ...\r\n",
      "Unpacking liblapacke:amd64 (3.10.0-2ubuntu1) ...\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libtmglib-dev:amd64.\r\n",
      "Preparing to unpack .../libtmglib-dev_3.10.0-2ubuntu1_amd64.deb ...\r\n",
      "Unpacking libtmglib-dev:amd64 (3.10.0-2ubuntu1) ...\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package liblapacke-dev:amd64.\r\n",
      "Preparing to unpack .../liblapacke-dev_3.10.0-2ubuntu1_amd64.deb ...\r\n",
      "Unpacking liblapacke-dev:amd64 (3.10.0-2ubuntu1) ...\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up libtmglib3:amd64 (3.10.0-2ubuntu1) ...\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up liblapacke:amd64 (3.10.0-2ubuntu1) ...\r\n",
      "Setting up libtmglib-dev:amd64 (3.10.0-2ubuntu1) ...\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up liblapacke-dev:amd64 (3.10.0-2ubuntu1) ...\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install liblapacke-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a4b8588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T20:16:29.048923Z",
     "iopub.status.busy": "2025-05-13T20:16:29.048611Z",
     "iopub.status.idle": "2025-05-13T20:19:06.607624Z",
     "shell.execute_reply": "2025-05-13T20:19:06.606767Z"
    },
    "papermill": {
     "duration": 157.661216,
     "end_time": "2025-05-13T20:19:06.609251",
     "exception": false,
     "start_time": "2025-05-13T20:16:28.948035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\r\n",
      "!!\r\n",
      "\r\n",
      "        ********************************************************************************\r\n",
      "        Please avoid running ``setup.py`` directly.\r\n",
      "        Instead, use pypa/build, pypa/installer or other\r\n",
      "        standards-based tools.\r\n",
      "\r\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\r\n",
      "        ********************************************************************************\r\n",
      "\r\n",
      "!!\r\n",
      "  self.initialize_options()\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\r\n",
      "!!\r\n",
      "\r\n",
      "        ********************************************************************************\r\n",
      "        Please avoid running ``setup.py`` and ``easy_install``.\r\n",
      "        Instead, use pypa/build, pypa/installer or other\r\n",
      "        standards-based tools.\r\n",
      "\r\n",
      "        See https://github.com/pypa/setuptools/issues/917 for details.\r\n",
      "        ********************************************************************************\r\n",
      "\r\n",
      "!!\r\n",
      "  self.initialize_options()\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\r\n",
      "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\r\n",
      "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\r\n",
      "  warnings.warn(\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emitting ninja build file /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/build.ninja...\r\n",
      "Compiling objects...\r\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/cqrrpt.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/linear.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/conv2d.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/conv2d.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/conv2d.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "/kaggle/working/Panther/pawX/conv2d.cpp: In function ‘at::Tensor sketched_conv2d_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const std::vector<long int>&, const std::vector<long int>&, const std::vector<long int>&, const std::optional<at::Tensor>&)’:\r\n",
      "/kaggle/working/Panther/pawX/conv2d.cpp:16:28: warning: unused variable ‘C’ [-Wunused-variable]\r\n",
      "   16 |     int64_t B = x.size(0), C = x.size(1), H = x.size(2), W = x.size(3);\r\n",
      "      |                            ^\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/attention.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/bindings.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n",
      "In file included from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/Exceptions.h:12,\r\n",
      "                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/python.h:11,\r\n",
      "                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/extension.h:9,\r\n",
      "                 from /kaggle/working/Panther/pawX/attention.h:3,\r\n",
      "                 from /kaggle/working/Panther/pawX/bindings.cpp:1:\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘class pybind11::class_<DistributionFamily>’:\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2216:7:   required from ‘class pybind11::enum_<DistributionFamily>’\r\n",
      "/kaggle/working/Panther/pawX/bindings.cpp:24:58:   required from here\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1539:7: warning: ‘pybind11::class_<DistributionFamily>’ declared with greater visibility than its base ‘pybind11::detail::generic_type’ [-Wattributes]\r\n",
      " 1539 | class class_ : public detail::generic_type {\r\n",
      "      |       ^~~~~~\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘pybind11::class_< <template-parameter-1-1>, <template-parameter-1-2> >::class_(pybind11::handle, const char*, const Extra& ...) [with Extra = {}; type_ = DistributionFamily; options = {}]’:\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2232:67:   required from ‘pybind11::enum_<Type>::enum_(const pybind11::handle&, const char*, const Extra& ...) [with Extra = {}; Type = DistributionFamily]’\r\n",
      "/kaggle/working/Panther/pawX/bindings.cpp:24:58:   required from here\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1599:28: warning: ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>’ declared with greater visibility than the type of its field ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>::<record capture>’ [-Wattributes]\r\n",
      " 1599 |             with_internals([&](internals &internals) {\r\n",
      "      |                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1600 |                 auto &instances = record.module_local ? get_local_internals().registered_types_cpp\r\n",
      "      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1601 |                                                       : internals.registered_types_cpp;\r\n",
      "      |                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1602 |                 instances[std::type_index(typeid(type_alias))]\r\n",
      "      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1603 |                     = instances[std::type_index(typeid(type))];\r\n",
      "      |                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      " 1604 |             });\r\n",
      "      |             ~               \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/skops.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/rsvd.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/8] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o.d -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/linear_cuda.cu -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pawX-0.0.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\r\n",
      "\u001b[0mObtaining file:///kaggle/working/Panther/pawX\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: pawX\r\n",
      "  Attempting uninstall: pawX\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: pawX 0.0.0\r\n",
      "    Uninstalling pawX-0.0.0:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled pawX-0.0.0\r\n",
      "  Running setup.py develop for pawX\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed pawX-0.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!cd /kaggle/working/Panther/pawX; python setup.py install\n",
    "!cd /kaggle/working/Panther/pawX; pip install --no-build-isolation -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f37f011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T20:19:06.696905Z",
     "iopub.status.busy": "2025-05-13T20:19:06.696004Z",
     "iopub.status.idle": "2025-05-13T20:19:06.700480Z",
     "shell.execute_reply": "2025-05-13T20:19:06.699899Z"
    },
    "papermill": {
     "duration": 0.048477,
     "end_time": "2025-05-13T20:19:06.701557",
     "exception": false,
     "start_time": "2025-05-13T20:19:06.653080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72f181b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T20:19:06.787579Z",
     "iopub.status.busy": "2025-05-13T20:19:06.786977Z",
     "iopub.status.idle": "2025-05-13T20:19:06.790626Z",
     "shell.execute_reply": "2025-05-13T20:19:06.790016Z"
    },
    "papermill": {
     "duration": 0.048045,
     "end_time": "2025-05-13T20:19:06.791754",
     "exception": false,
     "start_time": "2025-05-13T20:19:06.743709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/kaggle/working/Panther/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91bf2b39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T20:19:06.874482Z",
     "iopub.status.busy": "2025-05-13T20:19:06.873971Z",
     "iopub.status.idle": "2025-05-13T20:19:07.000065Z",
     "shell.execute_reply": "2025-05-13T20:19:06.999264Z"
    },
    "papermill": {
     "duration": 0.169019,
     "end_time": "2025-05-13T20:19:07.001536",
     "exception": false,
     "start_time": "2025-05-13T20:19:06.832517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Panther\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e69845be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T20:19:07.089668Z",
     "iopub.status.busy": "2025-05-13T20:19:07.089431Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-05-13T20:19:07.045592",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================== COMBINATION 1/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 42.097 ± 2.288 ms, Memory: 344.89 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (4) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 2/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 84.212 ± 3.572 ms, Memory: 689.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (4) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 3/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 172.039 ± 7.176 ms, Memory: 1362.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 548.12 MiB is free. Process 5476 has 14.20 GiB memory in use. Of the allocated memory 13.34 GiB is allocated by PyTorch, and 717.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 4/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 364.985 ± 12.807 ms, Memory: 2708.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 548.12 MiB is free. Process 5476 has 14.20 GiB memory in use. Of the allocated memory 10.66 GiB is allocated by PyTorch, and 3.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 5/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 760.862 ± 18.475 ms, Memory: 5400.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 8.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 6/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 42.368 ± 3.284 ms, Memory: 272.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (4) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 7/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 84.390 ± 4.294 ms, Memory: 528.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (4) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 8/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 168.436 ± 6.221 ms, Memory: 1040.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 13.20 GiB is allocated by PyTorch, and 837.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 9/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 339.725 ± 10.814 ms, Memory: 2064.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 10.39 GiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 10/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 704.480 ± 18.634 ms, Memory: 4112.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 4.77 GiB is allocated by PyTorch, and 9.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 11/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 60.081 ± 0.182 ms, Memory: 577.02 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (4) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 12/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 120.286 ± 0.255 ms, Memory: 1137.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 13.05 GiB is allocated by PyTorch, and 993.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 13/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 239.550 ± 0.496 ms, Memory: 2258.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 10.09 GiB is allocated by PyTorch, and 3.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 14/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 479.414 ± 1.434 ms, Memory: 4500.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 4.16 GiB is allocated by PyTorch, and 9.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 15/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 960.840 ± 1.396 ms, Memory: 8984.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 8.30 GiB is allocated by PyTorch, and 5.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 16/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 55.889 ± 0.698 ms, Memory: 432.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (4) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 17/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 111.611 ± 0.492 ms, Memory: 848.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 12.98 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 18/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 223.127 ± 0.441 ms, Memory: 1680.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 9.95 GiB is allocated by PyTorch, and 4.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 19/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 445.431 ± 0.787 ms, Memory: 3344.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 3.89 GiB is allocated by PyTorch, and 10.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 20/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 891.436 ± 1.551 ms, Memory: 6672.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 7.77 GiB is allocated by PyTorch, and 6.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 21/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 116.896 ± 0.274 ms, Memory: 1025.03 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 562.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 12.91 GiB is allocated by PyTorch, and 1.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 22/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 233.709 ± 0.422 ms, Memory: 2033.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 9.80 GiB is allocated by PyTorch, and 4.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 23/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 466.527 ± 0.441 ms, Memory: 4050.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.55 GiB is free. Process 5476 has 6.19 GiB memory in use. Of the allocated memory 3.59 GiB is allocated by PyTorch, and 2.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 24/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 931.596 ± 1.272 ms, Memory: 8084.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.55 GiB is free. Process 5476 has 9.19 GiB memory in use. Of the allocated memory 7.16 GiB is allocated by PyTorch, and 1.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 25/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 11.78 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 26/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 109.886 ± 0.225 ms, Memory: 752.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 12.88 GiB is allocated by PyTorch, and 1.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 27/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 220.300 ± 0.353 ms, Memory: 1488.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 9.73 GiB is allocated by PyTorch, and 4.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 28/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 439.846 ± 0.638 ms, Memory: 2960.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.55 GiB is free. Process 5476 has 6.19 GiB memory in use. Of the allocated memory 3.45 GiB is allocated by PyTorch, and 2.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 29/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 878.005 ± 1.065 ms, Memory: 5904.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.55 GiB is free. Process 5476 has 9.19 GiB memory in use. Of the allocated memory 6.89 GiB is allocated by PyTorch, and 2.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 30/360 ====================\n",
      "Embed dimension: 128, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1756.544 ± 1.410 ms, Memory: 11792.53 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 12.02 GiB is allocated by PyTorch, and 2.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 31/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 42.983 ± 1.954 ms, Memory: 577.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (8) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 32/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 85.423 ± 3.547 ms, Memory: 1138.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (8) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 33/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 189.874 ± 6.880 ms, Memory: 2260.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 10.09 GiB is allocated by PyTorch, and 1.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 34/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 400.438 ± 11.329 ms, Memory: 4504.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.55 GiB is free. Process 5476 has 8.19 GiB memory in use. Of the allocated memory 4.17 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 35/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 837.558 ± 18.520 ms, Memory: 8992.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 5476 has 11.19 GiB memory in use. Of the allocated memory 8.33 GiB is allocated by PyTorch, and 2.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 36/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 43.465 ± 2.249 ms, Memory: 432.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (8) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 37/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 85.223 ± 3.639 ms, Memory: 848.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (8) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 38/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 171.817 ± 6.385 ms, Memory: 1680.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 9.95 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 39/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 365.719 ± 12.323 ms, Memory: 3344.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.55 GiB is free. Process 5476 has 8.19 GiB memory in use. Of the allocated memory 3.89 GiB is allocated by PyTorch, and 4.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 40/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 757.825 ± 18.597 ms, Memory: 6672.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.55 GiB is free. Process 5476 has 9.19 GiB memory in use. Of the allocated memory 7.77 GiB is allocated by PyTorch, and 1.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 41/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 70.956 ± 0.180 ms, Memory: 1025.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (8) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 42/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 141.046 ± 0.471 ms, Memory: 2034.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 5476 has 11.19 GiB memory in use. Of the allocated memory 9.81 GiB is allocated by PyTorch, and 1.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 43/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 281.942 ± 0.364 ms, Memory: 4052.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.55 GiB is free. Process 5476 has 7.19 GiB memory in use. Of the allocated memory 3.59 GiB is allocated by PyTorch, and 3.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 44/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 561.137 ± 0.458 ms, Memory: 8088.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.55 GiB is free. Process 5476 has 9.19 GiB memory in use. Of the allocated memory 7.17 GiB is allocated by PyTorch, and 1.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 45/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 11.80 GiB is allocated by PyTorch, and 1.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 46/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 63.748 ± 1.068 ms, Memory: 752.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (8) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 47/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 126.533 ± 0.332 ms, Memory: 1488.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 5476 has 11.19 GiB memory in use. Of the allocated memory 9.73 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 48/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 252.153 ± 0.534 ms, Memory: 2960.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.55 GiB is free. Process 5476 has 7.19 GiB memory in use. Of the allocated memory 3.45 GiB is allocated by PyTorch, and 3.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 49/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 502.430 ± 0.248 ms, Memory: 5904.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.55 GiB is free. Process 5476 has 8.19 GiB memory in use. Of the allocated memory 6.89 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 50/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1008.254 ± 0.658 ms, Memory: 11792.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 12.02 GiB is allocated by PyTorch, and 2.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 51/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 135.528 ± 0.238 ms, Memory: 1921.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 9.66 GiB is allocated by PyTorch, and 2.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 52/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 271.100 ± 0.114 ms, Memory: 3826.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.55 GiB is free. Process 5476 has 8.19 GiB memory in use. Of the allocated memory 3.31 GiB is allocated by PyTorch, and 4.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 53/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 539.534 ± 0.225 ms, Memory: 7636.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.55 GiB is free. Process 5476 has 9.19 GiB memory in use. Of the allocated memory 6.59 GiB is allocated by PyTorch, and 2.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 54/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 12.90 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 55/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 9.80 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 56/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 122.763 ± 0.221 ms, Memory: 1392.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 2.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 57/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 245.869 ± 0.390 ms, Memory: 2768.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.55 GiB is free. Process 5476 has 8.19 GiB memory in use. Of the allocated memory 3.23 GiB is allocated by PyTorch, and 4.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 58/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 489.848 ± 0.293 ms, Memory: 5520.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.55 GiB is free. Process 5476 has 8.19 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 59/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 978.816 ± 0.393 ms, Memory: 11024.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 12.90 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 60/360 ====================\n",
      "Embed dimension: 128, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 13.52 GiB is allocated by PyTorch, and 517.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 61/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 46.055 ± 1.669 ms, Memory: 1026.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 62/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 104.961 ± 4.640 ms, Memory: 2036.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 63/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 228.764 ± 6.175 ms, Memory: 4056.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 11.61 GiB is allocated by PyTorch, and 2.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 64/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 480.366 ± 10.811 ms, Memory: 8096.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 7.20 GiB is allocated by PyTorch, and 5.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 65/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 11.83 GiB is allocated by PyTorch, and 1.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 66/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 44.432 ± 2.263 ms, Memory: 752.50 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 67/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 89.280 ± 4.723 ms, Memory: 1488.50 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 68/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 195.809 ± 5.974 ms, Memory: 2960.50 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 7.45 GiB is allocated by PyTorch, and 5.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 69/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 410.753 ± 10.244 ms, Memory: 5904.50 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 6.89 GiB is allocated by PyTorch, and 6.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 70/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 849.911 ± 18.356 ms, Memory: 11792.50 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 11.52 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 71/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 93.140 ± 0.362 ms, Memory: 1922.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 72/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 186.193 ± 0.200 ms, Memory: 3828.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 7.31 GiB is allocated by PyTorch, and 5.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 73/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 370.513 ± 0.330 ms, Memory: 7640.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 6.61 GiB is allocated by PyTorch, and 6.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 74/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 10.92 GiB is allocated by PyTorch, and 2.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 75/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 9.83 GiB is allocated by PyTorch, and 3.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 76/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 79.473 ± 0.207 ms, Memory: 1392.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 77/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 158.943 ± 0.265 ms, Memory: 2768.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 7.23 GiB is allocated by PyTorch, and 5.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 78/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 316.047 ± 0.106 ms, Memory: 5520.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 6.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 79/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 629.929 ± 0.491 ms, Memory: 11024.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 2.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 80/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 9.52 GiB is allocated by PyTorch, and 3.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 81/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 177.728 ± 0.093 ms, Memory: 3714.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 7.16 GiB is allocated by PyTorch, and 5.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 82/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 354.576 ± 0.191 ms, Memory: 7412.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 6.31 GiB is allocated by PyTorch, and 6.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 83/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 12.46 GiB is allocated by PyTorch, and 573.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 84/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 8.92 GiB is allocated by PyTorch, and 4.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 85/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 9.83 GiB is allocated by PyTorch, and 3.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 86/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 153.427 ± 0.122 ms, Memory: 2672.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 7.13 GiB is allocated by PyTorch, and 5.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 87/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 307.044 ± 0.081 ms, Memory: 5328.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 6.23 GiB is allocated by PyTorch, and 6.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 88/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 612.918 ± 0.438 ms, Memory: 10640.51 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 12.46 GiB is allocated by PyTorch, and 573.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 89/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 12.77 GiB is allocated by PyTorch, and 261.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 90/360 ====================\n",
      "Embed dimension: 128, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 9.52 GiB is allocated by PyTorch, and 3.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 91/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 62.323 ± 0.491 ms, Memory: 465.77 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (4) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 92/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 124.369 ± 0.453 ms, Memory: 914.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 8.96 GiB is allocated by PyTorch, and 4.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 93/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 248.854 ± 0.532 ms, Memory: 1811.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 3.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 94/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 499.919 ± 1.318 ms, Memory: 3605.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 3.78 GiB is allocated by PyTorch, and 9.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 95/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1002.418 ± 2.683 ms, Memory: 7193.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 7.55 GiB is allocated by PyTorch, and 5.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 96/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 58.833 ± 0.472 ms, Memory: 373.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (4) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 97/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 117.139 ± 0.636 ms, Memory: 725.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 8.83 GiB is allocated by PyTorch, and 4.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 98/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 234.025 ± 3.476 ms, Memory: 1429.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 9.64 GiB is allocated by PyTorch, and 3.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 99/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 467.552 ± 1.101 ms, Memory: 2837.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.55 GiB is free. Process 5476 has 5.19 GiB memory in use. Of the allocated memory 3.27 GiB is allocated by PyTorch, and 1.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 100/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 942.924 ± 3.142 ms, Memory: 5657.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.55 GiB is free. Process 5476 has 7.19 GiB memory in use. Of the allocated memory 6.52 GiB is allocated by PyTorch, and 516.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 101/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 113.159 ± 0.325 ms, Memory: 689.79 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 12.68 GiB is allocated by PyTorch, and 358.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 102/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 226.592 ± 0.427 ms, Memory: 1362.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 9.33 GiB is allocated by PyTorch, and 3.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 103/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 453.330 ± 0.783 ms, Memory: 2707.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.55 GiB is free. Process 5476 has 5.19 GiB memory in use. Of the allocated memory 2.65 GiB is allocated by PyTorch, and 2.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 104/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 908.126 ± 0.696 ms, Memory: 5397.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.05 GiB is free. Process 5476 has 7.69 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 2.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 105/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1819.115 ± 2.683 ms, Memory: 10777.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 5476 has 14.69 GiB memory in use. Of the allocated memory 12.56 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 106/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 108.107 ± 0.381 ms, Memory: 529.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 12.61 GiB is allocated by PyTorch, and 420.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 107/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 216.560 ± 0.545 ms, Memory: 1041.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 9.20 GiB is allocated by PyTorch, and 3.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 108/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 434.606 ± 0.677 ms, Memory: 2065.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.55 GiB is free. Process 5476 has 5.19 GiB memory in use. Of the allocated memory 2.39 GiB is allocated by PyTorch, and 2.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 109/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 867.435 ± 1.182 ms, Memory: 4113.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.05 GiB is free. Process 5476 has 6.69 GiB memory in use. Of the allocated memory 4.77 GiB is allocated by PyTorch, and 1.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 110/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1743.689 ± 2.930 ms, Memory: 8209.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 12.52 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 111/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 213.686 ± 0.345 ms, Memory: 1137.82 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.55 GiB is free. Process 5476 has 10.19 GiB memory in use. Of the allocated memory 9.05 GiB is allocated by PyTorch, and 998.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 112/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 426.026 ± 0.649 ms, Memory: 2258.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.55 GiB is free. Process 5476 has 10.19 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 7.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 113/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 852.583 ± 0.883 ms, Memory: 4499.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.05 GiB is free. Process 5476 has 9.69 GiB memory in use. Of the allocated memory 4.15 GiB is allocated by PyTorch, and 5.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 114/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1707.625 ± 2.108 ms, Memory: 8981.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.05 GiB is free. Process 5476 has 12.69 GiB memory in use. Of the allocated memory 8.28 GiB is allocated by PyTorch, and 4.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 115/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 5476 has 14.69 GiB memory in use. Of the allocated memory 13.53 GiB is allocated by PyTorch, and 1012.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 116/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 206.169 ± 0.385 ms, Memory: 849.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.05 GiB is free. Process 5476 has 10.69 GiB memory in use. Of the allocated memory 8.99 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 117/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 411.827 ± 0.515 ms, Memory: 1681.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.05 GiB is free. Process 5476 has 10.69 GiB memory in use. Of the allocated memory 1.95 GiB is allocated by PyTorch, and 8.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 118/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 822.506 ± 1.790 ms, Memory: 3345.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.55 GiB is free. Process 5476 has 10.19 GiB memory in use. Of the allocated memory 3.89 GiB is allocated by PyTorch, and 6.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 119/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1650.373 ± 1.568 ms, Memory: 6673.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 5476 has 11.19 GiB memory in use. Of the allocated memory 7.77 GiB is allocated by PyTorch, and 3.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 120/360 ====================\n",
      "Embed dimension: 256, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 3301.522 ± 4.970 ms, Memory: 13329.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 4.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 121/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 67.533 ± 0.330 ms, Memory: 690.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (8) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 122/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 135.304 ± 0.574 ms, Memory: 1363.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 13.34 GiB is allocated by PyTorch, and 700.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 123/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 269.546 ± 0.457 ms, Memory: 2709.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 10.66 GiB is allocated by PyTorch, and 3.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 124/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 541.422 ± 1.548 ms, Memory: 5401.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 8.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 125/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1089.976 ± 0.839 ms, Memory: 10785.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 10.60 GiB is allocated by PyTorch, and 3.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 126/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 61.405 ± 0.381 ms, Memory: 529.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (8) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 127/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 123.148 ± 0.574 ms, Memory: 1041.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 13.20 GiB is allocated by PyTorch, and 836.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 128/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 246.403 ± 0.462 ms, Memory: 2065.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 10.39 GiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 129/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 491.677 ± 1.230 ms, Memory: 4113.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 4.77 GiB is allocated by PyTorch, and 9.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 130/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 987.670 ± 1.660 ms, Memory: 8209.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 11.53 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 131/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 122.168 ± 0.256 ms, Memory: 1138.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 13.05 GiB is allocated by PyTorch, and 992.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 132/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 244.740 ± 0.417 ms, Memory: 2259.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 10.09 GiB is allocated by PyTorch, and 3.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 133/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 487.918 ± 0.634 ms, Memory: 4501.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 4.16 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 134/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 979.145 ± 1.579 ms, Memory: 8985.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 8.30 GiB is allocated by PyTorch, and 5.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 135/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 136/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 113.885 ± 0.391 ms, Memory: 849.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 12.99 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 137/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 228.316 ± 0.417 ms, Memory: 1681.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 9.95 GiB is allocated by PyTorch, and 4.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 138/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 454.250 ± 0.924 ms, Memory: 3345.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 3.89 GiB is allocated by PyTorch, and 10.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 139/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 909.397 ± 1.500 ms, Memory: 6673.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 7.77 GiB is allocated by PyTorch, and 6.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 140/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1827.713 ± 2.521 ms, Memory: 13329.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 141/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 231.352 ± 0.446 ms, Memory: 2034.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 9.80 GiB is allocated by PyTorch, and 4.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 142/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 461.363 ± 0.285 ms, Memory: 4051.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.55 GiB is free. Process 5476 has 6.19 GiB memory in use. Of the allocated memory 3.59 GiB is allocated by PyTorch, and 2.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 143/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 922.540 ± 0.891 ms, Memory: 8085.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.55 GiB is free. Process 5476 has 9.19 GiB memory in use. Of the allocated memory 7.16 GiB is allocated by PyTorch, and 1.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 144/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 11.78 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 145/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 146/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 217.615 ± 0.358 ms, Memory: 1489.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 5476 has 11.19 GiB memory in use. Of the allocated memory 9.74 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 147/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 434.396 ± 0.645 ms, Memory: 2961.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 5476 has 11.19 GiB memory in use. Of the allocated memory 3.45 GiB is allocated by PyTorch, and 7.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 148/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 867.829 ± 0.989 ms, Memory: 5905.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 5476 has 11.19 GiB memory in use. Of the allocated memory 6.89 GiB is allocated by PyTorch, and 4.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 149/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1739.859 ± 2.948 ms, Memory: 11793.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 12.02 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 150/360 ====================\n",
      "Embed dimension: 256, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 151/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 78.851 ± 0.301 ms, Memory: 1139.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 152/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 156.955 ± 0.305 ms, Memory: 2261.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 10.10 GiB is allocated by PyTorch, and 1.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 153/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 313.813 ± 0.745 ms, Memory: 4505.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 4.17 GiB is allocated by PyTorch, and 7.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 154/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 628.993 ± 1.972 ms, Memory: 8993.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 8.33 GiB is allocated by PyTorch, and 4.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 155/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 11.58 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================== COMBINATION 156/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 68.801 ± 0.360 ms, Memory: 849.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 2\n",
      "\n",
      "\n",
      "==================== COMBINATION 157/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 137.821 ± 0.500 ms, Memory: 1681.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 9.95 GiB is allocated by PyTorch, and 3.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 158/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 274.916 ± 1.004 ms, Memory: 3345.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 3.89 GiB is allocated by PyTorch, and 9.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 159/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 546.266 ± 0.963 ms, Memory: 6673.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 7.77 GiB is allocated by PyTorch, and 4.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 160/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1108.629 ± 1.558 ms, Memory: 13329.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 161/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 142.143 ± 0.235 ms, Memory: 2035.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 9.81 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================== COMBINATION 162/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 283.634 ± 0.356 ms, Memory: 4053.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.05 GiB is free. Process 5476 has 11.69 GiB memory in use. Of the allocated memory 3.60 GiB is allocated by PyTorch, and 7.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 163/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 567.070 ± 0.888 ms, Memory: 8089.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.05 GiB is free. Process 5476 has 12.69 GiB memory in use. Of the allocated memory 7.17 GiB is allocated by PyTorch, and 5.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 164/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 5476 has 14.69 GiB memory in use. Of the allocated memory 13.78 GiB is allocated by PyTorch, and 756.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 165/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.05 GiB is free. Process 5476 has 12.69 GiB memory in use. Of the allocated memory 11.58 GiB is allocated by PyTorch, and 964.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 166/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 127.330 ± 0.316 ms, Memory: 1489.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.05 GiB is free. Process 5476 has 12.69 GiB memory in use. Of the allocated memory 9.74 GiB is allocated by PyTorch, and 2.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 167/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 254.469 ± 0.499 ms, Memory: 2961.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.05 GiB is free. Process 5476 has 12.69 GiB memory in use. Of the allocated memory 3.45 GiB is allocated by PyTorch, and 9.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 168/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 507.190 ± 0.572 ms, Memory: 5905.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.05 GiB is free. Process 5476 has 12.69 GiB memory in use. Of the allocated memory 6.89 GiB is allocated by PyTorch, and 5.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 169/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1015.224 ± 1.419 ms, Memory: 11793.26 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 5476 has 13.69 GiB memory in use. Of the allocated memory 12.02 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 170/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.05 GiB is free. Process 5476 has 11.69 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 516.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 171/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 269.943 ± 0.170 ms, Memory: 3827.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.05 GiB is free. Process 5476 has 11.69 GiB memory in use. Of the allocated memory 3.31 GiB is allocated by PyTorch, and 8.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 172/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 537.953 ± 0.566 ms, Memory: 7637.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.05 GiB is free. Process 5476 has 10.69 GiB memory in use. Of the allocated memory 6.60 GiB is allocated by PyTorch, and 3.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 173/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 5476 has 14.69 GiB memory in use. Of the allocated memory 12.90 GiB is allocated by PyTorch, and 1.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 174/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 5476 has 14.69 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 740.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 175/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 5476 has 14.69 GiB memory in use. Of the allocated memory 11.58 GiB is allocated by PyTorch, and 2.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 176/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 244.527 ± 0.311 ms, Memory: 2769.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 5476 has 14.69 GiB memory in use. Of the allocated memory 11.24 GiB is allocated by PyTorch, and 3.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 177/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 487.853 ± 0.265 ms, Memory: 5521.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.05 GiB is free. Process 5476 has 8.69 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 178/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 976.998 ± 0.681 ms, Memory: 11025.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 5476 has 13.69 GiB memory in use. Of the allocated memory 12.90 GiB is allocated by PyTorch, and 636.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 179/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 5476 has 13.69 GiB memory in use. Of the allocated memory 13.52 GiB is allocated by PyTorch, and 4.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 180/360 ====================\n",
      "Embed dimension: 256, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 5476 has 13.69 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 181/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 123.795 ± 0.305 ms, Memory: 692.79 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 5476 has 13.69 GiB memory in use. Of the allocated memory 12.77 GiB is allocated by PyTorch, and 772.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 182/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 248.584 ± 0.744 ms, Memory: 1365.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 5476 has 13.69 GiB memory in use. Of the allocated memory 9.52 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 183/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 505.066 ± 0.685 ms, Memory: 2710.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.05 GiB is free. Process 5476 has 5.69 GiB memory in use. Of the allocated memory 3.03 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 184/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1011.287 ± 2.097 ms, Memory: 5400.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.05 GiB is free. Process 5476 has 8.69 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 185/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 2029.146 ± 4.092 ms, Memory: 10780.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 816.12 MiB is free. Process 5476 has 13.94 GiB memory in use. Of the allocated memory 13.56 GiB is allocated by PyTorch, and 219.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 186/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 119.170 ± 0.325 ms, Memory: 668.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.30 GiB is free. Process 5476 has 13.44 GiB memory in use. Of the allocated memory 12.65 GiB is allocated by PyTorch, and 644.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 187/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 239.313 ± 0.651 ms, Memory: 1308.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.30 GiB is free. Process 5476 has 13.44 GiB memory in use. Of the allocated memory 9.27 GiB is allocated by PyTorch, and 4.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 188/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 484.969 ± 0.642 ms, Memory: 2588.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.80 GiB is free. Process 5476 has 4.94 GiB memory in use. Of the allocated memory 2.52 GiB is allocated by PyTorch, and 2.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 189/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 975.035 ± 1.824 ms, Memory: 5148.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.05 GiB is free. Process 5476 has 6.69 GiB memory in use. Of the allocated memory 5.02 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 190/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1955.840 ± 2.425 ms, Memory: 10268.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 13.53 GiB is allocated by PyTorch, and 506.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 191/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 217.783 ± 0.332 ms, Memory: 916.82 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.05 GiB is free. Process 5476 has 9.69 GiB memory in use. Of the allocated memory 8.96 GiB is allocated by PyTorch, and 580.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 192/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 438.431 ± 0.700 ms, Memory: 1813.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.05 GiB is free. Process 5476 has 9.69 GiB memory in use. Of the allocated memory 1.90 GiB is allocated by PyTorch, and 7.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 193/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 887.336 ± 0.776 ms, Memory: 3606.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.05 GiB is free. Process 5476 has 9.69 GiB memory in use. Of the allocated memory 3.78 GiB is allocated by PyTorch, and 5.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 194/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1769.424 ± 1.976 ms, Memory: 7192.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.55 GiB is free. Process 5476 has 10.19 GiB memory in use. Of the allocated memory 7.54 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 195/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 3620.194 ± 6.670 ms, Memory: 14364.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 48.12 MiB is free. Process 5476 has 14.69 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 477.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 196/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 212.612 ± 0.270 ms, Memory: 740.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.05 GiB is free. Process 5476 has 9.69 GiB memory in use. Of the allocated memory 8.83 GiB is allocated by PyTorch, and 708.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 197/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 428.013 ± 0.679 ms, Memory: 1444.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.05 GiB is free. Process 5476 has 9.69 GiB memory in use. Of the allocated memory 1.65 GiB is allocated by PyTorch, and 7.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 198/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 861.301 ± 1.804 ms, Memory: 2852.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.05 GiB is free. Process 5476 has 9.69 GiB memory in use. Of the allocated memory 3.27 GiB is allocated by PyTorch, and 6.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 199/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1720.050 ± 2.491 ms, Memory: 5668.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.05 GiB is free. Process 5476 has 9.69 GiB memory in use. Of the allocated memory 6.52 GiB is allocated by PyTorch, and 3.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 200/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 3443.602 ± 4.330 ms, Memory: 11300.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 540.12 MiB is free. Process 5476 has 14.21 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 15.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 201/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 408.522 ± 0.534 ms, Memory: 1364.88 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.55 GiB is free. Process 5476 has 2.19 GiB memory in use. Of the allocated memory 1.34 GiB is allocated by PyTorch, and 708.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 202/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 818.281 ± 1.515 ms, Memory: 2709.38 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 11.30 GiB is free. Process 5476 has 3.44 GiB memory in use. Of the allocated memory 2.65 GiB is allocated by PyTorch, and 642.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 203/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1641.820 ± 1.583 ms, Memory: 5398.38 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.80 GiB is free. Process 5476 has 6.94 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 204/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 3295.097 ± 4.253 ms, Memory: 10776.38 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 302.12 MiB is free. Process 5476 has 14.44 GiB memory in use. Of the allocated memory 12.54 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 205/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.53 GiB is free. Process 5476 has 13.21 GiB memory in use. Of the allocated memory 13.04 GiB is allocated by PyTorch, and 9.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 206/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 399.131 ± 0.406 ms, Memory: 1044.38 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.55 GiB is free. Process 5476 has 2.19 GiB memory in use. Of the allocated memory 1.21 GiB is allocated by PyTorch, and 836.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 207/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 800.560 ± 1.087 ms, Memory: 2068.38 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 11.80 GiB is free. Process 5476 has 2.94 GiB memory in use. Of the allocated memory 2.40 GiB is allocated by PyTorch, and 390.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 208/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1610.892 ± 1.912 ms, Memory: 4116.38 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.80 GiB is free. Process 5476 has 5.94 GiB memory in use. Of the allocated memory 4.77 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 209/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 3224.294 ± 3.147 ms, Memory: 8212.38 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 814.12 MiB is free. Process 5476 has 13.94 GiB memory in use. Of the allocated memory 12.52 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 210/360 ====================\n",
      "Embed dimension: 512, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 5.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 211/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 128.982 ± 0.333 ms, Memory: 917.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 558.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 12.96 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 212/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 259.537 ± 0.766 ms, Memory: 1814.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 4.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 213/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 525.003 ± 0.625 ms, Memory: 3608.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.55 GiB is free. Process 5476 has 6.19 GiB memory in use. Of the allocated memory 3.79 GiB is allocated by PyTorch, and 2.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 214/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1055.761 ± 1.962 ms, Memory: 7196.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.55 GiB is free. Process 5476 has 9.19 GiB memory in use. Of the allocated memory 7.55 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 215/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 2197.549 ± 9.041 ms, Memory: 14372.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 48.12 MiB is free. Process 5476 has 14.69 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 437.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 216/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 121.674 ± 0.157 ms, Memory: 732.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 5476 has 13.69 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 710.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 217/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 243.829 ± 0.633 ms, Memory: 1436.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 5476 has 13.69 GiB memory in use. Of the allocated memory 9.65 GiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 218/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 494.957 ± 1.591 ms, Memory: 2844.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.05 GiB is free. Process 5476 has 5.69 GiB memory in use. Of the allocated memory 3.27 GiB is allocated by PyTorch, and 2.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 219/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 997.063 ± 2.467 ms, Memory: 5660.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.55 GiB is free. Process 5476 has 7.19 GiB memory in use. Of the allocated memory 6.52 GiB is allocated by PyTorch, and 516.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 220/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 2000.803 ± 2.383 ms, Memory: 11300.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 544.12 MiB is free. Process 5476 has 14.21 GiB memory in use. Of the allocated memory 13.04 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 221/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 229.368 ± 0.320 ms, Memory: 1365.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.53 GiB is free. Process 5476 has 10.21 GiB memory in use. Of the allocated memory 9.34 GiB is allocated by PyTorch, and 722.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 222/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 462.383 ± 0.879 ms, Memory: 2710.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.53 GiB is free. Process 5476 has 10.21 GiB memory in use. Of the allocated memory 2.65 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 223/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 927.474 ± 0.897 ms, Memory: 5400.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.53 GiB is free. Process 5476 has 10.21 GiB memory in use. Of the allocated memory 5.29 GiB is allocated by PyTorch, and 4.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 224/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1864.967 ± 3.014 ms, Memory: 10780.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 544.12 MiB is free. Process 5476 has 14.21 GiB memory in use. Of the allocated memory 11.56 GiB is allocated by PyTorch, and 2.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 225/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 544.12 MiB is free. Process 5476 has 14.21 GiB memory in use. Of the allocated memory 11.05 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 226/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 220.013 ± 0.270 ms, Memory: 1044.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.55 GiB is free. Process 5476 has 10.19 GiB memory in use. Of the allocated memory 9.21 GiB is allocated by PyTorch, and 838.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 227/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 442.194 ± 0.791 ms, Memory: 2068.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.55 GiB is free. Process 5476 has 10.19 GiB memory in use. Of the allocated memory 2.40 GiB is allocated by PyTorch, and 7.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 228/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 886.806 ± 1.119 ms, Memory: 4116.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.55 GiB is free. Process 5476 has 10.19 GiB memory in use. Of the allocated memory 4.77 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 229/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1779.218 ± 1.592 ms, Memory: 8212.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 11.53 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 230/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 5.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 231/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 429.630 ± 0.456 ms, Memory: 2261.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.55 GiB is free. Process 5476 has 4.19 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 1.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 232/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 860.943 ± 0.823 ms, Memory: 4502.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.55 GiB is free. Process 5476 has 6.19 GiB memory in use. Of the allocated memory 4.15 GiB is allocated by PyTorch, and 1.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 233/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1729.469 ± 1.393 ms, Memory: 8984.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.55 GiB is free. Process 5476 has 10.19 GiB memory in use. Of the allocated memory 8.29 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 234/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 11.54 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 235/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.52 GiB is free. Process 5476 has 11.22 GiB memory in use. Of the allocated memory 11.05 GiB is allocated by PyTorch, and 5.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================== COMBINATION 236/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 414.746 ± 0.394 ms, Memory: 1684.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.55 GiB is free. Process 5476 has 5.19 GiB memory in use. Of the allocated memory 1.96 GiB is allocated by PyTorch, and 3.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 237/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 828.470 ± 1.117 ms, Memory: 3348.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.05 GiB is free. Process 5476 has 5.69 GiB memory in use. Of the allocated memory 3.90 GiB is allocated by PyTorch, and 1.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 238/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1671.400 ± 1.173 ms, Memory: 6676.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.05 GiB is free. Process 5476 has 9.69 GiB memory in use. Of the allocated memory 7.77 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 239/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 3342.675 ± 4.533 ms, Memory: 13332.32 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 5476 has 13.69 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 517.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 240/360 ====================\n",
      "Embed dimension: 512, Num heads: 8, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 5476 has 11.19 GiB memory in use. Of the allocated memory 10.02 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 241/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 139.742 ± 0.285 ms, Memory: 1366.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 5476 has 11.19 GiB memory in use. Of the allocated memory 9.34 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 242/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 281.186 ± 1.024 ms, Memory: 2712.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.55 GiB is free. Process 5476 has 7.19 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 4.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 243/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 568.792 ± 1.199 ms, Memory: 5404.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.05 GiB is free. Process 5476 has 7.69 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 244/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1144.144 ± 2.718 ms, Memory: 10788.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 48.12 MiB is free. Process 5476 has 14.69 GiB memory in use. Of the allocated memory 12.60 GiB is allocated by PyTorch, and 1.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 245/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 5476 has 13.69 GiB memory in use. Of the allocated memory 13.08 GiB is allocated by PyTorch, and 453.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 246/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 127.818 ± 0.310 ms, Memory: 1044.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 48.12 MiB is free. Process 5476 has 14.69 GiB memory in use. Of the allocated memory 13.21 GiB is allocated by PyTorch, and 1.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 247/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 256.500 ± 0.830 ms, Memory: 2068.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.30 GiB is free. Process 5476 has 11.44 GiB memory in use. Of the allocated memory 10.40 GiB is allocated by PyTorch, and 902.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 248/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 518.384 ± 1.769 ms, Memory: 4116.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.30 GiB is free. Process 5476 has 11.44 GiB memory in use. Of the allocated memory 4.77 GiB is allocated by PyTorch, and 6.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 249/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1043.522 ± 2.049 ms, Memory: 8212.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 816.12 MiB is free. Process 5476 has 13.94 GiB memory in use. Of the allocated memory 12.54 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 250/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 12.02 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 251/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 247.240 ± 0.337 ms, Memory: 2262.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 10.09 GiB is allocated by PyTorch, and 1.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 252/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 497.432 ± 0.985 ms, Memory: 4504.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 4.16 GiB is allocated by PyTorch, and 7.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 253/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 998.066 ± 1.668 ms, Memory: 8988.28 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 8.30 GiB is allocated by PyTorch, and 3.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 254/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 255/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 11.08 GiB is allocated by PyTorch, and 2.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 256/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 230.614 ± 0.242 ms, Memory: 1684.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 9.96 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 257/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 462.729 ± 0.659 ms, Memory: 3348.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 3.90 GiB is allocated by PyTorch, and 8.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 258/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 926.703 ± 1.036 ms, Memory: 6676.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 7.77 GiB is allocated by PyTorch, and 4.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 259/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1868.095 ± 2.407 ms, Memory: 13332.27 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 5.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 260/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 10.02 GiB is allocated by PyTorch, and 3.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 261/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 463.971 ± 0.355 ms, Memory: 4054.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 3.59 GiB is allocated by PyTorch, and 9.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 262/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 927.952 ± 0.892 ms, Memory: 8088.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 7.16 GiB is allocated by PyTorch, and 4.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 263/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 11.79 GiB is allocated by PyTorch, and 2.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 264/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 265/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 7.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 266/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 436.628 ± 0.487 ms, Memory: 2964.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.55 GiB is free. Process 5476 has 6.19 GiB memory in use. Of the allocated memory 3.46 GiB is allocated by PyTorch, and 2.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 267/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 873.224 ± 0.913 ms, Memory: 5908.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.55 GiB is free. Process 5476 has 9.19 GiB memory in use. Of the allocated memory 6.90 GiB is allocated by PyTorch, and 2.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 268/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1760.549 ± 1.833 ms, Memory: 11796.29 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 560.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 12.02 GiB is allocated by PyTorch, and 2.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 269/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.55 GiB is free. Process 5476 has 12.19 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 270/360 ====================\n",
      "Embed dimension: 512, Num heads: 16, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.55 GiB is free. Process 5476 has 8.19 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 2.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 271/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 286.850 ± 1.655 ms, Memory: 1264.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.55 GiB is free. Process 5476 has 10.19 GiB memory in use. Of the allocated memory 9.35 GiB is allocated by PyTorch, and 689.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 272/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 579.910 ± 3.258 ms, Memory: 2480.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.55 GiB is free. Process 5476 has 10.19 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 7.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 273/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1157.025 ± 3.426 ms, Memory: 4912.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.55 GiB is free. Process 5476 has 10.19 GiB memory in use. Of the allocated memory 5.29 GiB is allocated by PyTorch, and 4.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 274/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 2318.658 ± 4.441 ms, Memory: 9776.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 12.30 GiB is allocated by PyTorch, and 1.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 275/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.55 GiB is free. Process 5476 has 13.19 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 1013.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 276/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 281.359 ± 1.906 ms, Memory: 1264.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 5476 has 11.19 GiB memory in use. Of the allocated memory 9.10 GiB is allocated by PyTorch, and 1.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 277/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 565.330 ± 1.014 ms, Memory: 2480.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 5476 has 11.19 GiB memory in use. Of the allocated memory 2.16 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 278/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1131.107 ± 3.024 ms, Memory: 4912.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.55 GiB is free. Process 5476 has 11.19 GiB memory in use. Of the allocated memory 4.29 GiB is allocated by PyTorch, and 6.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 279/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 2267.743 ± 4.229 ms, Memory: 9776.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 564.12 MiB is free. Process 5476 has 14.19 GiB memory in use. Of the allocated memory 8.54 GiB is allocated by PyTorch, and 5.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 280/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 5476 has 14.69 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================== COMBINATION 281/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 476.867 ± 1.402 ms, Memory: 1376.89 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.49 GiB is free. Process 5476 has 2.25 GiB memory in use. Of the allocated memory 1.54 GiB is allocated by PyTorch, and 561.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 282/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 953.124 ± 1.252 ms, Memory: 2721.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 11.24 GiB is free. Process 5476 has 3.50 GiB memory in use. Of the allocated memory 3.04 GiB is allocated by PyTorch, and 301.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 283/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1912.954 ± 2.030 ms, Memory: 5410.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.99 GiB is free. Process 5476 has 6.75 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 553.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 284/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 3829.908 ± 4.211 ms, Memory: 10788.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 1012.12 MiB is free. Process 5476 has 13.75 GiB memory in use. Of the allocated memory 12.08 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 285/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 500.12 MiB is free. Process 5476 has 14.25 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 286/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 467.981 ± 1.418 ms, Memory: 1344.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.49 GiB is free. Process 5476 has 2.25 GiB memory in use. Of the allocated memory 1.29 GiB is allocated by PyTorch, and 817.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 287/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 939.396 ± 2.205 ms, Memory: 2624.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 11.49 GiB is free. Process 5476 has 3.25 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 561.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 288/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1877.572 ± 3.377 ms, Memory: 5184.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.49 GiB is free. Process 5476 has 6.25 GiB memory in use. Of the allocated memory 5.04 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 289/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 3762.716 ± 5.180 ms, Memory: 10304.39 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 1012.12 MiB is free. Process 5476 has 13.75 GiB memory in use. Of the allocated memory 13.54 GiB is allocated by PyTorch, and 44.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 290/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 128\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 500.12 MiB is free. Process 5476 has 14.25 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 291/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 840.466 ± 1.456 ms, Memory: 1825.02 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.49 GiB is free. Process 5476 has 4.25 GiB memory in use. Of the allocated memory 1.91 GiB is allocated by PyTorch, and 2.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 292/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1677.655 ± 1.762 ms, Memory: 3617.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.99 GiB is free. Process 5476 has 4.75 GiB memory in use. Of the allocated memory 3.79 GiB is allocated by PyTorch, and 813.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 293/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 3361.809 ± 3.043 ms, Memory: 7202.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.49 GiB is free. Process 5476 has 8.25 GiB memory in use. Of the allocated memory 7.54 GiB is allocated by PyTorch, and 553.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 294/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 6816.036 ± 7.974 ms, Memory: 14372.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 500.12 MiB is free. Process 5476 has 14.25 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 33.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 295/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 500.12 MiB is free. Process 5476 has 14.25 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 53.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 296/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 830.447 ± 1.568 ms, Memory: 1504.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.49 GiB is free. Process 5476 has 4.25 GiB memory in use. Of the allocated memory 1.66 GiB is allocated by PyTorch, and 2.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 297/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1654.145 ± 1.995 ms, Memory: 2912.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.24 GiB is free. Process 5476 has 4.50 GiB memory in use. Of the allocated memory 3.29 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 298/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 3320.624 ± 2.761 ms, Memory: 5728.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.49 GiB is free. Process 5476 has 7.25 GiB memory in use. Of the allocated memory 6.54 GiB is allocated by PyTorch, and 561.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 299/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 6656.678 ± 6.579 ms, Memory: 11360.52 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 498.12 MiB is free. Process 5476 has 14.25 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 50.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 300/360 ====================\n",
      "Embed dimension: 1024, Num heads: 4, Num random features: 256\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 500.12 MiB is free. Process 5476 has 14.25 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 53.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 301/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 282.288 ± 1.348 ms, Memory: 1377.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.49 GiB is free. Process 5476 has 12.25 GiB memory in use. Of the allocated memory 9.54 GiB is allocated by PyTorch, and 2.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 302/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 565.122 ± 0.853 ms, Memory: 2722.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.49 GiB is free. Process 5476 has 12.25 GiB memory in use. Of the allocated memory 3.04 GiB is allocated by PyTorch, and 9.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 303/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1129.464 ± 2.809 ms, Memory: 5412.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.49 GiB is free. Process 5476 has 12.25 GiB memory in use. Of the allocated memory 6.05 GiB is allocated by PyTorch, and 6.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 304/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 2258.268 ± 6.853 ms, Memory: 10792.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 500.12 MiB is free. Process 5476 has 14.25 GiB memory in use. Of the allocated memory 12.08 GiB is allocated by PyTorch, and 2.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 305/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: softmax, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 500.12 MiB is free. Process 5476 has 14.25 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 53.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 306/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 270.765 ± 1.360 ms, Memory: 1328.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.49 GiB is free. Process 5476 has 10.25 GiB memory in use. Of the allocated memory 9.29 GiB is allocated by PyTorch, and 817.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 307/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 1024\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 545.291 ± 0.840 ms, Memory: 2608.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.49 GiB is free. Process 5476 has 10.25 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 7.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 308/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 2048\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 1090.501 ± 2.906 ms, Memory: 5168.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.49 GiB is free. Process 5476 has 10.25 GiB memory in use. Of the allocated memory 5.04 GiB is allocated by PyTorch, and 5.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 309/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 4096\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 2183.497 ± 5.279 ms, Memory: 10288.30 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n",
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 1012.12 MiB is free. Process 5476 has 13.75 GiB memory in use. Of the allocated memory 13.55 GiB is allocated by PyTorch, and 39.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 310/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 64\n",
      "Kernel function: relu, Causal: True, Sequence length: 8192\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error benchmarking RandMultiHeadAttention: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 500.12 MiB is free. Process 5476 has 14.25 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 53.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "==================== COMBINATION 311/360 ====================\n",
      "Embed dimension: 1024, Num heads: 8, Num random features: 128\n",
      "Kernel function: softmax, Causal: True, Sequence length: 512\n",
      "\n",
      "==================== Benchmarking RandMultiHeadAttention ====================\n",
      "\n",
      "=== RandMultiHeadAttention FORWARD PASS BENCHMARK ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandMultiHeadAttention forward: 470.900 ± 1.769 ms, Memory: 1825.33 ± 0.00 MB\n",
      "\n",
      "=== RandMultiHeadAttention BACKWARD PASS BENCHMARK ===\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch._dynamo\n",
    "import torch._inductor.config as config\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# Configure torch\n",
    "config.max_autotune_gemm = False\n",
    "torch._dynamo.config.cache_size_limit = 2**16\n",
    "torch._dynamo.config.accumulated_cache_size_limit = 2**16\n",
    "\n",
    "\n",
    "def is_valid_params(embed_dim, num_heads, num_random_features):\n",
    "    """\n",
    "    Check if parameter combination is valid:\n",
    "    embed_dim must be divisible by num_heads\n",
    "    """\n",
    "    return embed_dim % num_heads == 0\n",
    "\n",
    "\n",
    "class BenchmarkParams:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim=256,\n",
    "        num_heads=8,\n",
    "        num_random_features=128,\n",
    "        batch_size=64,\n",
    "        seq_length=32,\n",
    "        num_runs=200,\n",
    "        warmup=15,\n",
    "        device=torch.device(\"cuda\"),\n",
    "        dtype=torch.float32,\n",
    "    ):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_random_features = num_random_features\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.num_runs = num_runs\n",
    "        self.warmup = warmup\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "\n",
    "def benchmark_model(model, inputs, model_name, params):\n",
    "    """\n",
    "    Generic benchmarking function for any PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to benchmark\n",
    "        inputs: Dictionary of input tensors\n",
    "        model_name: Name of the model for logging\n",
    "        params: Benchmark parameters\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with benchmark results\n",
    "    """\n",
    "    # Compile the model\n",
    "    # model_compiled = torch.compile(\n",
    "    #     model,\n",
    "    #     backend=\"inductor\",\n",
    "    #     fullgraph=True,\n",
    "    #     dynamic=False\n",
    "    # )\n",
    "    model_compiled = model\n",
    "\n",
    "    # Benchmark forward pass\n",
    "    print(f\"\\n=== {model_name} FORWARD PASS BENCHMARK ===\")\n",
    "\n",
    "    # Warmup runs for forward pass\n",
    "    model_compiled.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(params.warmup):\n",
    "            _ = model_compiled(**inputs)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Actual timed runs for forward\n",
    "    forward_times = []\n",
    "    forward_memories = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(params.num_runs):\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            _ = model_compiled(**inputs)\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "\n",
    "            forward_times.append((end - start) * 1000)  # Convert to ms\n",
    "            forward_memories.append(\n",
    "                torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
    "            )  # Convert to MB\n",
    "\n",
    "    mean_forward = np.mean(forward_times)\n",
    "    std_forward = np.std(forward_times)\n",
    "    mean_forward_memory = np.mean(forward_memories)\n",
    "    std_forward_memory = np.std(forward_memories)\n",
    "    print(\n",
    "        f\"{model_name} forward: {mean_forward:.3f} ± {std_forward:.3f} ms, Memory: {mean_forward_memory:.2f} ± {std_forward_memory:.2f} MB\"\n",
    "    )\n",
    "\n",
    "    # Benchmark backward pass\n",
    "    print(f\"\\n=== {model_name} BACKWARD PASS BENCHMARK ===\")\n",
    "\n",
    "    # Get query for backward\n",
    "    query = inputs[\"query\"]\n",
    "\n",
    "    # Warmup runs for backward pass\n",
    "    model_compiled.train()\n",
    "    for _ in range(params.warmup):\n",
    "        out = model_compiled(**inputs)[0]\n",
    "        loss = out.sum()\n",
    "        loss.backward()\n",
    "        query.grad.zero_()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Actual timed runs for backward\n",
    "    backward_times = []\n",
    "    backward_memories = []\n",
    "    for _ in range(params.num_runs):\n",
    "        out = model_compiled(**inputs)[0]\n",
    "        loss = out.sum()\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        loss.backward()\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        backward_times.append((end - start) * 1000)  # Convert to ms\n",
    "        backward_memories.append(\n",
    "            torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
    "        )  # Convert to MB\n",
    "        query.grad.zero_()\n",
    "\n",
    "    mean_backward = np.mean(backward_times)\n",
    "    std_backward = np.std(backward_times)\n",
    "    mean_backward_memory = np.mean(backward_memories)\n",
    "    std_backward_memory = np.std(backward_memories)\n",
    "    print(\n",
    "        f\"{model_name} backward: {mean_backward:.3f} ± {std_backward:.3f} ms, Memory: {mean_backward_memory:.2f} ± {std_backward_memory:.2f} MB\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"forward\": {\n",
    "            \"mean\": mean_forward,\n",
    "            \"std\": std_forward,\n",
    "            \"times\": forward_times,\n",
    "            \"memory_mb\": mean_forward_memory,\n",
    "            \"memory_std\": std_forward_memory,\n",
    "            \"memories\": forward_memories,\n",
    "        },\n",
    "        \"backward\": {\n",
    "            \"mean\": mean_backward,\n",
    "            \"std\": std_backward,\n",
    "            \"times\": backward_times,\n",
    "            \"memory_mb\": mean_backward_memory,\n",
    "            \"memory_std\": std_backward_memory,\n",
    "            \"memories\": backward_memories,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def benchmark_model_factory(model_factory, model_name, params):\n",
    "    """\n",
    "    Benchmark a model using a factory function.\n",
    "\n",
    "    Args:\n",
    "        model_factory: Function that creates the model\n",
    "        model_name: Name of the model for logging\n",
    "        params: Benchmark parameters\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with benchmark results\n",
    "    """\n",
    "    # Create the model\n",
    "    torch.manual_seed(42)\n",
    "    model = model_factory(params)\n",
    "\n",
    "    # Create input tensors for benchmarking\n",
    "    query = torch.randn(\n",
    "        params.batch_size,\n",
    "        params.seq_length,\n",
    "        params.embed_dim,\n",
    "        dtype=params.dtype,\n",
    "        device=params.device,\n",
    "        requires_grad=True,\n",
    "    )\n",
    "    key = torch.randn(\n",
    "        params.batch_size,\n",
    "        params.seq_length,\n",
    "        params.embed_dim,\n",
    "        dtype=params.dtype,\n",
    "        device=params.device,\n",
    "    )\n",
    "    value = torch.randn(\n",
    "        params.batch_size,\n",
    "        params.seq_length,\n",
    "        params.embed_dim,\n",
    "        dtype=params.dtype,\n",
    "        device=params.device,\n",
    "    )\n",
    "\n",
    "    inputs = {\"query\": query, \"key\": key, \"value\": value, \"attention_mask\": None}\n",
    "\n",
    "    return benchmark_model(model, inputs, model_name, params)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import torch.nn as nn\n",
    "    from panther.nn.attention import RandMultiHeadAttention\n",
    "\n",
    "    # Parameter combinations to test\n",
    "    embed_dims = [128, 256, 512, 1024]\n",
    "    num_heads_options = [4, 8, 16]\n",
    "    num_random_features_options = [64, 128, 256]\n",
    "    kernel_fn_options = [\"softmax\", \"relu\"]\n",
    "    causal_options = [True]\n",
    "    # causal_options = [False, True]\n",
    "    seq_lens = [512, 1024, 2048, 4096, 8192]\n",
    "\n",
    "    # Define model factories\n",
    "    def create_attention(p):\n",
    "        return RandMultiHeadAttention(\n",
    "            embed_dim=p.embed_dim,\n",
    "            num_heads=p.num_heads,\n",
    "            num_random_features=p.num_random_features,\n",
    "            dropout=0.0,\n",
    "            kernel_fn=p.kernel_fn if hasattr(p, \"kernel_fn\") else \"softmax\",\n",
    "            iscausal=p.iscausal if hasattr(p, \"iscausal\") else False,\n",
    "            device=p.device,\n",
    "            dtype=p.dtype,\n",
    "        )\n",
    "\n",
    "    models_to_benchmark = [(create_attention, \"RandMultiHeadAttention\")]\n",
    "\n",
    "    # Prepare data structure to store all results\n",
    "    results_data = []\n",
    "\n",
    "    # Iterate through all parameter combinations\n",
    "    total_combinations = (\n",
    "        len(embed_dims)\n",
    "        * len(num_heads_options)\n",
    "        * len(num_random_features_options)\n",
    "        * len(kernel_fn_options)\n",
    "        * len(causal_options)\n",
    "        * len(seq_lens)\n",
    "    )\n",
    "    current_combo = 0\n",
    "\n",
    "    for (\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        num_random_features,\n",
    "        kernel_fn,\n",
    "        iscausal,\n",
    "        seq_length,\n",
    "    ) in itertools.product(\n",
    "        embed_dims,\n",
    "        num_heads_options,\n",
    "        num_random_features_options,\n",
    "        kernel_fn_options,\n",
    "        causal_options,\n",
    "        seq_lens,\n",
    "    ):\n",
    "        current_combo += 1\n",
    "        print(f\"\\n\\n{'='*20} COMBINATION {current_combo}/{total_combinations} {'='*20}\")\n",
    "        print(\n",
    "            f\"Embed dimension: {embed_dim}, Num heads: {num_heads}, Num random features: {num_random_features}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Kernel function: {kernel_fn}, Causal: {iscausal}, Sequence length: {seq_length}\"\n",
    "        )\n",
    "\n",
    "        # Check if parameters are valid\n",
    "        is_valid = is_valid_params(embed_dim, num_heads, num_random_features)\n",
    "\n",
    "        if not is_valid:\n",
    "            print(f\"INVALID COMBINATION: {embed_dim} is not divisible by {num_heads}\")\n",
    "            print(\"Skipping benchmarks for this invalid combination\")\n",
    "\n",
    "            # Add invalid entry to results data\n",
    "            for model_name in [m[1] for m in models_to_benchmark]:\n",
    "                results_data.append(\n",
    "                    {\n",
    "                        \"model\": model_name,\n",
    "                        \"embed_dim\": embed_dim,\n",
    "                        \"num_heads\": num_heads,\n",
    "                        \"num_random_features\": num_random_features,\n",
    "                        \"kernel_fn\": kernel_fn,\n",
    "                        \"iscausal\": iscausal,\n",
    "                        \"seq_length\": seq_length,\n",
    "                        \"forward_mean_ms\": float(\"nan\"),\n",
    "                        \"forward_std_ms\": float(\"nan\"),\n",
    "                        \"backward_mean_ms\": float(\"nan\"),\n",
    "                        \"backward_std_ms\": float(\"nan\"),\n",
    "                        \"forward_memory_mb\": float(\"nan\"),\n",
    "                        \"backward_memory_mb\": float(\"nan\"),\n",
    "                        \"is_valid\": False,\n",
    "                        \"error\": \"Invalid parameter combination\",\n",
    "                    }\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        # Create parameter object for this combination\n",
    "        params = BenchmarkParams(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_random_features=num_random_features,\n",
    "            seq_length=seq_length,\n",
    "        )\n",
    "        # Add the new parameters\n",
    "        params.kernel_fn = kernel_fn\n",
    "        params.iscausal = iscausal\n",
    "\n",
    "        all_results = {}\n",
    "        for model_factory, model_name in models_to_benchmark:\n",
    "            print(f\"\\n{'='*20} Benchmarking {model_name} {'='*20}\")\n",
    "            try:\n",
    "                results = benchmark_model_factory(model_factory, model_name, params)\n",
    "                all_results[model_name] = results\n",
    "\n",
    "                # Add result to our data collection\n",
    "                results_data.append(\n",
    "                    {\n",
    "                        \"model\": model_name,\n",
    "                        \"embed_dim\": embed_dim,\n",
    "                        \"num_heads\": num_heads,\n",
    "                        \"num_random_features\": num_random_features,\n",
    "                        \"kernel_fn\": kernel_fn,\n",
    "                        \"iscausal\": iscausal,\n",
    "                        \"seq_length\": seq_length,\n",
    "                        \"forward_mean_ms\": results[\"forward\"][\"mean\"],\n",
    "                        \"forward_std_ms\": results[\"forward\"][\"std\"],\n",
    "                        \"backward_mean_ms\": results[\"backward\"][\"mean\"],\n",
    "                        \"backward_std_ms\": results[\"backward\"][\"std\"],\n",
    "                        \"forward_memory_mb\": results[\"forward\"][\"memory_mb\"],\n",
    "                        \"backward_memory_mb\": results[\"backward\"][\"memory_mb\"],\n",
    "                        \"is_valid\": True,\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error benchmarking {model_name}: {e}\")\n",
    "                # Add error entry to data\n",
    "                results_data.append(\n",
    "                    {\n",
    "                        \"model\": model_name,\n",
    "                        \"embed_dim\": embed_dim,\n",
    "                        \"num_heads\": num_heads,\n",
    "                        \"num_random_features\": num_random_features,\n",
    "                        \"kernel_fn\": kernel_fn,\n",
    "                        \"iscausal\": iscausal,\n",
    "                        \"seq_length\": seq_length,\n",
    "                        \"forward_mean_ms\": float(\"nan\"),\n",
    "                        \"forward_std_ms\": float(\"nan\"),\n",
    "                        \"backward_mean_ms\": float(\"nan\"),\n",
    "                        \"backward_std_ms\": float(\"nan\"),\n",
    "                        \"forward_memory_mb\": float(\"nan\"),\n",
    "                        \"backward_memory_mb\": float(\"nan\"),\n",
    "                        \"is_valid\": True,\n",
    "                        \"error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Print comparative summary for this combination\n",
    "        if all_results:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(f\"{'='*20} SUMMARY FOR CURRENT COMBINATION {'='*20}\")\n",
    "            print(\"=\" * 60)\n",
    "            print(\n",
    "                f\"{'Model':<30} {'Forward (ms)':<25} {'Backward (ms)':<25} {'Forward Memory (MB)':<25} {'Backward Memory (MB)':<25}\"\n",
    "            )\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "            for model_name, results in all_results.items():\n",
    "                fwd = f\"{results['forward']['mean']:.3f} ± {results['forward']['std']:.3f}\"\n",
    "                bwd = f\"{results['backward']['mean']:.3f} ± {results['backward']['std']:.3f}\"\n",
    "                fwd_mem = f\"{results['forward']['memory_mb']:.2f}\"\n",
    "                bwd_mem = f\"{results['backward']['memory_mb']:.2f}\"\n",
    "                print(\n",
    "                    f\"{model_name:<30} {fwd:<25} {bwd:<25} {fwd_mem:<25} {bwd_mem:<25}\"\n",
    "                )\n",
    "\n",
    "    # Create a DataFrame with all results\n",
    "    df = pd.DataFrame(results_data)\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_file = \"attention_benchmark_results.csv\"\n",
    "    df.to_csv(results_file, index=False)\n",
    "    print(f\"\\nAll benchmark results saved to {results_file}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-13T20:13:43.659174",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}