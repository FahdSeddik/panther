{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"github_repos_wildcard\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:21:24.426854Z","iopub.execute_input":"2025-05-13T20:21:24.427089Z","iopub.status.idle":"2025-05-13T20:21:24.587083Z","shell.execute_reply.started":"2025-05-13T20:21:24.427071Z","shell.execute_reply":"2025-05-13T20:21:24.586344Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"repo_url = f\"https://{token}@github.com/gaserSami/panther.git\"\nbranch = \"autotuner\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:21:24.588351Z","iopub.execute_input":"2025-05-13T20:21:24.588562Z","iopub.status.idle":"2025-05-13T20:21:24.591943Z","shell.execute_reply.started":"2025-05-13T20:21:24.588546Z","shell.execute_reply":"2025-05-13T20:21:24.591348Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!git clone -b {branch} {repo_url}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:21:24.592547Z","iopub.execute_input":"2025-05-13T20:21:24.592716Z","iopub.status.idle":"2025-05-13T20:21:28.426028Z","shell.execute_reply.started":"2025-05-13T20:21:24.592702Z","shell.execute_reply":"2025-05-13T20:21:28.425205Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'panther'...\nremote: Enumerating objects: 1473, done.\u001b[K\nremote: Counting objects: 100% (369/369), done.\u001b[K\nremote: Compressing objects: 100% (110/110), done.\u001b[K\nremote: Total 1473 (delta 299), reused 298 (delta 259), pack-reused 1104 (from 1)\u001b[K\nReceiving objects: 100% (1473/1473), 31.55 MiB | 21.07 MiB/s, done.\nResolving deltas: 100% (957/957), done.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!mv panther Panther","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:21:28.427922Z","iopub.execute_input":"2025-05-13T20:21:28.428160Z","iopub.status.idle":"2025-05-13T20:21:28.562732Z","shell.execute_reply.started":"2025-05-13T20:21:28.428139Z","shell.execute_reply":"2025-05-13T20:21:28.561902Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# First uninstall existing torch, torchvision, torchaudio\n!pip uninstall -y torch torchvision torchaudio\n\n# Install the specified versions from PyTorch's official CUDA 12.4 wheels\n!pip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:21:28.564048Z","iopub.execute_input":"2025-05-13T20:21:28.564337Z","iopub.status.idle":"2025-05-13T20:24:32.287395Z","shell.execute_reply.started":"2025-05-13T20:21:28.564307Z","shell.execute_reply":"2025-05-13T20:24:32.286482Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.5.1+cu124\nUninstalling torch-2.5.1+cu124:\n  Successfully uninstalled torch-2.5.1+cu124\nFound existing installation: torchvision 0.20.1+cu124\nUninstalling torchvision-0.20.1+cu124:\n  Successfully uninstalled torchvision-0.20.1+cu124\nFound existing installation: torchaudio 2.5.1+cu124\nUninstalling torchaudio-2.5.1+cu124:\n  Successfully uninstalled torchaudio-2.5.1+cu124\nLooking in indexes: https://download.pytorch.org/whl/cu124\nCollecting torch==2.6.0+cu124\n  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (28 kB)\nCollecting torchvision==0.21.0+cu124\n  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\nCollecting torchaudio==2.6.0+cu124\n  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting triton==3.2.0 (from torch==2.6.0+cu124)\n  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (1.13.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (11.1.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0+cu124) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0+cu124) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.21.0+cu124) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0+cu124) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.21.0+cu124) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.21.0+cu124) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.21.0+cu124) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.21.0+cu124) (2024.2.0)\nDownloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (768.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.5/768.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl (7.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchaudio, torchvision\n  Attempting uninstall: triton\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu124 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nvjitlink-cu12-12.4.127 torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124 triton-3.2.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nprint(torch.__version__)\nimport triton\nprint(triton.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:24:32.288532Z","iopub.execute_input":"2025-05-13T20:24:32.288778Z","iopub.status.idle":"2025-05-13T20:24:34.647967Z","shell.execute_reply.started":"2025-05-13T20:24:32.288746Z","shell.execute_reply":"2025-05-13T20:24:34.647427Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu124\n3.2.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# !export LC_ALL=\"en_US.UTF-8\"\n# !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n# !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n# !ldconfig /usr/lib64-nvidia","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:24:34.648616Z","iopub.execute_input":"2025-05-13T20:24:34.648949Z","iopub.status.idle":"2025-05-13T20:24:34.652487Z","shell.execute_reply.started":"2025-05-13T20:24:34.648922Z","shell.execute_reply":"2025-05-13T20:24:34.651705Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"%%writefile /kaggle/working/Panther/pawX/setup.py\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name=\"pawX\",\n    ext_modules=[\n        CUDAExtension(\n            name=\"pawX\",\n            sources=[\n                \"skops.cpp\",\n                \"bindings.cpp\",\n                \"linear.cpp\",\n                \"linear_cuda.cu\",\n                \"cqrrpt.cpp\",\n                \"rsvd.cpp\",\n                \"attention.cpp\",\n                \"conv2d.cpp\"\n            ],\n            # Use system includes and libraries\n            include_dirs=[\"/usr/include/x86_64-linux-gnu\"],\n            library_dirs=[],\n            libraries=[\"openblas\"],\n            extra_compile_args={\"cxx\": [\"-O2\", \"-fopenmp\"], \"nvcc\": [\"-O2\"]},\n            extra_link_args=[\"-llapacke\", \"-lopenblas\"]\n        )\n    ],\n    cmdclass={\"build_ext\": BuildExtension},\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:24:34.653191Z","iopub.execute_input":"2025-05-13T20:24:34.653479Z","iopub.status.idle":"2025-05-13T20:24:34.679835Z","shell.execute_reply.started":"2025-05-13T20:24:34.653435Z","shell.execute_reply":"2025-05-13T20:24:34.679234Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/Panther/pawX/setup.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!sudo apt-get install liblapacke-dev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:24:34.680444Z","iopub.execute_input":"2025-05-13T20:24:34.680661Z","iopub.status.idle":"2025-05-13T20:24:47.829919Z","shell.execute_reply.started":"2025-05-13T20:24:34.680645Z","shell.execute_reply":"2025-05-13T20:24:47.829225Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  liblapacke libtmglib-dev libtmglib3\nSuggested packages:\n  liblapack-doc\nThe following NEW packages will be installed:\n  liblapacke liblapacke-dev libtmglib-dev libtmglib3\n0 upgraded, 4 newly installed, 0 to remove and 122 not upgraded.\nNeed to get 1,071 kB of archives.\nAfter this operation, 12.3 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtmglib3 amd64 3.10.0-2ubuntu1 [144 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblapacke amd64 3.10.0-2ubuntu1 [435 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtmglib-dev amd64 3.10.0-2ubuntu1 [134 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblapacke-dev amd64 3.10.0-2ubuntu1 [358 kB]\nFetched 1,071 kB in 1s (999 kB/s)       \ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\ndebconf: falling back to frontend: Readline\nSelecting previously unselected package libtmglib3:amd64.\n(Reading database ... 128691 files and directories currently installed.)\nPreparing to unpack .../libtmglib3_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking libtmglib3:amd64 (3.10.0-2ubuntu1) ...\nSelecting previously unselected package liblapacke:amd64.\nPreparing to unpack .../liblapacke_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking liblapacke:amd64 (3.10.0-2ubuntu1) ...\nSelecting previously unselected package libtmglib-dev:amd64.\nPreparing to unpack .../libtmglib-dev_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking libtmglib-dev:amd64 (3.10.0-2ubuntu1) ...\nSelecting previously unselected package liblapacke-dev:amd64.\nPreparing to unpack .../liblapacke-dev_3.10.0-2ubuntu1_amd64.deb ...\nUnpacking liblapacke-dev:amd64 (3.10.0-2ubuntu1) ...\nSetting up libtmglib3:amd64 (3.10.0-2ubuntu1) ...\nSetting up liblapacke:amd64 (3.10.0-2ubuntu1) ...\nSetting up libtmglib-dev:amd64 (3.10.0-2ubuntu1) ...\nSetting up liblapacke-dev:amd64 (3.10.0-2ubuntu1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!cd /kaggle/working/Panther/pawX; python setup.py install\n!cd /kaggle/working/Panther/pawX; pip install --no-build-isolation -e .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:24:47.832672Z","iopub.execute_input":"2025-05-13T20:24:47.833396Z","iopub.status.idle":"2025-05-13T20:27:39.214171Z","shell.execute_reply.started":"2025-05-13T20:24:47.833364Z","shell.execute_reply":"2025-05-13T20:27:39.213445Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` and ``easy_install``.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://github.com/pypa/setuptools/issues/917 for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nEmitting ninja build file /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/build.ninja...\nCompiling objects...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/linear.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[2/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/cqrrpt.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/cqrrpt.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[3/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/conv2d.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/conv2d.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/conv2d.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n/kaggle/working/Panther/pawX/conv2d.cpp: In function ‘at::Tensor sketched_conv2d_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const std::vector<long int>&, const std::vector<long int>&, const std::vector<long int>&, const std::optional<at::Tensor>&)’:\n/kaggle/working/Panther/pawX/conv2d.cpp:17:28: warning: unused variable ‘C’ [-Wunused-variable]\n   17 |     int64_t B = x.size(0), C = x.size(1), H = x.size(2), W = x.size(3);\n      |                            ^\n[4/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/attention.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/attention.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[5/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/bindings.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/bindings.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\nIn file included from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/Exceptions.h:12,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/python.h:11,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/extension.h:9,\n                 from /kaggle/working/Panther/pawX/attention.h:3,\n                 from /kaggle/working/Panther/pawX/bindings.cpp:1:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘class pybind11::class_<DistributionFamily>’:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2216:7:   required from ‘class pybind11::enum_<DistributionFamily>’\n/kaggle/working/Panther/pawX/bindings.cpp:26:60:   required from here\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1539:7: warning: ‘pybind11::class_<DistributionFamily>’ declared with greater visibility than its base ‘pybind11::detail::generic_type’ [-Wattributes]\n 1539 | class class_ : public detail::generic_type {\n      |       ^~~~~~\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘pybind11::class_< <template-parameter-1-1>, <template-parameter-1-2> >::class_(pybind11::handle, const char*, const Extra& ...) [with Extra = {}; type_ = DistributionFamily; options = {}]’:\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:2232:67:   required from ‘pybind11::enum_<Type>::enum_(const pybind11::handle&, const char*, const Extra& ...) [with Extra = {}; Type = DistributionFamily]’\n/kaggle/working/Panther/pawX/bindings.cpp:26:60:   required from here\n/usr/local/lib/python3.11/dist-packages/torch/include/pybind11/pybind11.h:1599:28: warning: ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>’ declared with greater visibility than the type of its field ‘pybind11::class_<DistributionFamily>::class_<>(pybind11::handle, const char*)::<lambda(pybind11::detail::internals&)>::<record capture>’ [-Wattributes]\n 1599 |             with_internals([&](internals &internals) {\n      |                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1600 |                 auto &instances = record.module_local ? get_local_internals().registered_types_cpp\n      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1601 |                                                       : internals.registered_types_cpp;\n      |                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1602 |                 instances[std::type_index(typeid(type_alias))]\n      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1603 |                     = instances[std::type_index(typeid(type))];\n      |                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n 1604 |             });\n      |             ~               \n[6/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/rsvd.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/rsvd.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[7/8] c++ -MMD -MF /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/skops.cpp -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/skops.o -O2 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[8/8] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o.d -I/usr/include/x86_64-linux-gnu -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/Panther/pawX/linear_cuda.cu -o /kaggle/working/Panther/pawX/build/temp.linux-x86_64-cpython-311/linear_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=pawX -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pawX-0.0.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n\u001b[0mObtaining file:///kaggle/working/Panther/pawX\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nInstalling collected packages: pawX\n  Attempting uninstall: pawX\n    Found existing installation: pawX 0.0.0\n    Uninstalling pawX-0.0.0:\n      Successfully uninstalled pawX-0.0.0\n  Running setup.py develop for pawX\nSuccessfully installed pawX-0.0.0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:27:39.215227Z","iopub.execute_input":"2025-05-13T20:27:39.215536Z","iopub.status.idle":"2025-05-13T20:27:39.219924Z","shell.execute_reply.started":"2025-05-13T20:27:39.215506Z","shell.execute_reply":"2025-05-13T20:27:39.219207Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/Panther/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:27:39.220705Z","iopub.execute_input":"2025-05-13T20:27:39.220912Z","iopub.status.idle":"2025-05-13T20:27:39.240167Z","shell.execute_reply.started":"2025-05-13T20:27:39.220896Z","shell.execute_reply":"2025-05-13T20:27:39.239684Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:27:39.240931Z","iopub.execute_input":"2025-05-13T20:27:39.241161Z","iopub.status.idle":"2025-05-13T20:27:39.380179Z","shell.execute_reply.started":"2025-05-13T20:27:39.241137Z","shell.execute_reply":"2025-05-13T20:27:39.379463Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/Panther\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!pip install botorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:27:53.165472Z","iopub.execute_input":"2025-05-13T20:27:53.166047Z","iopub.status.idle":"2025-05-13T20:27:58.931013Z","shell.execute_reply.started":"2025-05-13T20:27:53.166024Z","shell.execute_reply":"2025-05-13T20:27:58.929662Z"}},"outputs":[{"name":"stdout","text":"Collecting botorch\n  Downloading botorch-0.14.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from botorch) (4.13.1)\nCollecting pyre_extensions (from botorch)\n  Downloading pyre_extensions-0.0.32-py3-none-any.whl.metadata (4.0 kB)\nCollecting gpytorch==1.14 (from botorch)\n  Downloading gpytorch-1.14-py3-none-any.whl.metadata (8.0 kB)\nCollecting linear_operator==0.6 (from botorch)\n  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from botorch) (2.6.0+cu124)\nCollecting pyro-ppl>=1.8.4 (from botorch)\n  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from botorch) (1.15.2)\nRequirement already satisfied: multipledispatch in /usr/local/lib/python3.11/dist-packages (from botorch) (1.0.0)\nRequirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from botorch) (3.6.0)\nCollecting jaxtyping (from gpytorch==1.14->botorch)\n  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.11/dist-packages (from gpytorch==1.14->botorch) (1.3.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from gpytorch==1.14->botorch) (1.2.2)\nRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (3.4.0)\nCollecting pyro-api>=0.1.1 (from pyro-ppl>=1.8.4->botorch)\n  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (1.13.1)\nRequirement already satisfied: typing-inspect in /usr/local/lib/python3.11/dist-packages (from pyre_extensions->botorch) (0.9.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2.4.1)\nCollecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch==1.14->botorch)\n  Downloading wadler_lindig-0.1.5-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.1->botorch) (3.0.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch==1.14->botorch) (1.4.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect->pyre_extensions->botorch) (1.0.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.7->pyro-ppl>=1.8.4->botorch) (2024.2.0)\nDownloading botorch-0.14.0-py3-none-any.whl (738 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m738.3/738.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading gpytorch-1.14-py3-none-any.whl (277 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.7/277.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading linear_operator-0.6-py3-none-any.whl (176 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyre_extensions-0.0.32-py3-none-any.whl (12 kB)\nDownloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\nDownloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wadler_lindig-0.1.5-py3-none-any.whl (20 kB)\nInstalling collected packages: pyro-api, wadler-lindig, pyre_extensions, jaxtyping, linear_operator, pyro-ppl, gpytorch, botorch\nSuccessfully installed botorch-0.14.0 gpytorch-1.14 jaxtyping-0.3.2 linear_operator-0.6 pyre_extensions-0.0.32 pyro-api-0.1.2 pyro-ppl-1.9.1 wadler-lindig-0.1.5\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import time\nimport numpy as np\nimport torch\nimport torch._dynamo\nimport torch._inductor.config as config\nimport itertools\nimport pandas as pd\n\n# Configure torch\nconfig.max_autotune_gemm = False\ntorch._dynamo.config.cache_size_limit = 2**16\ntorch._dynamo.config.accumulated_cache_size_limit = 2**16\n\ndef is_valid_params(embed_dim, num_heads, num_random_features):\n    \"\"\"\n    Check if parameter combination is valid:\n    embed_dim must be divisible by num_heads\n    \"\"\"\n    return embed_dim % num_heads == 0\n\nclass BenchmarkParams:\n    def __init__(self, \n                 embed_dim=256,\n                 num_heads=8,\n                 num_random_features=128,\n                 batch_size=64, \n                 seq_length=32,\n                 num_runs=200, \n                 warmup=15, \n                 device=torch.device(\"cuda\"),\n                 dtype=torch.float32):\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.num_random_features = num_random_features\n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.num_runs = num_runs\n        self.warmup = warmup\n        self.device = device\n        self.dtype = dtype\n\ndef benchmark_model(model, inputs, model_name, params):\n    \"\"\"\n    Generic benchmarking function for any PyTorch model.\n    \n    Args:\n        model: The PyTorch model to benchmark\n        inputs: Dictionary of input tensors\n        model_name: Name of the model for logging\n        params: Benchmark parameters\n    \n    Returns:\n        Dictionary with benchmark results\n    \"\"\"\n    # Compile the model\n    # model_compiled = torch.compile(\n    #     model,\n    #     backend=\"inductor\",\n    #     fullgraph=True,\n    #     dynamic=False\n    # )\n    model_compiled = model\n    \n    # Benchmark forward pass\n    print(f\"\\n=== {model_name} FORWARD PASS BENCHMARK ===\")\n    \n    # Warmup runs for forward pass\n    model_compiled.eval()\n    with torch.no_grad():\n        for _ in range(params.warmup):\n            _ = model_compiled(**inputs)\n    \n    torch.cuda.synchronize()\n    \n    # Actual timed runs for forward\n    forward_times = []\n    forward_memories = []\n    with torch.no_grad():\n        for _ in range(params.num_runs):\n            torch.cuda.reset_peak_memory_stats()\n            torch.cuda.synchronize()\n            start = time.perf_counter()\n            _ = model_compiled(**inputs)\n            torch.cuda.synchronize()\n            end = time.perf_counter()\n            \n            forward_times.append((end - start) * 1000)  # Convert to ms\n            forward_memories.append(torch.cuda.max_memory_allocated() / (1024 * 1024))  # Convert to MB\n    \n    mean_forward = np.mean(forward_times)\n    std_forward = np.std(forward_times)\n    mean_forward_memory = np.mean(forward_memories)\n    std_forward_memory = np.std(forward_memories)\n    print(f\"{model_name} forward: {mean_forward:.3f} ± {std_forward:.3f} ms, Memory: {mean_forward_memory:.2f} ± {std_forward_memory:.2f} MB\")\n    \n    # Benchmark backward pass\n    print(f\"\\n=== {model_name} BACKWARD PASS BENCHMARK ===\")\n    \n    # Get query for backward\n    query = inputs['query']\n    \n    # Warmup runs for backward pass\n    model_compiled.train()\n    for _ in range(params.warmup):\n        out = model_compiled(**inputs)[0]\n        loss = out.sum()\n        loss.backward()\n        query.grad.zero_()\n    \n    torch.cuda.synchronize()\n    \n    # Actual timed runs for backward\n    backward_times = []\n    backward_memories = []\n    for _ in range(params.num_runs):\n        out = model_compiled(**inputs)[0]\n        loss = out.sum()\n        \n        torch.cuda.reset_peak_memory_stats()\n        torch.cuda.synchronize()\n        start = time.perf_counter()\n        loss.backward()\n        torch.cuda.synchronize()\n        end = time.perf_counter()\n        \n        backward_times.append((end - start) * 1000)  # Convert to ms\n        backward_memories.append(torch.cuda.max_memory_allocated() / (1024 * 1024))  # Convert to MB\n        query.grad.zero_()\n    \n    mean_backward = np.mean(backward_times)\n    std_backward = np.std(backward_times)\n    mean_backward_memory = np.mean(backward_memories)\n    std_backward_memory = np.std(backward_memories)\n    print(f\"{model_name} backward: {mean_backward:.3f} ± {std_backward:.3f} ms, Memory: {mean_backward_memory:.2f} ± {std_backward_memory:.2f} MB\")\n    \n    return {\n        \"forward\": {\n            \"mean\": mean_forward,\n            \"std\": std_forward,\n            \"times\": forward_times,\n            \"memory_mb\": mean_forward_memory,\n            \"memory_std\": std_forward_memory,\n            \"memories\": forward_memories\n        },\n        \"backward\": {\n            \"mean\": mean_backward,\n            \"std\": std_backward,\n            \"times\": backward_times,\n            \"memory_mb\": mean_backward_memory,\n            \"memory_std\": std_backward_memory,\n            \"memories\": backward_memories\n        }\n    }\n\ndef benchmark_model_factory(model_factory, model_name, params):\n    \"\"\"\n    Benchmark a model using a factory function.\n    \n    Args:\n        model_factory: Function that creates the model\n        model_name: Name of the model for logging\n        params: Benchmark parameters\n    \n    Returns:\n        Dictionary with benchmark results\n    \"\"\"\n    # Create the model\n    torch.manual_seed(42)\n    model = model_factory(params)\n    \n    # Create input tensors for benchmarking\n    query = torch.randn(params.batch_size, params.seq_length, params.embed_dim, \n                      dtype=params.dtype, device=params.device, requires_grad=True)\n    key = torch.randn(params.batch_size, params.seq_length, params.embed_dim, \n                     dtype=params.dtype, device=params.device)\n    value = torch.randn(params.batch_size, params.seq_length, params.embed_dim, \n                       dtype=params.dtype, device=params.device)\n    \n    inputs = {\n        'query': query,\n        'key': key,\n        'value': value,\n    }\n\n    if isinstance(model, torch.nn.MultiheadAttention):\n        seq_len = params.seq_length\n        attn_mask = torch.nn.Transformer.generate_square_subsequent_mask(seq_len).to(params.device)\n        inputs[\"attn_mask\"] = attn_mask\n        inputs[\"is_causal\"] = params.iscausal\n    \n    return benchmark_model(model, inputs, model_name, params)\n\nif __name__ == \"__main__\":\n    import torch.nn as nn\n    from panther.nn.attention import RandMultiHeadAttention\n    \n    # Parameter combinations to test\n    embed_dims = [128, 256, 512, 1024]\n    num_heads_options = [4, 8, 16]\n    num_random_features_options = [64, 128, 256]\n    kernel_fn_options = [\"softmax\", \"relu\"]\n    causal_options = [True]\n    # causal_options = [False, True]\n    seq_lens = [512, 1024, 2048, 4096, 8192]\n    \n    # Define model factories\n    def create_attention(p):\n        return RandMultiHeadAttention(\n            embed_dim=p.embed_dim,\n            num_heads=p.num_heads,\n            num_random_features=p.num_random_features,\n            dropout=0.0,\n            kernel_fn=p.kernel_fn if hasattr(p, 'kernel_fn') else \"softmax\",\n            iscausal=p.iscausal if hasattr(p, 'iscausal') else False,\n            device=p.device,\n            dtype=p.dtype\n        )\n    \n    def create_torch_attention(p):\n        return torch.nn.MultiheadAttention(\n            embed_dim=p.embed_dim,\n            num_heads=p.num_heads,\n            dropout=0.0,\n            batch_first=True,  # Since your inputs are [batch, seq, dim]\n            device=p.device,\n            dtype=p.dtype\n        )\n    \n    models_to_benchmark = [\n        (create_torch_attention, \"attention\")\n    ]\n    \n    # Prepare data structure to store all results\n    results_data = []\n    \n    # Iterate through all parameter combinations\n    total_combinations = len(embed_dims) * len(num_heads_options) * len(num_random_features_options) * len(kernel_fn_options) * len(causal_options) * len(seq_lens)\n    current_combo = 0\n    \n    for embed_dim, num_heads, num_random_features, kernel_fn, iscausal, seq_length in itertools.product(\n        embed_dims, num_heads_options, num_random_features_options, kernel_fn_options, causal_options, seq_lens\n    ):\n        current_combo += 1\n        print(f\"\\n\\n{'='*20} COMBINATION {current_combo}/{total_combinations} {'='*20}\")\n        print(f\"Embed dimension: {embed_dim}, Num heads: {num_heads}, Num random features: {num_random_features}\")\n        print(f\"Kernel function: {kernel_fn}, Causal: {iscausal}, Sequence length: {seq_length}\")\n        \n        # Check if parameters are valid\n        is_valid = is_valid_params(embed_dim, num_heads, num_random_features)\n        \n        if not is_valid:\n            print(f\"INVALID COMBINATION: {embed_dim} is not divisible by {num_heads}\")\n            print(\"Skipping benchmarks for this invalid combination\")\n            \n            # Add invalid entry to results data\n            for model_name in [m[1] for m in models_to_benchmark]:\n                results_data.append({\n                    'model': model_name,\n                    'embed_dim': embed_dim,\n                    'num_heads': num_heads,\n                    'num_random_features': num_random_features,\n                    'kernel_fn': kernel_fn,\n                    'iscausal': iscausal,\n                    'seq_length': seq_length,\n                    'forward_mean_ms': float('nan'),\n                    'forward_std_ms': float('nan'),\n                    'backward_mean_ms': float('nan'),\n                    'backward_std_ms': float('nan'),\n                    'forward_memory_mb': float('nan'),\n                    'backward_memory_mb': float('nan'),\n                    'is_valid': False,\n                    'error': \"Invalid parameter combination\"\n                })\n            continue\n        \n        # Create parameter object for this combination\n        params = BenchmarkParams(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            num_random_features=num_random_features,\n            seq_length=seq_length\n        )\n        # Add the new parameters\n        params.kernel_fn = kernel_fn\n        params.iscausal = iscausal\n        \n        all_results = {}\n        for model_factory, model_name in models_to_benchmark:\n            print(f\"\\n{'='*20} Benchmarking {model_name} {'='*20}\")\n            try:\n                results = benchmark_model_factory(model_factory, model_name, params)\n                all_results[model_name] = results\n                \n                # Add result to our data collection\n                results_data.append({\n                    'model': model_name,\n                    'embed_dim': embed_dim,\n                    'num_heads': num_heads,\n                    'num_random_features': num_random_features,\n                    'kernel_fn': kernel_fn,\n                    'iscausal': iscausal,\n                    'seq_length': seq_length,\n                    'forward_mean_ms': results['forward']['mean'],\n                    'forward_std_ms': results['forward']['std'],\n                    'backward_mean_ms': results['backward']['mean'],\n                    'backward_std_ms': results['backward']['std'],\n                    'forward_memory_mb': results['forward']['memory_mb'],\n                    'backward_memory_mb': results['backward']['memory_mb'],\n                    'is_valid': True\n                })\n            except Exception as e:\n                print(f\"Error benchmarking {model_name}: {e}\")\n                # Add error entry to data\n                results_data.append({\n                    'model': model_name,\n                    'embed_dim': embed_dim,\n                    'num_heads': num_heads,\n                    'num_random_features': num_random_features,\n                    'kernel_fn': kernel_fn, \n                    'iscausal': iscausal,\n                    'seq_length': seq_length,\n                    'forward_mean_ms': float('nan'),\n                    'forward_std_ms': float('nan'),\n                    'backward_mean_ms': float('nan'),\n                    'backward_std_ms': float('nan'),\n                    'forward_memory_mb': float('nan'),\n                    'backward_memory_mb': float('nan'),\n                    'is_valid': True,\n                    'error': str(e)\n                })\n        \n        # Print comparative summary for this combination\n        if all_results:\n            print(\"\\n\" + \"=\"*60)\n            print(f\"{'='*20} SUMMARY FOR CURRENT COMBINATION {'='*20}\")\n            print(\"=\"*60)\n            print(f\"{'Model':<30} {'Forward (ms)':<25} {'Backward (ms)':<25} {'Forward Memory (MB)':<25} {'Backward Memory (MB)':<25}\")\n            print(\"-\"*60)\n            \n            for model_name, results in all_results.items():\n                fwd = f\"{results['forward']['mean']:.3f} ± {results['forward']['std']:.3f}\"\n                bwd = f\"{results['backward']['mean']:.3f} ± {results['backward']['std']:.3f}\"\n                fwd_mem = f\"{results['forward']['memory_mb']:.2f}\"\n                bwd_mem = f\"{results['backward']['memory_mb']:.2f}\"\n                print(f\"{model_name:<30} {fwd:<25} {bwd:<25} {fwd_mem:<25} {bwd_mem:<25}\")\n    \n    # Create a DataFrame with all results\n    df = pd.DataFrame(results_data)\n    \n    # Save results to CSV\n    results_file = \"attention_benchmark_results.csv\"\n    df.to_csv(results_file, index=False)\n    print(f\"\\nAll benchmark results saved to {results_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:39:06.892145Z","iopub.execute_input":"2025-05-13T20:39:06.892763Z","iopub.status.idle":"2025-05-13T23:50:19.102879Z","shell.execute_reply.started":"2025-05-13T20:39:06.892740Z","shell.execute_reply":"2025-05-13T23:50:19.102086Z"}},"outputs":[{"name":"stdout","text":"\n\n==================== COMBINATION 1/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 12.361 ± 0.305 ms, Memory: 633.38 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 16.923 ± 0.117 ms, Memory: 1217.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      12.361 ± 0.305            16.923 ± 0.117            633.38                    1217.75                  \n\n\n==================== COMBINATION 2/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 47.980 ± 0.213 ms, Memory: 2292.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 64.732 ± 2.430 ms, Memory: 4468.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      47.980 ± 0.213            64.732 ± 2.430            2292.50                   4468.75                  \n\n\n==================== COMBINATION 3/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 216.669 ± 4.760 ms, Memory: 8672.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.01 GiB is free. Process 2955 has 12.73 GiB memory in use. Of the allocated memory 8.66 GiB is allocated by PyTorch, and 3.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 4/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.07 GiB is free. Process 2955 has 4.66 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 3.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 5/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.51 GiB is free. Process 2955 has 4.23 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 6/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 12.677 ± 0.144 ms, Memory: 641.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 17.460 ± 0.188 ms, Memory: 1217.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      12.677 ± 0.144            17.460 ± 0.188            641.50                    1217.75                  \n\n\n==================== COMBINATION 7/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 48.637 ± 0.353 ms, Memory: 2292.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 66.302 ± 2.357 ms, Memory: 4468.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      48.637 ± 0.353            66.302 ± 2.357            2292.50                   4468.75                  \n\n\n==================== COMBINATION 8/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 212.988 ± 1.620 ms, Memory: 8672.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.66 GiB is allocated by PyTorch, and 449.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 9/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.51 GiB is free. Process 2955 has 1.23 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 145.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 10/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.51 GiB is free. Process 2955 has 2.23 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 81.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 11/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 12.775 ± 0.150 ms, Memory: 641.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 17.652 ± 0.319 ms, Memory: 1217.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      12.775 ± 0.150            17.652 ± 0.319            641.50                    1217.75                  \n\n\n==================== COMBINATION 12/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 49.508 ± 0.505 ms, Memory: 2292.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 67.126 ± 2.105 ms, Memory: 4468.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      49.508 ± 0.505            67.126 ± 2.105            2292.50                   4468.75                  \n\n\n==================== COMBINATION 13/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 212.277 ± 1.224 ms, Memory: 8672.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.76 GiB is free. Process 2955 has 12.98 GiB memory in use. Of the allocated memory 12.66 GiB is allocated by PyTorch, and 193.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 14/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 3.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 15/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 2.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 16/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 12.891 ± 0.153 ms, Memory: 641.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 17.811 ± 0.421 ms, Memory: 1217.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      12.891 ± 0.153            17.811 ± 0.421            641.50                    1217.75                  \n\n\n==================== COMBINATION 17/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 49.358 ± 0.504 ms, Memory: 2292.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 66.346 ± 2.209 ms, Memory: 4468.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      49.358 ± 0.504            66.346 ± 2.209            2292.50                   4468.75                  \n\n\n==================== COMBINATION 18/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 212.416 ± 1.104 ms, Memory: 8672.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.76 GiB is free. Process 2955 has 12.98 GiB memory in use. Of the allocated memory 12.66 GiB is allocated by PyTorch, and 193.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 19/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 3.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 20/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 2.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 21/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 12.939 ± 0.193 ms, Memory: 641.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 17.842 ± 0.441 ms, Memory: 1217.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      12.939 ± 0.193            17.842 ± 0.441            641.50                    1217.75                  \n\n\n==================== COMBINATION 22/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 49.555 ± 0.491 ms, Memory: 2292.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 65.782 ± 2.261 ms, Memory: 4468.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      49.555 ± 0.491            65.782 ± 2.261            2292.50                   4468.75                  \n\n\n==================== COMBINATION 23/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 212.514 ± 0.905 ms, Memory: 8672.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.76 GiB is free. Process 2955 has 12.98 GiB memory in use. Of the allocated memory 12.66 GiB is allocated by PyTorch, and 193.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 24/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 3.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 25/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 2.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 26/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 13.028 ± 0.191 ms, Memory: 641.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 18.040 ± 0.435 ms, Memory: 1217.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      13.028 ± 0.191            18.040 ± 0.435            641.50                    1217.75                  \n\n\n==================== COMBINATION 27/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 49.679 ± 0.540 ms, Memory: 2292.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 66.238 ± 2.225 ms, Memory: 4468.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      49.679 ± 0.540            66.238 ± 2.225            2292.50                   4468.75                  \n\n\n==================== COMBINATION 28/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 212.239 ± 0.759 ms, Memory: 8672.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.76 GiB is free. Process 2955 has 12.98 GiB memory in use. Of the allocated memory 12.66 GiB is allocated by PyTorch, and 193.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 29/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 3.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 30/360 ====================\nEmbed dimension: 128, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 2.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 31/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 21.687 ± 0.172 ms, Memory: 1153.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 30.855 ± 0.324 ms, Memory: 2241.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      21.687 ± 0.172            30.855 ± 0.324            1153.50                   2241.75                  \n\n\n==================== COMBINATION 32/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 89.562 ± 0.589 ms, Memory: 4340.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 116.499 ± 0.994 ms, Memory: 8564.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      89.562 ± 0.589            116.499 ± 0.994           4340.50                   8564.75                  \n\n\n==================== COMBINATION 33/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 8.47 GiB is allocated by PyTorch, and 129.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 34/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 7.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 35/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 36/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 21.683 ± 0.165 ms, Memory: 1153.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 30.915 ± 0.327 ms, Memory: 2241.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      21.683 ± 0.165            30.915 ± 0.327            1153.50                   2241.75                  \n\n\n==================== COMBINATION 37/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 89.596 ± 0.587 ms, Memory: 4340.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 116.945 ± 0.874 ms, Memory: 8564.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      89.596 ± 0.587            116.945 ± 0.874           4340.50                   8564.75                  \n\n\n==================== COMBINATION 38/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 8.47 GiB is allocated by PyTorch, and 129.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 39/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 7.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 40/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 41/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 21.707 ± 0.173 ms, Memory: 1153.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 30.959 ± 0.328 ms, Memory: 2241.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      21.707 ± 0.173            30.959 ± 0.328            1153.50                   2241.75                  \n\n\n==================== COMBINATION 42/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 89.670 ± 0.608 ms, Memory: 4340.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 117.123 ± 0.720 ms, Memory: 8564.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      89.670 ± 0.608            117.123 ± 0.720           4340.50                   8564.75                  \n\n\n==================== COMBINATION 43/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 8.47 GiB is allocated by PyTorch, and 129.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 44/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 7.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 45/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 46/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 21.687 ± 0.160 ms, Memory: 1153.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 30.946 ± 0.346 ms, Memory: 2241.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      21.687 ± 0.160            30.946 ± 0.346            1153.50                   2241.75                  \n\n\n==================== COMBINATION 47/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 89.706 ± 0.612 ms, Memory: 4340.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 117.028 ± 0.849 ms, Memory: 8564.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      89.706 ± 0.612            117.028 ± 0.849           4340.50                   8564.75                  \n\n\n==================== COMBINATION 48/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 8.47 GiB is allocated by PyTorch, and 129.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 49/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 7.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 50/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 51/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 21.690 ± 0.172 ms, Memory: 1153.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 30.970 ± 0.355 ms, Memory: 2241.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      21.690 ± 0.172            30.970 ± 0.355            1153.50                   2241.75                  \n\n\n==================== COMBINATION 52/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 89.705 ± 0.628 ms, Memory: 4340.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 116.790 ± 0.870 ms, Memory: 8564.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      89.705 ± 0.628            116.790 ± 0.870           4340.50                   8564.75                  \n\n\n==================== COMBINATION 53/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 8.47 GiB is allocated by PyTorch, and 129.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 54/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 7.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 55/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 56/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 21.695 ± 0.170 ms, Memory: 1153.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 30.885 ± 0.337 ms, Memory: 2241.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      21.695 ± 0.170            30.885 ± 0.337            1153.50                   2241.75                  \n\n\n==================== COMBINATION 57/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 89.610 ± 0.611 ms, Memory: 4340.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 116.499 ± 0.987 ms, Memory: 8564.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      89.610 ± 0.611            116.499 ± 0.987           4340.50                   8564.75                  \n\n\n==================== COMBINATION 58/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 8.47 GiB is allocated by PyTorch, and 129.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 59/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 7.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 60/360 ====================\nEmbed dimension: 128, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.01 GiB is free. Process 2955 has 8.73 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 61/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 39.818 ± 0.382 ms, Memory: 2177.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 56.955 ± 0.862 ms, Memory: 4289.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      39.818 ± 0.382            56.955 ± 0.862            2177.50                   4289.75                  \n\n\n==================== COMBINATION 62/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 173.068 ± 0.576 ms, Memory: 8436.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.01 GiB is free. Process 2955 has 12.73 GiB memory in use. Of the allocated memory 12.33 GiB is allocated by PyTorch, and 269.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 63/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.01 GiB is free. Process 2955 has 744.00 MiB memory in use. Of the allocated memory 480.50 MiB is allocated by PyTorch, and 129.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 64/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.51 GiB is free. Process 2955 has 1.23 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 145.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 65/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.51 GiB is free. Process 2955 has 2.23 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 81.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 66/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 40.184 ± 0.288 ms, Memory: 2177.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 56.984 ± 0.843 ms, Memory: 4289.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      40.184 ± 0.288            56.984 ± 0.843            2177.50                   4289.75                  \n\n\n==================== COMBINATION 67/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 173.350 ± 0.634 ms, Memory: 8436.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.01 GiB is free. Process 2955 has 12.73 GiB memory in use. Of the allocated memory 12.33 GiB is allocated by PyTorch, and 269.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 68/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.01 GiB is free. Process 2955 has 744.00 MiB memory in use. Of the allocated memory 480.50 MiB is allocated by PyTorch, and 129.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 69/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.51 GiB is free. Process 2955 has 1.23 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 145.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 70/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.51 GiB is free. Process 2955 has 2.23 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 81.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 71/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 40.174 ± 0.290 ms, Memory: 2177.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 57.374 ± 1.012 ms, Memory: 4289.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      40.174 ± 0.290            57.374 ± 1.012            2177.50                   4289.75                  \n\n\n==================== COMBINATION 72/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 173.435 ± 0.600 ms, Memory: 8436.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.01 GiB is free. Process 2955 has 12.73 GiB memory in use. Of the allocated memory 12.33 GiB is allocated by PyTorch, and 269.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 73/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.01 GiB is free. Process 2955 has 744.00 MiB memory in use. Of the allocated memory 480.50 MiB is allocated by PyTorch, and 129.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 74/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.51 GiB is free. Process 2955 has 1.23 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 145.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 75/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.51 GiB is free. Process 2955 has 2.23 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 81.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 76/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 40.255 ± 0.217 ms, Memory: 2177.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 57.030 ± 0.877 ms, Memory: 4289.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      40.255 ± 0.217            57.030 ± 0.877            2177.50                   4289.75                  \n\n\n==================== COMBINATION 77/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 173.089 ± 0.586 ms, Memory: 8436.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.01 GiB is free. Process 2955 has 12.73 GiB memory in use. Of the allocated memory 12.33 GiB is allocated by PyTorch, and 269.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 78/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.01 GiB is free. Process 2955 has 744.00 MiB memory in use. Of the allocated memory 480.50 MiB is allocated by PyTorch, and 129.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 79/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.51 GiB is free. Process 2955 has 1.23 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 145.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 80/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.51 GiB is free. Process 2955 has 2.23 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 81.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 81/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 39.924 ± 0.386 ms, Memory: 2177.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 56.903 ± 0.839 ms, Memory: 4289.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      39.924 ± 0.386            56.903 ± 0.839            2177.50                   4289.75                  \n\n\n==================== COMBINATION 82/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 173.236 ± 0.600 ms, Memory: 8436.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.01 GiB is free. Process 2955 has 12.73 GiB memory in use. Of the allocated memory 12.33 GiB is allocated by PyTorch, and 269.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 83/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.01 GiB is free. Process 2955 has 744.00 MiB memory in use. Of the allocated memory 480.50 MiB is allocated by PyTorch, and 129.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 84/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.51 GiB is free. Process 2955 has 1.23 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 145.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 85/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.51 GiB is free. Process 2955 has 2.23 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 81.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 86/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 40.135 ± 0.353 ms, Memory: 2177.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 57.035 ± 0.869 ms, Memory: 4289.75 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      40.135 ± 0.353            57.035 ± 0.869            2177.50                   4289.75                  \n\n\n==================== COMBINATION 87/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 173.233 ± 0.572 ms, Memory: 8436.50 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.01 GiB is free. Process 2955 has 12.73 GiB memory in use. Of the allocated memory 12.33 GiB is allocated by PyTorch, and 269.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 88/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.01 GiB is free. Process 2955 has 744.00 MiB memory in use. Of the allocated memory 480.50 MiB is allocated by PyTorch, and 129.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 89/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.51 GiB is free. Process 2955 has 1.23 GiB memory in use. Of the allocated memory 976.50 MiB is allocated by PyTorch, and 145.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 90/360 ====================\nEmbed dimension: 128, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.51 GiB is free. Process 2955 has 2.23 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 81.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 91/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 19.894 ± 0.220 ms, Memory: 754.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 28.877 ± 0.587 ms, Memory: 1395.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      19.894 ± 0.220            28.877 ± 0.587            754.25                    1395.26                  \n\n\n==================== COMBINATION 92/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 68.233 ± 2.417 ms, Memory: 2517.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 94.419 ± 2.909 ms, Memory: 4822.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      68.233 ± 2.417            94.419 ± 2.909            2517.25                   4822.26                  \n\n\n==================== COMBINATION 93/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 260.856 ± 1.515 ms, Memory: 9121.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.01 GiB is free. Process 2955 has 13.73 GiB memory in use. Of the allocated memory 13.28 GiB is allocated by PyTorch, and 320.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 94/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.01 GiB is free. Process 2955 has 5.73 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 3.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 95/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.26 GiB is free. Process 2955 has 4.48 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 592.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 96/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 19.938 ± 0.274 ms, Memory: 754.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 28.372 ± 0.535 ms, Memory: 1395.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      19.938 ± 0.274            28.372 ± 0.535            754.25                    1395.26                  \n\n\n==================== COMBINATION 97/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 67.304 ± 2.504 ms, Memory: 2517.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 93.388 ± 2.973 ms, Memory: 4822.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      67.304 ± 2.504            93.388 ± 2.973            2517.25                   4822.26                  \n\n\n==================== COMBINATION 98/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 262.032 ± 1.323 ms, Memory: 9121.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 268.12 MiB is free. Process 2955 has 14.48 GiB memory in use. Of the allocated memory 13.28 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 99/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.26 GiB is free. Process 2955 has 2.48 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 528.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 100/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.76 GiB is free. Process 2955 has 3.98 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 80.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 101/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 19.928 ± 0.267 ms, Memory: 754.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 28.536 ± 0.572 ms, Memory: 1395.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      19.928 ± 0.267            28.536 ± 0.572            754.25                    1395.26                  \n\n\n==================== COMBINATION 102/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 67.738 ± 2.430 ms, Memory: 2517.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 94.576 ± 2.735 ms, Memory: 4822.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      67.738 ± 2.430            94.576 ± 2.735            2517.25                   4822.26                  \n\n\n==================== COMBINATION 103/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 261.874 ± 1.356 ms, Memory: 9121.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 780.12 MiB is free. Process 2955 has 13.98 GiB memory in use. Of the allocated memory 13.28 GiB is allocated by PyTorch, and 576.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 104/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.76 GiB is free. Process 2955 has 5.98 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 4.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 105/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.76 GiB is free. Process 2955 has 5.98 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 106/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 19.956 ± 0.238 ms, Memory: 754.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 28.481 ± 0.535 ms, Memory: 1395.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      19.956 ± 0.238            28.481 ± 0.535            754.25                    1395.26                  \n\n\n==================== COMBINATION 107/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 67.552 ± 2.433 ms, Memory: 2517.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 94.012 ± 2.841 ms, Memory: 4822.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      67.552 ± 2.433            94.012 ± 2.841            2517.25                   4822.26                  \n\n\n==================== COMBINATION 108/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 261.757 ± 1.429 ms, Memory: 9121.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 780.12 MiB is free. Process 2955 has 13.98 GiB memory in use. Of the allocated memory 13.28 GiB is allocated by PyTorch, and 576.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 109/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.76 GiB is free. Process 2955 has 5.98 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 4.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 110/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.76 GiB is free. Process 2955 has 5.98 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 111/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 19.967 ± 0.280 ms, Memory: 754.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 28.515 ± 0.517 ms, Memory: 1395.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      19.967 ± 0.280            28.515 ± 0.517            754.25                    1395.26                  \n\n\n==================== COMBINATION 112/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 67.359 ± 2.413 ms, Memory: 2517.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 93.701 ± 2.922 ms, Memory: 4822.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      67.359 ± 2.413            93.701 ± 2.922            2517.25                   4822.26                  \n\n\n==================== COMBINATION 113/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 261.624 ± 1.365 ms, Memory: 9121.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 780.12 MiB is free. Process 2955 has 13.98 GiB memory in use. Of the allocated memory 13.28 GiB is allocated by PyTorch, and 576.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 114/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.76 GiB is free. Process 2955 has 5.98 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 4.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 115/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.76 GiB is free. Process 2955 has 5.98 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 116/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 19.968 ± 0.257 ms, Memory: 754.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 28.587 ± 0.536 ms, Memory: 1395.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      19.968 ± 0.257            28.587 ± 0.536            754.25                    1395.26                  \n\n\n==================== COMBINATION 117/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 67.479 ± 2.522 ms, Memory: 2517.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 93.672 ± 2.922 ms, Memory: 4822.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      67.479 ± 2.522            93.672 ± 2.922            2517.25                   4822.26                  \n\n\n==================== COMBINATION 118/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 261.741 ± 1.468 ms, Memory: 9121.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 780.12 MiB is free. Process 2955 has 13.98 GiB memory in use. Of the allocated memory 13.28 GiB is allocated by PyTorch, and 576.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 119/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.76 GiB is free. Process 2955 has 5.98 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 4.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 120/360 ====================\nEmbed dimension: 256, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 8.76 GiB is free. Process 2955 has 5.98 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 121/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 28.182 ± 0.387 ms, Memory: 1266.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 40.893 ± 1.038 ms, Memory: 2419.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      28.182 ± 0.387            40.893 ± 1.038            1266.25                   2419.26                  \n\n\n==================== COMBINATION 122/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 104.074 ± 0.552 ms, Memory: 4565.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 139.808 ± 1.109 ms, Memory: 8918.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      104.074 ± 0.552           139.808 ± 1.109           4565.25                   8918.26                  \n\n\n==================== COMBINATION 123/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 448.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 124/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 7.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 125/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 5.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 126/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 28.195 ± 0.414 ms, Memory: 1266.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 41.109 ± 1.076 ms, Memory: 2419.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      28.195 ± 0.414            41.109 ± 1.076            1266.25                   2419.26                  \n\n\n==================== COMBINATION 127/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 104.601 ± 0.644 ms, Memory: 4565.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 141.078 ± 1.636 ms, Memory: 8918.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      104.601 ± 0.644           141.078 ± 1.636           4565.25                   8918.26                  \n\n\n==================== COMBINATION 128/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 448.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 129/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 7.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 130/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 5.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 131/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 28.367 ± 0.425 ms, Memory: 1266.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 41.267 ± 1.131 ms, Memory: 2419.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      28.367 ± 0.425            41.267 ± 1.131            1266.25                   2419.26                  \n\n\n==================== COMBINATION 132/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 104.857 ± 0.537 ms, Memory: 4565.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 141.603 ± 1.600 ms, Memory: 8918.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      104.857 ± 0.537           141.603 ± 1.600           4565.25                   8918.26                  \n\n\n==================== COMBINATION 133/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 448.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 134/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 7.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 135/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 5.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 136/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 28.437 ± 0.434 ms, Memory: 1266.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 41.453 ± 1.120 ms, Memory: 2419.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      28.437 ± 0.434            41.453 ± 1.120            1266.25                   2419.26                  \n\n\n==================== COMBINATION 137/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 105.038 ± 0.644 ms, Memory: 4565.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 141.510 ± 1.696 ms, Memory: 8918.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      105.038 ± 0.644           141.510 ± 1.696           4565.25                   8918.26                  \n\n\n==================== COMBINATION 138/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 448.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 139/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 7.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 140/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 5.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 141/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 28.389 ± 0.426 ms, Memory: 1266.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 41.271 ± 1.106 ms, Memory: 2419.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      28.389 ± 0.426            41.271 ± 1.106            1266.25                   2419.26                  \n\n\n==================== COMBINATION 142/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 104.940 ± 0.494 ms, Memory: 4565.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 141.451 ± 1.711 ms, Memory: 8918.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      104.940 ± 0.494           141.451 ± 1.711           4565.25                   8918.26                  \n\n\n==================== COMBINATION 143/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 448.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 144/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 7.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 145/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 5.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 146/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 28.449 ± 0.436 ms, Memory: 1266.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 41.328 ± 1.112 ms, Memory: 2419.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      28.449 ± 0.436            41.328 ± 1.112            1266.25                   2419.26                  \n\n\n==================== COMBINATION 147/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 104.989 ± 0.565 ms, Memory: 4565.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 141.583 ± 1.666 ms, Memory: 8918.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      104.989 ± 0.565           141.583 ± 1.666           4565.25                   8918.26                  \n\n\n==================== COMBINATION 148/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 448.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 149/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 7.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 150/360 ====================\nEmbed dimension: 256, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.26 GiB is free. Process 2955 has 9.48 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 5.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 151/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 45.384 ± 0.658 ms, Memory: 2290.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 66.256 ± 1.410 ms, Memory: 4467.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      45.384 ± 0.658            66.256 ± 1.410            2290.25                   4467.26                  \n\n\n==================== COMBINATION 152/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 186.327 ± 0.650 ms, Memory: 8661.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.76 GiB is free. Process 2955 has 12.98 GiB memory in use. Of the allocated memory 12.65 GiB is allocated by PyTorch, and 204.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 153/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 929.25 MiB is allocated by PyTorch, and 3.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 154/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 3.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 155/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 156/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 45.353 ± 0.666 ms, Memory: 2290.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 66.647 ± 1.510 ms, Memory: 4467.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      45.353 ± 0.666            66.647 ± 1.510            2290.25                   4467.26                  \n\n\n==================== COMBINATION 157/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 186.836 ± 0.881 ms, Memory: 8661.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.76 GiB is free. Process 2955 has 12.98 GiB memory in use. Of the allocated memory 12.65 GiB is allocated by PyTorch, and 204.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 158/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 929.25 MiB is allocated by PyTorch, and 3.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 159/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 3.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 160/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 161/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 45.211 ± 0.620 ms, Memory: 2290.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 65.874 ± 1.375 ms, Memory: 4467.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      45.211 ± 0.620            65.874 ± 1.375            2290.25                   4467.26                  \n\n\n==================== COMBINATION 162/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 186.416 ± 0.776 ms, Memory: 8661.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.76 GiB is free. Process 2955 has 12.98 GiB memory in use. Of the allocated memory 12.65 GiB is allocated by PyTorch, and 204.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 163/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 929.25 MiB is allocated by PyTorch, and 3.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 164/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 3.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 165/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 166/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 45.385 ± 0.659 ms, Memory: 2290.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 66.295 ± 1.514 ms, Memory: 4467.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      45.385 ± 0.659            66.295 ± 1.514            2290.25                   4467.26                  \n\n\n==================== COMBINATION 167/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 186.753 ± 0.916 ms, Memory: 8661.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.76 GiB is free. Process 2955 has 12.98 GiB memory in use. Of the allocated memory 12.65 GiB is allocated by PyTorch, and 204.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 168/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 929.25 MiB is allocated by PyTorch, and 3.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 169/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 3.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 170/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 171/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 45.469 ± 0.658 ms, Memory: 2290.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 66.533 ± 1.473 ms, Memory: 4467.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      45.469 ± 0.658            66.533 ± 1.473            2290.25                   4467.26                  \n\n\n==================== COMBINATION 172/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 187.021 ± 0.874 ms, Memory: 8661.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.76 GiB is free. Process 2955 has 12.98 GiB memory in use. Of the allocated memory 12.65 GiB is allocated by PyTorch, and 204.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 173/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 929.25 MiB is allocated by PyTorch, and 3.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 174/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 3.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 175/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 176/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 45.525 ± 0.680 ms, Memory: 2290.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 66.718 ± 1.488 ms, Memory: 4467.26 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      45.525 ± 0.680            66.718 ± 1.488            2290.25                   4467.26                  \n\n\n==================== COMBINATION 177/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 186.812 ± 0.868 ms, Memory: 8661.25 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.76 GiB is free. Process 2955 has 12.98 GiB memory in use. Of the allocated memory 12.65 GiB is allocated by PyTorch, and 204.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 178/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 929.25 MiB is allocated by PyTorch, and 3.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 179/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 3.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 180/360 ====================\nEmbed dimension: 256, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.76 GiB is free. Process 2955 has 4.98 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 181/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 41.301 ± 1.047 ms, Memory: 981.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 62.363 ± 0.572 ms, Memory: 1753.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      41.301 ± 1.047            62.363 ± 0.572            981.26                    1753.27                  \n\n\n==================== COMBINATION 182/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 121.108 ± 0.578 ms, Memory: 2968.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 166.844 ± 0.860 ms, Memory: 5532.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      121.108 ± 0.578           166.844 ± 0.860           2968.26                   5532.27                  \n\n\n==================== COMBINATION 183/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 398.344 ± 1.411 ms, Memory: 10020.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.76 GiB is free. Process 2955 has 10.98 GiB memory in use. Of the allocated memory 7.04 GiB is allocated by PyTorch, and 3.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 184/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.76 GiB is free. Process 2955 has 6.98 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 3.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 185/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 186/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 40.088 ± 0.411 ms, Memory: 981.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 60.850 ± 1.441 ms, Memory: 1753.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      40.088 ± 0.411            60.850 ± 1.441            981.26                    1753.27                  \n\n\n==================== COMBINATION 187/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 120.744 ± 0.815 ms, Memory: 2968.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 166.701 ± 0.669 ms, Memory: 5532.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      120.744 ± 0.815           166.701 ± 0.669           2968.26                   5532.27                  \n\n\n==================== COMBINATION 188/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 399.666 ± 1.848 ms, Memory: 10020.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.26 GiB is free. Process 2955 has 11.48 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 830.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 189/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 3.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 190/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 191/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 40.086 ± 0.422 ms, Memory: 981.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 60.481 ± 1.504 ms, Memory: 1753.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      40.086 ± 0.422            60.481 ± 1.504            981.26                    1753.27                  \n\n\n==================== COMBINATION 192/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 119.523 ± 0.890 ms, Memory: 2968.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 167.433 ± 0.841 ms, Memory: 5532.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      119.523 ± 0.890           167.433 ± 0.841           2968.26                   5532.27                  \n\n\n==================== COMBINATION 193/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 399.377 ± 1.515 ms, Memory: 10020.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.26 GiB is free. Process 2955 has 11.48 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 830.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 194/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 3.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 195/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 196/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 40.059 ± 0.451 ms, Memory: 981.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 60.703 ± 1.398 ms, Memory: 1753.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      40.059 ± 0.451            60.703 ± 1.398            981.26                    1753.27                  \n\n\n==================== COMBINATION 197/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 119.855 ± 0.979 ms, Memory: 2968.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 167.565 ± 0.765 ms, Memory: 5532.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      119.855 ± 0.979           167.565 ± 0.765           2968.26                   5532.27                  \n\n\n==================== COMBINATION 198/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 399.620 ± 1.431 ms, Memory: 10020.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.26 GiB is free. Process 2955 has 11.48 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 830.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 199/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 3.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 200/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 201/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 40.200 ± 0.636 ms, Memory: 981.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 61.134 ± 1.361 ms, Memory: 1753.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      40.200 ± 0.636            61.134 ± 1.361            981.26                    1753.27                  \n\n\n==================== COMBINATION 202/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 120.391 ± 0.899 ms, Memory: 2968.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 167.343 ± 0.811 ms, Memory: 5532.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      120.391 ± 0.899           167.343 ± 0.811           2968.26                   5532.27                  \n\n\n==================== COMBINATION 203/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 398.884 ± 1.846 ms, Memory: 10020.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.26 GiB is free. Process 2955 has 11.48 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 830.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 204/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 3.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 205/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 206/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 40.647 ± 0.842 ms, Memory: 981.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 61.457 ± 1.275 ms, Memory: 1753.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      40.647 ± 0.842            61.457 ± 1.275            981.26                    1753.27                  \n\n\n==================== COMBINATION 207/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 121.189 ± 0.623 ms, Memory: 2968.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 166.784 ± 0.629 ms, Memory: 5532.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      121.189 ± 0.623           166.784 ± 0.629           2968.26                   5532.27                  \n\n\n==================== COMBINATION 208/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 398.345 ± 1.624 ms, Memory: 10020.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.26 GiB is free. Process 2955 has 11.48 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 830.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 209/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 3.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 210/360 ====================\nEmbed dimension: 512, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 211/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 48.340 ± 1.210 ms, Memory: 1493.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 68.780 ± 1.355 ms, Memory: 2777.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      48.340 ± 1.210            68.780 ± 1.355            1493.26                   2777.27                  \n\n\n==================== COMBINATION 212/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 152.867 ± 0.969 ms, Memory: 5016.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 209.661 ± 2.101 ms, Memory: 9628.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      152.867 ± 0.969           209.661 ± 2.101           5016.26                   9628.27                  \n\n\n==================== COMBINATION 213/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 9.79 GiB is allocated by PyTorch, and 573.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 214/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 6.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 215/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 216/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 49.202 ± 1.288 ms, Memory: 1493.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 69.846 ± 1.137 ms, Memory: 2777.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      49.202 ± 1.288            69.846 ± 1.137            1493.26                   2777.27                  \n\n\n==================== COMBINATION 217/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 153.744 ± 1.093 ms, Memory: 5016.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 209.726 ± 1.980 ms, Memory: 9628.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      153.744 ± 1.093           209.726 ± 1.980           5016.26                   9628.27                  \n\n\n==================== COMBINATION 218/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 9.79 GiB is allocated by PyTorch, and 573.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 219/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 6.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 220/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 221/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 49.378 ± 1.349 ms, Memory: 1493.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 69.903 ± 1.142 ms, Memory: 2777.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      49.378 ± 1.349            69.903 ± 1.142            1493.26                   2777.27                  \n\n\n==================== COMBINATION 222/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 153.349 ± 1.158 ms, Memory: 5016.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 209.758 ± 2.253 ms, Memory: 9628.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      153.349 ± 1.158           209.758 ± 2.253           5016.26                   9628.27                  \n\n\n==================== COMBINATION 223/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 9.79 GiB is allocated by PyTorch, and 573.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 224/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 6.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 225/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 226/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 49.293 ± 1.346 ms, Memory: 1493.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 69.836 ± 1.109 ms, Memory: 2777.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      49.293 ± 1.346            69.836 ± 1.109            1493.26                   2777.27                  \n\n\n==================== COMBINATION 227/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 153.526 ± 1.329 ms, Memory: 5016.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 209.811 ± 2.029 ms, Memory: 9628.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      153.526 ± 1.329           209.811 ± 2.029           5016.26                   9628.27                  \n\n\n==================== COMBINATION 228/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 9.79 GiB is allocated by PyTorch, and 573.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 229/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 6.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 230/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 231/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 49.103 ± 1.279 ms, Memory: 1493.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 69.913 ± 1.139 ms, Memory: 2777.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      49.103 ± 1.279            69.913 ± 1.139            1493.26                   2777.27                  \n\n\n==================== COMBINATION 232/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 154.529 ± 1.201 ms, Memory: 5016.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 210.070 ± 2.073 ms, Memory: 9628.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      154.529 ± 1.201           210.070 ± 2.073           5016.26                   9628.27                  \n\n\n==================== COMBINATION 233/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 9.79 GiB is allocated by PyTorch, and 573.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 234/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 6.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 235/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 236/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 48.857 ± 1.301 ms, Memory: 1493.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 69.614 ± 1.097 ms, Memory: 2777.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      48.857 ± 1.301            69.614 ± 1.097            1493.26                   2777.27                  \n\n\n==================== COMBINATION 237/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 154.322 ± 1.043 ms, Memory: 5016.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 210.481 ± 2.001 ms, Memory: 9628.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      154.322 ± 1.043           210.481 ± 2.001           5016.26                   9628.27                  \n\n\n==================== COMBINATION 238/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 9.79 GiB is allocated by PyTorch, and 573.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 239/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 6.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 240/360 ====================\nEmbed dimension: 512, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.26 GiB is free. Process 2955 has 10.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 241/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 64.325 ± 2.246 ms, Memory: 2517.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 97.776 ± 0.601 ms, Memory: 4825.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      64.325 ± 2.246            97.776 ± 0.601            2517.26                   4825.27                  \n\n\n==================== COMBINATION 242/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 229.338 ± 0.782 ms, Memory: 9112.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 266.12 MiB is free. Process 2955 has 14.48 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 243/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.26 GiB is free. Process 2955 has 2.48 GiB memory in use. Of the allocated memory 1.79 GiB is allocated by PyTorch, and 573.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 244/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.76 GiB is free. Process 2955 has 3.98 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 269.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 245/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 246/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 64.439 ± 2.137 ms, Memory: 2517.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 97.612 ± 0.469 ms, Memory: 4825.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      64.439 ± 2.137            97.612 ± 0.469            2517.26                   4825.27                  \n\n\n==================== COMBINATION 247/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 229.661 ± 0.789 ms, Memory: 9112.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.26 GiB is free. Process 2955 has 13.48 GiB memory in use. Of the allocated memory 9.27 GiB is allocated by PyTorch, and 4.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 248/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.26 GiB is free. Process 2955 has 5.48 GiB memory in use. Of the allocated memory 1.79 GiB is allocated by PyTorch, and 3.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 249/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.26 GiB is free. Process 2955 has 5.48 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 250/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 251/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 64.141 ± 2.356 ms, Memory: 2517.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 97.717 ± 0.570 ms, Memory: 4825.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      64.141 ± 2.356            97.717 ± 0.570            2517.26                   4825.27                  \n\n\n==================== COMBINATION 252/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 230.052 ± 0.731 ms, Memory: 9112.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 266.12 MiB is free. Process 2955 has 14.48 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 253/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.26 GiB is free. Process 2955 has 2.48 GiB memory in use. Of the allocated memory 1.79 GiB is allocated by PyTorch, and 573.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 254/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.76 GiB is free. Process 2955 has 3.98 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 269.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 255/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 256/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 64.891 ± 2.007 ms, Memory: 2517.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 97.655 ± 0.436 ms, Memory: 4825.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      64.891 ± 2.007            97.655 ± 0.436            2517.26                   4825.27                  \n\n\n==================== COMBINATION 257/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 229.107 ± 1.086 ms, Memory: 9112.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.26 GiB is free. Process 2955 has 13.48 GiB memory in use. Of the allocated memory 9.27 GiB is allocated by PyTorch, and 4.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 258/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.26 GiB is free. Process 2955 has 5.48 GiB memory in use. Of the allocated memory 1.79 GiB is allocated by PyTorch, and 3.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 259/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.26 GiB is free. Process 2955 has 5.48 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 260/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 261/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 65.120 ± 1.967 ms, Memory: 2517.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 97.438 ± 1.479 ms, Memory: 4825.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      65.120 ± 1.967            97.438 ± 1.479            2517.26                   4825.27                  \n\n\n==================== COMBINATION 262/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 229.393 ± 0.880 ms, Memory: 9112.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 266.12 MiB is free. Process 2955 has 14.48 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 263/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 12.26 GiB is free. Process 2955 has 2.48 GiB memory in use. Of the allocated memory 1.79 GiB is allocated by PyTorch, and 573.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 264/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.76 GiB is free. Process 2955 has 3.98 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 269.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 265/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 266/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 64.903 ± 2.037 ms, Memory: 2517.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 96.844 ± 1.719 ms, Memory: 4825.27 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      64.903 ± 2.037            96.844 ± 1.719            2517.26                   4825.27                  \n\n\n==================== COMBINATION 267/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 229.847 ± 1.104 ms, Memory: 9112.26 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.26 GiB is free. Process 2955 has 13.48 GiB memory in use. Of the allocated memory 9.27 GiB is allocated by PyTorch, and 4.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 268/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.26 GiB is free. Process 2955 has 5.48 GiB memory in use. Of the allocated memory 1.79 GiB is allocated by PyTorch, and 3.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 269/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.26 GiB is free. Process 2955 has 5.48 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 270/360 ====================\nEmbed dimension: 512, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.26 GiB is free. Process 2955 has 7.48 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 271/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 112.015 ± 1.322 ms, Memory: 1442.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 163.916 ± 1.898 ms, Memory: 2481.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      112.015 ± 1.322           163.916 ± 1.898           1442.27                   2481.28                  \n\n\n==================== COMBINATION 272/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 274.254 ± 1.235 ms, Memory: 3876.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 403.740 ± 1.923 ms, Memory: 6964.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      274.254 ± 1.235           403.740 ± 1.923           3876.27                   6964.28                  \n\n\n==================== COMBINATION 273/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 7.55 GiB is allocated by PyTorch, and 3.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 274/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 275/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 276/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 110.881 ± 1.551 ms, Memory: 1442.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 164.533 ± 2.084 ms, Memory: 2481.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      110.881 ± 1.551           164.533 ± 2.084           1442.27                   2481.28                  \n\n\n==================== COMBINATION 277/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 275.123 ± 0.870 ms, Memory: 3876.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 404.830 ± 1.959 ms, Memory: 6964.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      275.123 ± 0.870           404.830 ± 1.959           3876.27                   6964.28                  \n\n\n==================== COMBINATION 278/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 785.614 ± 1.574 ms, Memory: 11824.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 10.05 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 279/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.51 GiB is free. Process 2955 has 9.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 280/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 281/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 110.711 ± 1.437 ms, Memory: 1442.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 163.923 ± 1.562 ms, Memory: 2481.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      110.711 ± 1.437           163.923 ± 1.562           1442.27                   2481.28                  \n\n\n==================== COMBINATION 282/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 274.582 ± 1.007 ms, Memory: 3876.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 404.206 ± 1.756 ms, Memory: 6964.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      274.582 ± 1.007           404.206 ± 1.756           3876.27                   6964.28                  \n\n\n==================== COMBINATION 283/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 785.994 ± 1.555 ms, Memory: 11824.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 10.05 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 284/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.51 GiB is free. Process 2955 has 9.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 285/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 286/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 110.588 ± 1.485 ms, Memory: 1442.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 163.424 ± 1.619 ms, Memory: 2481.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      110.588 ± 1.485           163.424 ± 1.619           1442.27                   2481.28                  \n\n\n==================== COMBINATION 287/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 274.509 ± 1.605 ms, Memory: 3876.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 404.745 ± 1.946 ms, Memory: 6964.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      274.509 ± 1.605           404.745 ± 1.946           3876.27                   6964.28                  \n\n\n==================== COMBINATION 288/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 783.759 ± 2.424 ms, Memory: 11824.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 10.05 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 289/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.51 GiB is free. Process 2955 has 9.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 290/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 291/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 110.723 ± 1.524 ms, Memory: 1442.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 163.999 ± 1.652 ms, Memory: 2481.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      110.723 ± 1.524           163.999 ± 1.652           1442.27                   2481.28                  \n\n\n==================== COMBINATION 292/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 274.790 ± 0.959 ms, Memory: 3876.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 404.448 ± 1.949 ms, Memory: 6964.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      274.790 ± 0.959           404.448 ± 1.949           3876.27                   6964.28                  \n\n\n==================== COMBINATION 293/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 784.683 ± 1.939 ms, Memory: 11824.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 10.05 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 294/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.51 GiB is free. Process 2955 has 9.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 295/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 296/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 110.642 ± 1.340 ms, Memory: 1442.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 163.893 ± 1.726 ms, Memory: 2481.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      110.642 ± 1.340           163.893 ± 1.726           1442.27                   2481.28                  \n\n\n==================== COMBINATION 297/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 274.292 ± 0.984 ms, Memory: 3876.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 404.317 ± 1.823 ms, Memory: 6964.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      274.292 ± 0.984           404.317 ± 1.823           3876.27                   6964.28                  \n\n\n==================== COMBINATION 298/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 784.819 ± 1.743 ms, Memory: 11824.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 10.05 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 299/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.51 GiB is free. Process 2955 has 9.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 300/360 ====================\nEmbed dimension: 1024, Num heads: 4, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 301/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 118.424 ± 1.979 ms, Memory: 1953.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 171.984 ± 1.422 ms, Memory: 3505.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      118.424 ± 1.979           171.984 ± 1.422           1953.27                   3505.28                  \n\n\n==================== COMBINATION 302/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 305.958 ± 1.105 ms, Memory: 5924.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 441.713 ± 2.217 ms, Memory: 11060.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      305.958 ± 1.105           441.713 ± 2.217           5924.27                   11060.28                 \n\n\n==================== COMBINATION 303/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 304/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 6.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 305/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 306/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 119.954 ± 1.632 ms, Memory: 1953.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 172.619 ± 1.821 ms, Memory: 3505.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      119.954 ± 1.632           172.619 ± 1.821           1953.27                   3505.28                  \n\n\n==================== COMBINATION 307/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 305.801 ± 1.024 ms, Memory: 5924.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 441.557 ± 2.359 ms, Memory: 11060.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      305.801 ± 1.024           441.557 ± 2.359           5924.27                   11060.28                 \n\n\n==================== COMBINATION 308/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 309/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 6.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 310/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 311/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 119.205 ± 1.820 ms, Memory: 1953.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 172.147 ± 1.432 ms, Memory: 3505.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      119.205 ± 1.820           172.147 ± 1.432           1953.27                   3505.28                  \n\n\n==================== COMBINATION 312/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 305.703 ± 0.990 ms, Memory: 5924.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 441.199 ± 2.479 ms, Memory: 11060.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      305.703 ± 0.990           441.199 ± 2.479           5924.27                   11060.28                 \n\n\n==================== COMBINATION 313/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 314/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 6.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 315/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 316/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 118.473 ± 1.863 ms, Memory: 1953.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 171.803 ± 1.236 ms, Memory: 3505.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      118.473 ± 1.863           171.803 ± 1.236           1953.27                   3505.28                  \n\n\n==================== COMBINATION 317/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 305.927 ± 1.061 ms, Memory: 5924.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 441.012 ± 2.109 ms, Memory: 11060.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      305.927 ± 1.061           441.012 ± 2.109           5924.27                   11060.28                 \n\n\n==================== COMBINATION 318/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 319/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 6.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 320/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 321/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 119.112 ± 1.863 ms, Memory: 1953.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 172.881 ± 1.680 ms, Memory: 3505.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      119.112 ± 1.863           172.881 ± 1.680           1953.27                   3505.28                  \n\n\n==================== COMBINATION 322/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 307.587 ± 1.077 ms, Memory: 5924.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 441.568 ± 2.044 ms, Memory: 11060.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      307.587 ± 1.077           441.568 ± 2.044           5924.27                   11060.28                 \n\n\n==================== COMBINATION 323/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 324/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 6.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 325/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 326/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 119.199 ± 1.791 ms, Memory: 1953.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 172.960 ± 1.782 ms, Memory: 3505.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      119.199 ± 1.791           172.960 ± 1.782           1953.27                   3505.28                  \n\n\n==================== COMBINATION 327/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 307.510 ± 1.125 ms, Memory: 5924.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 440.845 ± 2.334 ms, Memory: 11060.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      307.510 ± 1.125           440.845 ± 2.334           5924.27                   11060.28                 \n\n\n==================== COMBINATION 328/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 329/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 6.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 330/360 ====================\nEmbed dimension: 1024, Num heads: 8, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 331/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 131.290 ± 1.703 ms, Memory: 2977.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 191.670 ± 1.500 ms, Memory: 5553.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      131.290 ± 1.703           191.670 ± 1.500           2977.27                   5553.28                  \n\n\n==================== COMBINATION 332/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 375.999 ± 1.245 ms, Memory: 10020.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 569.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 333/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 3.55 GiB is allocated by PyTorch, and 7.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 334/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 335/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 64\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 336/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 131.457 ± 1.639 ms, Memory: 2977.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 191.680 ± 2.048 ms, Memory: 5553.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      131.457 ± 1.639           191.680 ± 2.048           2977.27                   5553.28                  \n\n\n==================== COMBINATION 337/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 374.789 ± 1.495 ms, Memory: 10020.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 569.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 338/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 3.55 GiB is allocated by PyTorch, and 7.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 339/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 340/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 64\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 341/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 131.483 ± 1.414 ms, Memory: 2977.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 191.319 ± 1.044 ms, Memory: 5553.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      131.483 ± 1.414           191.319 ± 1.044           2977.27                   5553.28                  \n\n\n==================== COMBINATION 342/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 374.087 ± 1.417 ms, Memory: 10020.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 569.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 343/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 3.55 GiB is allocated by PyTorch, and 7.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 344/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 345/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 128\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 346/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 131.252 ± 1.987 ms, Memory: 2977.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 191.393 ± 1.344 ms, Memory: 5553.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      131.252 ± 1.987           191.393 ± 1.344           2977.27                   5553.28                  \n\n\n==================== COMBINATION 347/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 374.065 ± 1.485 ms, Memory: 10020.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 569.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 348/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 3.55 GiB is allocated by PyTorch, and 7.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 349/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 350/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 128\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 351/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 131.391 ± 2.122 ms, Memory: 2977.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 191.595 ± 1.627 ms, Memory: 5553.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      131.391 ± 2.122           191.595 ± 1.627           2977.27                   5553.28                  \n\n\n==================== COMBINATION 352/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 373.735 ± 1.484 ms, Memory: 10020.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 569.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 353/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 3.55 GiB is allocated by PyTorch, and 7.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 354/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 355/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 256\nKernel function: softmax, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 356/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 512\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 131.242 ± 1.850 ms, Memory: 2977.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nattention backward: 191.819 ± 2.017 ms, Memory: 5553.28 ± 0.00 MB\n\n============================================================\n==================== SUMMARY FOR CURRENT COMBINATION ====================\n============================================================\nModel                          Forward (ms)              Backward (ms)             Forward Memory (MB)       Backward Memory (MB)     \n------------------------------------------------------------\nattention                      131.242 ± 1.850           191.819 ± 2.017           2977.27                   5553.28                  \n\n\n==================== COMBINATION 357/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 1024\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nattention forward: 374.354 ± 1.498 ms, Memory: 10020.27 ± 0.00 MB\n\n=== attention BACKWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 10.54 GiB is allocated by PyTorch, and 569.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 358/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 2048\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 3.55 GiB is allocated by PyTorch, and 7.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 359/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 4096\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.51 GiB is free. Process 2955 has 11.23 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n==================== COMBINATION 360/360 ====================\nEmbed dimension: 1024, Num heads: 16, Num random features: 256\nKernel function: relu, Causal: True, Sequence length: 8192\n\n==================== Benchmarking attention ====================\n\n=== attention FORWARD PASS BENCHMARK ===\nError benchmarking attention: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.51 GiB is free. Process 2955 has 13.23 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 833.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\nAll benchmark results saved to attention_benchmark_results.csv\n","output_type":"stream"}],"execution_count":20}]}