{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:05:51.378076Z",
     "iopub.status.busy": "2025-03-18T13:05:51.377803Z",
     "iopub.status.idle": "2025-03-18T13:05:51.382057Z",
     "shell.execute_reply": "2025-03-18T13:05:51.380950Z",
     "shell.execute_reply.started": "2025-03-18T13:05:51.378037Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# secret_value_0 = user_secrets.get_secret(\"github_repos_wildcard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:05:51.383814Z",
     "iopub.status.busy": "2025-03-18T13:05:51.383519Z",
     "iopub.status.idle": "2025-03-18T13:05:51.401749Z",
     "shell.execute_reply": "2025-03-18T13:05:51.401076Z",
     "shell.execute_reply.started": "2025-03-18T13:05:51.383769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.rmtree(\"panther\")\n",
    "\n",
    "# GITHUB_TOKEN = secret_value_0\n",
    "# USER = \"gaserSami\"\n",
    "# CLONE_URL = f\"https://{USER}:{GITHUB_TOKEN}@github.com/{USER}/panther.git\"\n",
    "# get_ipython().system(f\"git clone --branch testing_triton {CLONE_URL}\")\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"panther\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:05:51.403337Z",
     "iopub.status.busy": "2025-03-18T13:05:51.403150Z",
     "iopub.status.idle": "2025-03-18T13:05:51.416593Z",
     "shell.execute_reply": "2025-03-18T13:05:51.416020Z",
     "shell.execute_reply.started": "2025-03-18T13:05:51.403321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !python -m pip uninstall -y torch triton pytorch-triton triton\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:05:51.418206Z",
     "iopub.status.busy": "2025-03-18T13:05:51.417925Z",
     "iopub.status.idle": "2025-03-18T13:08:20.845346Z",
     "shell.execute_reply": "2025-03-18T13:08:20.844560Z",
     "shell.execute_reply.started": "2025-03-18T13:05:51.418184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp310-cp310-linux_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.2.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n",
      "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl (848.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp310-cp310-linux_x86_64.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchaudio, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cu121\n",
      "    Uninstalling torch-2.5.1+cu121:\n",
      "      Successfully uninstalled torch-2.5.1+cu121\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.5.1+cu121\n",
      "    Uninstalling torchaudio-2.5.1+cu121:\n",
      "      Successfully uninstalled torchaudio-2.5.1+cu121\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.20.1+cu121\n",
      "    Uninstalling torchvision-0.20.1+cu121:\n",
      "      Successfully uninstalled torchvision-0.20.1+cu121\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118 torchaudio-2.6.0+cu118 torchvision-0.21.0+cu118 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \\\n",
    "  torch torchvision torchaudio \\\n",
    "  --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:20.846461Z",
     "iopub.status.busy": "2025-03-18T13:08:20.846238Z",
     "iopub.status.idle": "2025-03-18T13:08:22.955826Z",
     "shell.execute_reply": "2025-03-18T13:08:22.954939Z",
     "shell.execute_reply.started": "2025-03-18T13:08:20.846430Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "\n",
      "2.6.0+cu118\n",
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_COMPILE_DEBUG\"] = \"1\"\n",
    "print(os.environ[\"TORCH_COMPILE_DEBUG\"], \"\\n\")\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import triton\n",
    "print(triton.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:22.957153Z",
     "iopub.status.busy": "2025-03-18T13:08:22.956736Z",
     "iopub.status.idle": "2025-03-18T13:08:22.966592Z",
     "shell.execute_reply": "2025-03-18T13:08:22.965861Z",
     "shell.execute_reply.started": "2025-03-18T13:08:22.957120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def uniform_dense_sketch(m, n, device=None, dtype=None):\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    return torch.empty(m, n, **factory_kwargs).uniform_(-1, 1)\n",
    "\n",
    "\n",
    "def gaussian_dense_sketch(m, n, device=None, dtype=None):\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    return torch.randn(m, n, **factory_kwargs)\n",
    "\n",
    "\n",
    "def hadamard_sketch(m, device=None, dtype=None):\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    if m & (m - 1) != 0:\n",
    "        raise ValueError(\"m must be a power of 2\")\n",
    "\n",
    "    H = torch.tensor([[1.0]])\n",
    "    while H.shape[0] < m:\n",
    "        H = torch.cat((torch.cat((H, H), dim=1), torch.cat((H, -H), dim=1)), dim=0)\n",
    "\n",
    "    return H / torch.sqrt(torch.tensor(m, **factory_kwargs))\n",
    "\n",
    "\n",
    "def gaussian_orthonormal_sketch(m, n, device=None, dtype=None):\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    return torch.qr(torch.randn(m, n, **factory_kwargs))[0]\n",
    "\n",
    "\n",
    "def scaled_sign_sketch(m, n, device=None, dtype=None):\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    return (torch.randint(0, 2, (m, n), **factory_kwargs) * 2 - 1) / torch.sqrt(\n",
    "        torch.tensor(m, **factory_kwargs)\n",
    "    )\n",
    "\n",
    "\n",
    "def clarkson_woodruff_sketch(m, n, device=None, dtype=None):\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    indices = torch.randint(0, m, (n,), **factory_kwargs)\n",
    "    signs = torch.randint(0, 2, (n,), **factory_kwargs) * 2 - 1\n",
    "    sketch = torch.zeros(m, n, **factory_kwargs)\n",
    "    sketch[indices, torch.arange(n)] = signs\n",
    "    return sketch\n",
    "\n",
    "\n",
    "def sparse_sign_embeddings_sketch(m, n, sparsity=0.1):\n",
    "    mask = torch.rand(m, n) < sparsity\n",
    "    signs = torch.randint(0, 2, (m, n)) * 2 - 1\n",
    "    return mask.float() * signs.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:22.967725Z",
     "iopub.status.busy": "2025-03-18T13:08:22.967429Z",
     "iopub.status.idle": "2025-03-18T13:08:26.687133Z",
     "shell.execute_reply": "2025-03-18T13:08:26.686202Z",
     "shell.execute_reply.started": "2025-03-18T13:08:22.967695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from typing import Any, Dict, List, Optional, Union, Tuple\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch._inductor.config as config\n",
    "# config.max_autotune_gemm = False\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch._dynamo\n",
    "    \n",
    "# Note that forward, setup_context, and backward are @staticmethods\n",
    "def sklinear_forward(\n",
    "    input: torch.Tensor,\n",
    "    S1s: torch.Tensor,\n",
    "    S2s: torch.Tensor,\n",
    "    U1s: torch.Tensor,\n",
    "    U2s: torch.Tensor,\n",
    "    bias: torch.Tensor,\n",
    "):\n",
    "    num_terms = S2s.shape[0]\n",
    "    # Efficiently perform the sum over all l terms\n",
    "    input = input.unsqueeze(0).expand(num_terms, input.shape[0], input.shape[1])\n",
    "    return (\n",
    "        ((input.bmm(S1s)).bmm(U1s)).mean(0) / 2\n",
    "        + ((input.bmm(U2s)).bmm(S2s)).mean(0) / 2\n",
    "        + bias\n",
    "    )\n",
    "\n",
    "def sklinear_backward(ctx: Any, *grad_output: Any) -> Any:\n",
    "    # dl/dS2_i = U1_i g h_in^T / 2 * l\n",
    "    # dl/dS1_i = g h_in^T U2_i^T / 2 * l\n",
    "    # dl/dh_in = 1/(2*l) * (sum_{i=1}^{l} (S1_i^T U1_i g) + sum_{i=1}^{l} (U2_i^T S2_i g))\n",
    "    # dl/db = g\n",
    "    input, S1s, S2s, U1s, U2s, _ = ctx.saved_tensors\n",
    "    num_terms = S2s.shape[0]\n",
    "    g = grad_output[0] / (2 * num_terms)\n",
    "    g = g.unsqueeze(0).expand(num_terms, g.shape[0], g.shape[1])\n",
    "    input = (\n",
    "        input.unsqueeze(0)\n",
    "        .expand(num_terms, input.shape[0], input.shape[1])\n",
    "        .transpose(1, 2)\n",
    "    )\n",
    "    U1s = U1s.transpose(1, 2)\n",
    "    S1s = S1s.transpose(1, 2)\n",
    "    U2s = U2s.transpose(1, 2)\n",
    "    S2s = S2s.transpose(1, 2)\n",
    "    t1 = g.bmm(U1s)\n",
    "    grad = t1.bmm(S1s).sum(0) + g.bmm(S2s).bmm(U2s).sum(0)\n",
    "    grad_S2s = (U2s.bmm(input)).bmm(g)\n",
    "    grad_S1s = input.bmm(g.bmm(U1s))\n",
    "\n",
    "    g = g[0]\n",
    "    return (\n",
    "        grad,\n",
    "        grad_S1s,\n",
    "        grad_S2s,\n",
    "        None,\n",
    "        None,\n",
    "        # sum g on batch dimension input.shape[0]\n",
    "        g.reshape(input.shape[2], -1).sum(0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:26.688710Z",
     "iopub.status.busy": "2025-03-18T13:08:26.688202Z",
     "iopub.status.idle": "2025-03-18T13:08:26.693310Z",
     "shell.execute_reply": "2025-03-18T13:08:26.692523Z",
     "shell.execute_reply.started": "2025-03-18T13:08:26.688676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "1 2\n"
     ]
    }
   ],
   "source": [
    "x = (1,2)\n",
    "print(x)\n",
    "print(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:26.697523Z",
     "iopub.status.busy": "2025-03-18T13:08:26.697208Z",
     "iopub.status.idle": "2025-03-18T13:08:26.730460Z",
     "shell.execute_reply": "2025-03-18T13:08:26.729767Z",
     "shell.execute_reply.started": "2025-03-18T13:08:26.697491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sklinear_forward_compiled = torch.compile(\n",
    "                    sklinear_forward,\n",
    "                    fullgraph=True,\n",
    "                    dynamic=True,\n",
    "                )\n",
    "\n",
    "sklinear_backward_compiled = torch.compile(\n",
    "                    sklinear_backward,\n",
    "                    fullgraph=True,\n",
    "                    dynamic=True,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:26.731813Z",
     "iopub.status.busy": "2025-03-18T13:08:26.731553Z",
     "iopub.status.idle": "2025-03-18T13:08:26.741307Z",
     "shell.execute_reply": "2025-03-18T13:08:26.740593Z",
     "shell.execute_reply.started": "2025-03-18T13:08:26.731794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "in_features: int = 4096\n",
    "out_features: int = 2048\n",
    "num_terms: int = 4\n",
    "low_rank: int = 64\n",
    "batch_size = 32\n",
    "W_init=None\n",
    "bias: bool = True,\n",
    "dtype=torch.float32\n",
    "device='cuda'\n",
    "\n",
    "num_warmups = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:26.742138Z",
     "iopub.status.busy": "2025-03-18T13:08:26.741953Z",
     "iopub.status.idle": "2025-03-18T13:08:26.758655Z",
     "shell.execute_reply": "2025-03-18T13:08:26.757863Z",
     "shell.execute_reply.started": "2025-03-18T13:08:26.742122Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "hi = SimpleNamespace()\n",
    "hi.s = '1'\n",
    "print(hi.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:26.759744Z",
     "iopub.status.busy": "2025-03-18T13:08:26.759520Z",
     "iopub.status.idle": "2025-03-18T13:08:27.122559Z",
     "shell.execute_reply": "2025-03-18T13:08:27.121875Z",
     "shell.execute_reply.started": "2025-03-18T13:08:26.759726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "factory_kwargs = {\"dtype\": dtype, \"device\": device}\n",
    "self = SimpleNamespace()\n",
    "\n",
    "# Register U1s and U2s as buffers since they are not learnable\n",
    "self.U1s = torch.stack(\n",
    "        [\n",
    "            scaled_sign_sketch(low_rank, out_features, **factory_kwargs)\n",
    "            for _ in range(num_terms)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "self.U2s = torch.stack(\n",
    "        [\n",
    "            scaled_sign_sketch(in_features, low_rank, **factory_kwargs)\n",
    "            for _ in range(num_terms)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# W is used to only initialize S\n",
    "if W_init is None:\n",
    "    W = torch.empty(in_features, out_features, **factory_kwargs)\n",
    "    init.kaiming_uniform_(W, a=math.sqrt(5))\n",
    "else:\n",
    "    W = W_init.T.detach().clone()\n",
    "\n",
    "# S1s and S2s are precomputed but not updated in the backward pass\n",
    "self.S1s = torch.stack([torch.matmul(W, self.U1s[i].T) for i in range(num_terms)])\n",
    "self.S2s = torch.stack([torch.matmul(self.U2s[i].T, W) for i in range(num_terms)])\n",
    "\n",
    "# Bias term initialized with a small standard deviation\n",
    "if bias:\n",
    "    self.bias = torch.empty(out_features, **factory_kwargs)\n",
    "    fan_in, _ = init._calculate_fan_in_and_fan_out(W)\n",
    "    bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "    init.uniform_(self.bias, -bound, bound)\n",
    "else:\n",
    "    self.bias = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:27.123657Z",
     "iopub.status.busy": "2025-03-18T13:08:27.123314Z",
     "iopub.status.idle": "2025-03-18T13:08:27.127865Z",
     "shell.execute_reply": "2025-03-18T13:08:27.127126Z",
     "shell.execute_reply.started": "2025-03-18T13:08:27.123628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x = 1\n",
    "h_in = torch.randn(batch_size, in_features, device=device)\n",
    "config.max_autotune_gemm = True\n",
    "\n",
    "ctx = SimpleNamespace()\n",
    "ctx.saved_tensors =  (h_in, self.S1s, self.S2s, self.U1s, self.U2s, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:27.129046Z",
     "iopub.status.busy": "2025-03-18T13:08:27.128764Z",
     "iopub.status.idle": "2025-03-18T13:08:27.142872Z",
     "shell.execute_reply": "2025-03-18T13:08:27.142146Z",
     "shell.execute_reply.started": "2025-03-18T13:08:27.129013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def warmup_sklinear_forward_compiled(h_in):\n",
    "    for _ in range(num_warmups):\n",
    "        sklinear_forward_compiled(\n",
    "            h_in, self.S1s, self.S2s, self.U1s, self.U2s, self.bias\n",
    "        )\n",
    "\n",
    "def warmup_sklinear_backward_compiled(ctx, out_grad):\n",
    "    for _ in range(num_warmups):\n",
    "        sklinear_backward_compiled(\n",
    "            ctx, out_grad\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:27.143771Z",
     "iopub.status.busy": "2025-03-18T13:08:27.143584Z",
     "iopub.status.idle": "2025-03-18T13:08:27.165937Z",
     "shell.execute_reply": "2025-03-18T13:08:27.165171Z",
     "shell.execute_reply.started": "2025-03-18T13:08:27.143754Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1aaf09f0f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:27.167170Z",
     "iopub.status.busy": "2025-03-18T13:08:27.166867Z",
     "iopub.status.idle": "2025-03-18T13:08:27.270787Z",
     "shell.execute_reply": "2025-03-18T13:08:27.270026Z",
     "shell.execute_reply.started": "2025-03-18T13:08:27.167142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1940,  2.1614, -0.1721,  ...,  0.1124,  0.2769, -1.2535],\n",
      "        [ 1.1346, -0.6329, -0.9147,  ..., -1.0449,  1.5397, -2.0042],\n",
      "        [-0.5852, -0.9346,  0.9684,  ...,  0.0506, -0.5426,  0.0125],\n",
      "        ...,\n",
      "        [-0.2164, -0.1439,  1.1548,  ..., -0.7282,  0.5087,  0.1318],\n",
      "        [-1.2356, -0.4083, -0.4695,  ...,  0.2585, -0.1683, -0.7625],\n",
      "        [ 0.1757, -1.5658, -1.0879,  ..., -0.4814,  0.1595,  1.4141]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 1.3914e-01, -1.0822e-01, -7.1742e-01,  ..., -2.0609e+00,\n",
      "          1.5548e+00, -9.8551e-01],\n",
      "        [ 1.4723e-03,  9.2328e-01,  1.7156e+00,  ...,  1.8512e+00,\n",
      "          1.4714e+00,  1.1165e+00],\n",
      "        [ 9.1507e-01, -9.8272e-01,  2.9564e-01,  ..., -5.8359e-01,\n",
      "         -9.6585e-01, -5.5795e-01],\n",
      "        ...,\n",
      "        [-1.2285e+00, -6.1759e-01,  1.0773e+00,  ...,  7.1975e-01,\n",
      "         -5.8945e-02,  1.9731e+00],\n",
      "        [-2.1609e-01,  1.7308e+00,  1.8940e-01,  ...,  9.4094e-01,\n",
      "         -3.8447e-01, -2.6132e-01],\n",
      "        [ 5.5475e-01,  4.2692e-01, -1.8394e+00,  ..., -1.4447e+00,\n",
      "          2.3305e-01,  8.8388e-01]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "h_in = torch.randn(batch_size, in_features, device=device)\n",
    "print(h_in)\n",
    "grad_out = torch.randn(batch_size, out_features, device=device)\n",
    "print(grad_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:27.271993Z",
     "iopub.status.busy": "2025-03-18T13:08:27.271686Z",
     "iopub.status.idle": "2025-03-18T13:08:27.277105Z",
     "shell.execute_reply": "2025-03-18T13:08:27.276366Z",
     "shell.execute_reply.started": "2025-03-18T13:08:27.271952Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def deep_copy_ctx(ctx):\n",
    "    new_ctx = SimpleNamespace()\n",
    "    \n",
    "    # Clone each saved tensor\n",
    "    h_in, S1s, S2s, U1s, U2s, x = ctx.saved_tensors\n",
    "    \n",
    "    new_ctx.saved_tensors = (\n",
    "        h_in.clone().detach(),\n",
    "        S1s.clone().detach(),\n",
    "        S2s.clone().detach(),\n",
    "        U1s.clone().detach(),\n",
    "        U2s.clone().detach(),\n",
    "        x\n",
    "    )\n",
    "    \n",
    "    # Copy any other attributes from ctx\n",
    "    for key, value in vars(ctx).items():\n",
    "        if key != 'saved_tensors':\n",
    "            if hasattr(value, 'clone') and callable(value.clone):\n",
    "                # If it's a tensor, clone it\n",
    "                setattr(new_ctx, key, value.clone().detach())\n",
    "            else:\n",
    "                # For non-tensor attributes, use a simple copy\n",
    "                setattr(new_ctx, key, copy.deepcopy(value))\n",
    "    \n",
    "    return new_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:27.278180Z",
     "iopub.status.busy": "2025-03-18T13:08:27.277931Z",
     "iopub.status.idle": "2025-03-18T13:08:35.580031Z",
     "shell.execute_reply": "2025-03-18T13:08:35.579289Z",
     "shell.execute_reply.started": "2025-03-18T13:08:27.278159Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time before warmups\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0318 13:08:30.286000 31 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "W0318 13:08:32.498000 31 torch/_inductor/debug.py:435] [0/0] model__0_inference_0 debug trace: /kaggle/working/torch_compile_debug/run_2025_03_18_13_08_27_295883-pid_31/torchinductor/model__0_inference_0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward :  5.4370562789999894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
      "  warnings.warn(\n",
      "W0318 13:08:34.981000 31 torch/_inductor/debug.py:435] [1/0] model__1_inference_1 debug trace: /kaggle/working/torch_compile_debug/run_2025_03_18_13_08_27_295883-pid_31/torchinductor/model__1_inference_1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward :  2.830728950999969\n",
      "time after warmups\n",
      "\n",
      "forward :  0.0008980919999430625\n",
      "backward :  0.0013123839999025222\n"
     ]
    }
   ],
   "source": [
    "old_ctx = deep_copy_ctx(ctx)\n",
    "old_ctx2 = deep_copy_ctx(ctx)\n",
    "old_grad_out = grad_out.clone().detach()\n",
    "\n",
    "print(\"time before warmups\\n\")\n",
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "out_forward_compiled = sklinear_forward_compiled(h_in, self.S1s, self.S2s, self.U1s, self.U2s, self.bias)\n",
    "torch.cuda.synchronize()\n",
    "print(\"forward : \", time.perf_counter() - start)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "out_backward_compiled = sklinear_backward_compiled(old_ctx, old_grad_out)\n",
    "torch.cuda.synchronize()\n",
    "print(\"backward : \", time.perf_counter() - start)\n",
    "\n",
    "warmup_sklinear_forward_compiled(h_in)\n",
    "warmup_sklinear_backward_compiled(ctx, grad_out)\n",
    "\n",
    "print(\"time after warmups\\n\")\n",
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "out_backward_compiled = sklinear_forward_compiled(h_in, self.S1s, self.S2s, self.U1s, self.U2s, self.bias)\n",
    "torch.cuda.synchronize()\n",
    "print(\"forward : \", time.perf_counter() - start)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "out_backward_compiled = sklinear_backward_compiled(old_ctx2, old_grad_out)\n",
    "torch.cuda.synchronize()\n",
    "print(\"backward : \", time.perf_counter() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:35.581206Z",
     "iopub.status.busy": "2025-03-18T13:08:35.580808Z",
     "iopub.status.idle": "2025-03-18T13:08:35.729931Z",
     "shell.execute_reply": "2025-03-18T13:08:35.728876Z",
     "shell.execute_reply.started": "2025-03-18T13:08:35.581181Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /kaggle/working/torch_compile_debug/run_2025_03_18_11_13_13_422647-pid_31/torchinductor/model__1_inference_1.1: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat /kaggle/working/torch_compile_debug/run_2025_03_18_11_13_13_422647-pid_31/torchinductor/model__1_inference_1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING TRITON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:08:35.731435Z",
     "iopub.status.busy": "2025-03-18T13:08:35.731148Z",
     "iopub.status.idle": "2025-03-18T13:08:35.862365Z",
     "shell.execute_reply": "2025-03-18T13:08:35.861203Z",
     "shell.execute_reply.started": "2025-03-18T13:08:35.731412Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed!\n",
      "Max absolute difference: 2.384185791015625e-07\n",
      "Mean absolute difference: 1.742000677040778e-08\n",
      "Positions where they differ: tensor([[   0],\n",
      "        [   1],\n",
      "        [   2],\n",
      "        [   3],\n",
      "        [   5],\n",
      "        [   6],\n",
      "        [   7],\n",
      "        [  10],\n",
      "        [  11],\n",
      "        [  18],\n",
      "        [  23],\n",
      "        [  28],\n",
      "        [  31],\n",
      "        [  42],\n",
      "        [  43],\n",
      "        [  45],\n",
      "        [  50],\n",
      "        [  51],\n",
      "        [  53],\n",
      "        [  54],\n",
      "        [  55],\n",
      "        [  61],\n",
      "        [  62],\n",
      "        [  69],\n",
      "        [  73],\n",
      "        [  75],\n",
      "        [  79],\n",
      "        [  85],\n",
      "        [  86],\n",
      "        [  87],\n",
      "        [  88],\n",
      "        [  93],\n",
      "        [  95],\n",
      "        [  96],\n",
      "        [ 100],\n",
      "        [ 101],\n",
      "        [ 103],\n",
      "        [ 106],\n",
      "        [ 107],\n",
      "        [ 108],\n",
      "        [ 109],\n",
      "        [ 114],\n",
      "        [ 118],\n",
      "        [ 120],\n",
      "        [ 122],\n",
      "        [ 124],\n",
      "        [ 125],\n",
      "        [ 126],\n",
      "        [ 127],\n",
      "        [ 128],\n",
      "        [ 130],\n",
      "        [ 131],\n",
      "        [ 133],\n",
      "        [ 134],\n",
      "        [ 135],\n",
      "        [ 139],\n",
      "        [ 144],\n",
      "        [ 148],\n",
      "        [ 149],\n",
      "        [ 151],\n",
      "        [ 156],\n",
      "        [ 161],\n",
      "        [ 163],\n",
      "        [ 164],\n",
      "        [ 173],\n",
      "        [ 174],\n",
      "        [ 177],\n",
      "        [ 178],\n",
      "        [ 187],\n",
      "        [ 193],\n",
      "        [ 198],\n",
      "        [ 200],\n",
      "        [ 202],\n",
      "        [ 204],\n",
      "        [ 208],\n",
      "        [ 221],\n",
      "        [ 223],\n",
      "        [ 224],\n",
      "        [ 225],\n",
      "        [ 226],\n",
      "        [ 227],\n",
      "        [ 230],\n",
      "        [ 231],\n",
      "        [ 237],\n",
      "        [ 239],\n",
      "        [ 240],\n",
      "        [ 242],\n",
      "        [ 243],\n",
      "        [ 245],\n",
      "        [ 250],\n",
      "        [ 252],\n",
      "        [ 256],\n",
      "        [ 259],\n",
      "        [ 265],\n",
      "        [ 268],\n",
      "        [ 270],\n",
      "        [ 271],\n",
      "        [ 278],\n",
      "        [ 282],\n",
      "        [ 284],\n",
      "        [ 286],\n",
      "        [ 289],\n",
      "        [ 293],\n",
      "        [ 295],\n",
      "        [ 301],\n",
      "        [ 302],\n",
      "        [ 304],\n",
      "        [ 306],\n",
      "        [ 307],\n",
      "        [ 313],\n",
      "        [ 321],\n",
      "        [ 323],\n",
      "        [ 324],\n",
      "        [ 332],\n",
      "        [ 334],\n",
      "        [ 337],\n",
      "        [ 342],\n",
      "        [ 345],\n",
      "        [ 346],\n",
      "        [ 353],\n",
      "        [ 356],\n",
      "        [ 364],\n",
      "        [ 369],\n",
      "        [ 371],\n",
      "        [ 373],\n",
      "        [ 374],\n",
      "        [ 375],\n",
      "        [ 378],\n",
      "        [ 379],\n",
      "        [ 380],\n",
      "        [ 383],\n",
      "        [ 387],\n",
      "        [ 390],\n",
      "        [ 397],\n",
      "        [ 399],\n",
      "        [ 402],\n",
      "        [ 407],\n",
      "        [ 410],\n",
      "        [ 411],\n",
      "        [ 413],\n",
      "        [ 423],\n",
      "        [ 424],\n",
      "        [ 426],\n",
      "        [ 427],\n",
      "        [ 430],\n",
      "        [ 432],\n",
      "        [ 433],\n",
      "        [ 444],\n",
      "        [ 451],\n",
      "        [ 452],\n",
      "        [ 453],\n",
      "        [ 455],\n",
      "        [ 456],\n",
      "        [ 461],\n",
      "        [ 463],\n",
      "        [ 467],\n",
      "        [ 470],\n",
      "        [ 472],\n",
      "        [ 476],\n",
      "        [ 477],\n",
      "        [ 478],\n",
      "        [ 480],\n",
      "        [ 483],\n",
      "        [ 486],\n",
      "        [ 489],\n",
      "        [ 492],\n",
      "        [ 493],\n",
      "        [ 494],\n",
      "        [ 495],\n",
      "        [ 496],\n",
      "        [ 498],\n",
      "        [ 499],\n",
      "        [ 502],\n",
      "        [ 506],\n",
      "        [ 513],\n",
      "        [ 516],\n",
      "        [ 517],\n",
      "        [ 522],\n",
      "        [ 524],\n",
      "        [ 525],\n",
      "        [ 526],\n",
      "        [ 530],\n",
      "        [ 535],\n",
      "        [ 536],\n",
      "        [ 538],\n",
      "        [ 541],\n",
      "        [ 542],\n",
      "        [ 547],\n",
      "        [ 548],\n",
      "        [ 552],\n",
      "        [ 555],\n",
      "        [ 559],\n",
      "        [ 562],\n",
      "        [ 563],\n",
      "        [ 565],\n",
      "        [ 569],\n",
      "        [ 570],\n",
      "        [ 574],\n",
      "        [ 576],\n",
      "        [ 577],\n",
      "        [ 578],\n",
      "        [ 579],\n",
      "        [ 584],\n",
      "        [ 588],\n",
      "        [ 589],\n",
      "        [ 590],\n",
      "        [ 591],\n",
      "        [ 593],\n",
      "        [ 595],\n",
      "        [ 599],\n",
      "        [ 602],\n",
      "        [ 610],\n",
      "        [ 611],\n",
      "        [ 612],\n",
      "        [ 613],\n",
      "        [ 614],\n",
      "        [ 616],\n",
      "        [ 618],\n",
      "        [ 621],\n",
      "        [ 624],\n",
      "        [ 626],\n",
      "        [ 629],\n",
      "        [ 631],\n",
      "        [ 632],\n",
      "        [ 633],\n",
      "        [ 636],\n",
      "        [ 643],\n",
      "        [ 644],\n",
      "        [ 645],\n",
      "        [ 648],\n",
      "        [ 652],\n",
      "        [ 653],\n",
      "        [ 656],\n",
      "        [ 659],\n",
      "        [ 662],\n",
      "        [ 665],\n",
      "        [ 667],\n",
      "        [ 673],\n",
      "        [ 677],\n",
      "        [ 678],\n",
      "        [ 685],\n",
      "        [ 690],\n",
      "        [ 692],\n",
      "        [ 697],\n",
      "        [ 701],\n",
      "        [ 702],\n",
      "        [ 704],\n",
      "        [ 707],\n",
      "        [ 718],\n",
      "        [ 721],\n",
      "        [ 724],\n",
      "        [ 727],\n",
      "        [ 728],\n",
      "        [ 732],\n",
      "        [ 733],\n",
      "        [ 734],\n",
      "        [ 736],\n",
      "        [ 737],\n",
      "        [ 738],\n",
      "        [ 740],\n",
      "        [ 741],\n",
      "        [ 743],\n",
      "        [ 744],\n",
      "        [ 746],\n",
      "        [ 747],\n",
      "        [ 752],\n",
      "        [ 754],\n",
      "        [ 759],\n",
      "        [ 765],\n",
      "        [ 767],\n",
      "        [ 768],\n",
      "        [ 769],\n",
      "        [ 774],\n",
      "        [ 785],\n",
      "        [ 791],\n",
      "        [ 793],\n",
      "        [ 795],\n",
      "        [ 798],\n",
      "        [ 799],\n",
      "        [ 802],\n",
      "        [ 805],\n",
      "        [ 806],\n",
      "        [ 807],\n",
      "        [ 808],\n",
      "        [ 811],\n",
      "        [ 813],\n",
      "        [ 815],\n",
      "        [ 816],\n",
      "        [ 820],\n",
      "        [ 821],\n",
      "        [ 823],\n",
      "        [ 824],\n",
      "        [ 828],\n",
      "        [ 833],\n",
      "        [ 835],\n",
      "        [ 837],\n",
      "        [ 841],\n",
      "        [ 845],\n",
      "        [ 846],\n",
      "        [ 850],\n",
      "        [ 852],\n",
      "        [ 853],\n",
      "        [ 856],\n",
      "        [ 857],\n",
      "        [ 860],\n",
      "        [ 862],\n",
      "        [ 868],\n",
      "        [ 869],\n",
      "        [ 872],\n",
      "        [ 875],\n",
      "        [ 876],\n",
      "        [ 881],\n",
      "        [ 888],\n",
      "        [ 894],\n",
      "        [ 904],\n",
      "        [ 905],\n",
      "        [ 906],\n",
      "        [ 908],\n",
      "        [ 912],\n",
      "        [ 914],\n",
      "        [ 915],\n",
      "        [ 918],\n",
      "        [ 920],\n",
      "        [ 923],\n",
      "        [ 924],\n",
      "        [ 925],\n",
      "        [ 929],\n",
      "        [ 931],\n",
      "        [ 933],\n",
      "        [ 934],\n",
      "        [ 938],\n",
      "        [ 940],\n",
      "        [ 943],\n",
      "        [ 946],\n",
      "        [ 947],\n",
      "        [ 960],\n",
      "        [ 961],\n",
      "        [ 963],\n",
      "        [ 964],\n",
      "        [ 972],\n",
      "        [ 973],\n",
      "        [ 978],\n",
      "        [ 982],\n",
      "        [ 984],\n",
      "        [ 985],\n",
      "        [ 986],\n",
      "        [ 988],\n",
      "        [ 989],\n",
      "        [ 992],\n",
      "        [1000],\n",
      "        [1001],\n",
      "        [1004],\n",
      "        [1005],\n",
      "        [1006],\n",
      "        [1007],\n",
      "        [1009],\n",
      "        [1010],\n",
      "        [1011],\n",
      "        [1012],\n",
      "        [1014],\n",
      "        [1015],\n",
      "        [1016],\n",
      "        [1020],\n",
      "        [1022],\n",
      "        [1023],\n",
      "        [1027],\n",
      "        [1029],\n",
      "        [1039],\n",
      "        [1042],\n",
      "        [1043],\n",
      "        [1045],\n",
      "        [1048],\n",
      "        [1049],\n",
      "        [1054],\n",
      "        [1055],\n",
      "        [1057],\n",
      "        [1060],\n",
      "        [1066],\n",
      "        [1067],\n",
      "        [1070],\n",
      "        [1071],\n",
      "        [1072],\n",
      "        [1073],\n",
      "        [1074],\n",
      "        [1077],\n",
      "        [1078],\n",
      "        [1080],\n",
      "        [1083],\n",
      "        [1087],\n",
      "        [1095],\n",
      "        [1100],\n",
      "        [1103],\n",
      "        [1104],\n",
      "        [1107],\n",
      "        [1112],\n",
      "        [1115],\n",
      "        [1117],\n",
      "        [1118],\n",
      "        [1120],\n",
      "        [1121],\n",
      "        [1123],\n",
      "        [1127],\n",
      "        [1128],\n",
      "        [1129],\n",
      "        [1133],\n",
      "        [1134],\n",
      "        [1136],\n",
      "        [1139],\n",
      "        [1142],\n",
      "        [1144],\n",
      "        [1146],\n",
      "        [1149],\n",
      "        [1154],\n",
      "        [1156],\n",
      "        [1157],\n",
      "        [1158],\n",
      "        [1159],\n",
      "        [1161],\n",
      "        [1165],\n",
      "        [1167],\n",
      "        [1169],\n",
      "        [1172],\n",
      "        [1173],\n",
      "        [1174],\n",
      "        [1175],\n",
      "        [1176],\n",
      "        [1178],\n",
      "        [1179],\n",
      "        [1181],\n",
      "        [1183],\n",
      "        [1184],\n",
      "        [1185],\n",
      "        [1186],\n",
      "        [1190],\n",
      "        [1193],\n",
      "        [1199],\n",
      "        [1205],\n",
      "        [1206],\n",
      "        [1207],\n",
      "        [1208],\n",
      "        [1209],\n",
      "        [1210],\n",
      "        [1219],\n",
      "        [1220],\n",
      "        [1222],\n",
      "        [1223],\n",
      "        [1224],\n",
      "        [1225],\n",
      "        [1226],\n",
      "        [1227],\n",
      "        [1228],\n",
      "        [1229],\n",
      "        [1242],\n",
      "        [1243],\n",
      "        [1246],\n",
      "        [1248],\n",
      "        [1249],\n",
      "        [1250],\n",
      "        [1254],\n",
      "        [1256],\n",
      "        [1262],\n",
      "        [1266],\n",
      "        [1267],\n",
      "        [1272],\n",
      "        [1274],\n",
      "        [1275],\n",
      "        [1278],\n",
      "        [1279],\n",
      "        [1280],\n",
      "        [1282],\n",
      "        [1285],\n",
      "        [1289],\n",
      "        [1295],\n",
      "        [1297],\n",
      "        [1308],\n",
      "        [1309],\n",
      "        [1312],\n",
      "        [1320],\n",
      "        [1325],\n",
      "        [1326],\n",
      "        [1328],\n",
      "        [1331],\n",
      "        [1339],\n",
      "        [1341],\n",
      "        [1345],\n",
      "        [1348],\n",
      "        [1351],\n",
      "        [1352],\n",
      "        [1354],\n",
      "        [1355],\n",
      "        [1356],\n",
      "        [1357],\n",
      "        [1358],\n",
      "        [1361],\n",
      "        [1362],\n",
      "        [1363],\n",
      "        [1364],\n",
      "        [1368],\n",
      "        [1371],\n",
      "        [1372],\n",
      "        [1376],\n",
      "        [1377],\n",
      "        [1378],\n",
      "        [1388],\n",
      "        [1389],\n",
      "        [1390],\n",
      "        [1399],\n",
      "        [1402],\n",
      "        [1405],\n",
      "        [1408],\n",
      "        [1409],\n",
      "        [1416],\n",
      "        [1417],\n",
      "        [1419],\n",
      "        [1424],\n",
      "        [1426],\n",
      "        [1428],\n",
      "        [1429],\n",
      "        [1431],\n",
      "        [1434],\n",
      "        [1438],\n",
      "        [1439],\n",
      "        [1440],\n",
      "        [1441],\n",
      "        [1445],\n",
      "        [1451],\n",
      "        [1453],\n",
      "        [1454],\n",
      "        [1455],\n",
      "        [1457],\n",
      "        [1458],\n",
      "        [1459],\n",
      "        [1460],\n",
      "        [1462],\n",
      "        [1465],\n",
      "        [1466],\n",
      "        [1467],\n",
      "        [1468],\n",
      "        [1469],\n",
      "        [1471],\n",
      "        [1472],\n",
      "        [1474],\n",
      "        [1476],\n",
      "        [1478],\n",
      "        [1479],\n",
      "        [1481],\n",
      "        [1482],\n",
      "        [1489],\n",
      "        [1490],\n",
      "        [1494],\n",
      "        [1504],\n",
      "        [1508],\n",
      "        [1509],\n",
      "        [1510],\n",
      "        [1513],\n",
      "        [1514],\n",
      "        [1515],\n",
      "        [1516],\n",
      "        [1519],\n",
      "        [1520],\n",
      "        [1521],\n",
      "        [1522],\n",
      "        [1524],\n",
      "        [1527],\n",
      "        [1534],\n",
      "        [1536],\n",
      "        [1539],\n",
      "        [1544],\n",
      "        [1547],\n",
      "        [1550],\n",
      "        [1551],\n",
      "        [1552],\n",
      "        [1553],\n",
      "        [1555],\n",
      "        [1558],\n",
      "        [1559],\n",
      "        [1560],\n",
      "        [1561],\n",
      "        [1563],\n",
      "        [1567],\n",
      "        [1568],\n",
      "        [1571],\n",
      "        [1573],\n",
      "        [1577],\n",
      "        [1578],\n",
      "        [1579],\n",
      "        [1582],\n",
      "        [1585],\n",
      "        [1587],\n",
      "        [1588],\n",
      "        [1593],\n",
      "        [1594],\n",
      "        [1600],\n",
      "        [1601],\n",
      "        [1603],\n",
      "        [1610],\n",
      "        [1612],\n",
      "        [1613],\n",
      "        [1617],\n",
      "        [1618],\n",
      "        [1621],\n",
      "        [1627],\n",
      "        [1630],\n",
      "        [1631],\n",
      "        [1632],\n",
      "        [1633],\n",
      "        [1634],\n",
      "        [1636],\n",
      "        [1637],\n",
      "        [1640],\n",
      "        [1644],\n",
      "        [1646],\n",
      "        [1649],\n",
      "        [1653],\n",
      "        [1654],\n",
      "        [1656],\n",
      "        [1657],\n",
      "        [1658],\n",
      "        [1660],\n",
      "        [1661],\n",
      "        [1662],\n",
      "        [1668],\n",
      "        [1669],\n",
      "        [1671],\n",
      "        [1673],\n",
      "        [1675],\n",
      "        [1680],\n",
      "        [1683],\n",
      "        [1685],\n",
      "        [1686],\n",
      "        [1689],\n",
      "        [1691],\n",
      "        [1696],\n",
      "        [1697],\n",
      "        [1699],\n",
      "        [1700],\n",
      "        [1701],\n",
      "        [1702],\n",
      "        [1706],\n",
      "        [1709],\n",
      "        [1711],\n",
      "        [1712],\n",
      "        [1713],\n",
      "        [1716],\n",
      "        [1717],\n",
      "        [1718],\n",
      "        [1720],\n",
      "        [1721],\n",
      "        [1723],\n",
      "        [1726],\n",
      "        [1728],\n",
      "        [1735],\n",
      "        [1740],\n",
      "        [1741],\n",
      "        [1742],\n",
      "        [1743],\n",
      "        [1745],\n",
      "        [1748],\n",
      "        [1750],\n",
      "        [1756],\n",
      "        [1758],\n",
      "        [1759],\n",
      "        [1762],\n",
      "        [1766],\n",
      "        [1767],\n",
      "        [1768],\n",
      "        [1771],\n",
      "        [1772],\n",
      "        [1773],\n",
      "        [1778],\n",
      "        [1783],\n",
      "        [1784],\n",
      "        [1787],\n",
      "        [1794],\n",
      "        [1795],\n",
      "        [1796],\n",
      "        [1797],\n",
      "        [1799],\n",
      "        [1800],\n",
      "        [1803],\n",
      "        [1804],\n",
      "        [1807],\n",
      "        [1808],\n",
      "        [1810],\n",
      "        [1814],\n",
      "        [1817],\n",
      "        [1819],\n",
      "        [1821],\n",
      "        [1822],\n",
      "        [1823],\n",
      "        [1824],\n",
      "        [1825],\n",
      "        [1828],\n",
      "        [1831],\n",
      "        [1833],\n",
      "        [1838],\n",
      "        [1839],\n",
      "        [1843],\n",
      "        [1846],\n",
      "        [1848],\n",
      "        [1858],\n",
      "        [1860],\n",
      "        [1861],\n",
      "        [1862],\n",
      "        [1863],\n",
      "        [1864],\n",
      "        [1868],\n",
      "        [1869],\n",
      "        [1875],\n",
      "        [1876],\n",
      "        [1877],\n",
      "        [1878],\n",
      "        [1879],\n",
      "        [1884],\n",
      "        [1885],\n",
      "        [1888],\n",
      "        [1894],\n",
      "        [1895],\n",
      "        [1896],\n",
      "        [1898],\n",
      "        [1900],\n",
      "        [1903],\n",
      "        [1906],\n",
      "        [1908],\n",
      "        [1912],\n",
      "        [1913],\n",
      "        [1917],\n",
      "        [1918],\n",
      "        [1922],\n",
      "        [1923],\n",
      "        [1924],\n",
      "        [1925],\n",
      "        [1926],\n",
      "        [1931],\n",
      "        [1934],\n",
      "        [1935],\n",
      "        [1936],\n",
      "        [1942],\n",
      "        [1945],\n",
      "        [1947],\n",
      "        [1952],\n",
      "        [1956],\n",
      "        [1957],\n",
      "        [1959],\n",
      "        [1962],\n",
      "        [1963],\n",
      "        [1964],\n",
      "        [1968],\n",
      "        [1971],\n",
      "        [1973],\n",
      "        [1975],\n",
      "        [1977],\n",
      "        [1978],\n",
      "        [1979],\n",
      "        [1983],\n",
      "        [1984],\n",
      "        [1986],\n",
      "        [1987],\n",
      "        [1988],\n",
      "        [1991],\n",
      "        [1992],\n",
      "        [1995],\n",
      "        [1999],\n",
      "        [2001],\n",
      "        [2003],\n",
      "        [2010],\n",
      "        [2011],\n",
      "        [2013],\n",
      "        [2015],\n",
      "        [2016],\n",
      "        [2020],\n",
      "        [2021],\n",
      "        [2022],\n",
      "        [2023],\n",
      "        [2027],\n",
      "        [2032],\n",
      "        [2034],\n",
      "        [2035],\n",
      "        [2036],\n",
      "        [2037],\n",
      "        [2040],\n",
      "        [2042]], device='cuda:0')\n",
      "Values at those positions:\n",
      "  Position (0,): 0.6889573931694031 vs 0.6889574527740479\n",
      "  Position (1,): 0.735554575920105 vs 0.7355546355247498\n",
      "  Position (2,): 0.37725359201431274 vs 0.37725356221199036\n",
      "  Position (3,): -0.2203873097896576 vs -0.22038733959197998\n",
      "  Position (5,): -1.1010934114456177 vs -1.1010935306549072\n",
      "  Position (6,): 0.5541995167732239 vs 0.5541994571685791\n",
      "  Position (7,): 1.360838770866394 vs 1.3608388900756836\n",
      "  Position (10,): 0.009813867509365082 vs 0.009813874959945679\n",
      "  Position (11,): -0.7984451055526733 vs -0.7984451651573181\n",
      "  Position (18,): 0.2542675733566284 vs 0.25426754355430603\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-af73a7a9c8b0>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# print(i, j)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;31m# Print the differences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "normal_time = []\n",
    "compiled_time = []\n",
    "\n",
    "backward_normal_time = []\n",
    "backward_compiled_time = []\n",
    "\n",
    "for i in range(100):\n",
    "    old_ctx = deep_copy_ctx(ctx)\n",
    "    old_ctx2 = deep_copy_ctx(ctx)\n",
    "    \n",
    "    h_in = torch.randn(batch_size, in_features, device=device)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    out_forward = sklinear_forward(h_in, self.S1s, self.S2s, self.U1s, self.U2s, self.bias)\n",
    "    torch.cuda.synchronize()\n",
    "    normal_time.append(time.perf_counter() - start)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    out_forward_compiled = sklinear_forward_compiled(h_in, self.S1s, self.S2s, self.U1s, self.U2s, self.bias)\n",
    "    torch.cuda.synchronize()\n",
    "    compiled_time.append(time.perf_counter() - start)\n",
    "\n",
    "    assert torch.allclose(out_forward, out_forward_compiled)\n",
    "\n",
    "    grad_out = torch.randn(batch_size, out_features, device=device)\n",
    "    grad_out1 = grad_out.clone().detach()\n",
    "    grad_out2 = grad_out.clone().detach()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    out_forward = sklinear_backward(old_ctx, grad_out1)\n",
    "    torch.cuda.synchronize()\n",
    "    backward_normal_time.append(time.perf_counter() - start)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    out_forward_compiled = sklinear_backward_compiled(old_ctx2, grad_out2)\n",
    "    torch.cuda.synchronize()\n",
    "    backward_compiled_time.append(time.perf_counter() - start)\n",
    "\n",
    "    m = -1\n",
    "    for i, j in zip(out_forward, out_forward_compiled):\n",
    "        m = m + 1\n",
    "        if i is None and j is None:\n",
    "            continue\n",
    "        elif (i is None and j is not None) or (i is not None and j is None):\n",
    "            raise TypeError(\"not both are None\")\n",
    "        else:\n",
    "            # print(\"in i :\", m)\n",
    "            # print(type(i), type(j))\n",
    "            # print(i, j)\n",
    "            try:\n",
    "                assert torch.allclose(i, j)\n",
    "            except AssertionError:\n",
    "                # Print the differences\n",
    "                print(\"Assertion failed!\")\n",
    "                print(f\"Max absolute difference: {torch.max(torch.abs(i - j)).item()}\")\n",
    "                print(f\"Mean absolute difference: {torch.mean(torch.abs(i - j)).item()}\")\n",
    "                print(f\"Positions where they differ: {(i != j).nonzero()}\")\n",
    "                print(f\"Values at those positions:\")\n",
    "                different_indices = (i != j).nonzero()\n",
    "                if len(different_indices) > 0:\n",
    "                    # Only show up to 10 differences to avoid overwhelming output\n",
    "                    for idx in different_indices[:10]:\n",
    "                        idx_tuple = tuple(idx.tolist())\n",
    "                        print(f\"  Position {idx_tuple}: {i[idx_tuple].item()} vs {j[idx_tuple].item()}\")\n",
    "                # Re-raise the assertion error\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-18T13:08:35.863145Z",
     "iopub.status.idle": "2025-03-18T13:08:35.863408Z",
     "shell.execute_reply": "2025-03-18T13:08:35.863306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_benchmark_results(normal_time, compiled_time):\n",
    "    iterations = range(1, len(normal_time) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot times\n",
    "    plt.plot(iterations, normal_time, 'b-o', label='Normal Execution')\n",
    "    plt.plot(iterations, compiled_time, 'r-o', label='TorchScript Compiled')\n",
    "    \n",
    "    # Add mean time lines\n",
    "    plt.axhline(y=np.mean(normal_time), color='b', linestyle='--', \n",
    "                label=f'Avg Normal: {np.mean(normal_time):.6f}s')\n",
    "    plt.axhline(y=np.mean(compiled_time), color='r', linestyle='--', \n",
    "                label=f'Avg Compiled: {np.mean(compiled_time):.6f}s')\n",
    "    \n",
    "    # Speedup calculation\n",
    "    speedup = np.mean(normal_time) / np.mean(compiled_time)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title(f'PyTorch Normal vs Compiled Execution Time\\nSpeedup: {speedup:.2f}x')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add text showing speedup\n",
    "    plt.text(0.02, 0.95, f'Speedup: {speedup:.2f}x', transform=plt.gca().transAxes, \n",
    "             bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_benchmark_results(normal_time, compiled_time)\n",
    "plot_benchmark_results(backward_normal_time, backward_compiled_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-18T13:08:35.864236Z",
     "iopub.status.idle": "2025-03-18T13:08:35.864519Z",
     "shell.execute_reply": "2025-03-18T13:08:35.864390Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from panther.utils import mysin\n",
    "\n",
    "# x = torch.randn(3, device=\"cuda\")\n",
    "# y = mysin(x)\n",
    "\n",
    "# assert torch.allclose(y, x.sin())\n",
    "# print(x)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-18T13:08:35.865059Z",
     "iopub.status.idle": "2025-03-18T13:08:35.865379Z",
     "shell.execute_reply": "2025-03-18T13:08:35.865271Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import panther\n",
    "# print(dir(panther.utils))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-18T13:08:35.866730Z",
     "iopub.status.idle": "2025-03-18T13:08:35.867044Z",
     "shell.execute_reply": "2025-03-18T13:08:35.866878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from panther.utils import out_add_fn\n",
    "\n",
    "# x = torch.randn(4, device=\"cuda\")\n",
    "# y = torch.randn(4, device=\"cuda\")\n",
    "# out = out_add_fn(x, y)\n",
    "# print(f\"Vector addition of\\nX:\\t{x}\\nY:\\t{y}\\nis equal to\\n{out}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
