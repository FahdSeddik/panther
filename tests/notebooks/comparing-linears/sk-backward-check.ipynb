{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9YQL8cV2icf",
    "outputId": "4da1cd95-0593-4b60-b533-a0c21c261934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed: True\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "class SketchedLinearFunction(Function):\n",
    "    # Note that forward, setup_context, and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        input: torch.Tensor,\n",
    "        S1s: torch.Tensor,\n",
    "        S2s: torch.Tensor,\n",
    "        U1s: torch.Tensor,\n",
    "        U2s: torch.Tensor,\n",
    "        bias: torch.Tensor,\n",
    "    ):\n",
    "        d1, num_terms = S2s.shape[2], S2s.shape[0]\n",
    "        tot = torch.zeros(\n",
    "            (input.shape[0], d1), device=input.device, dtype=torch.float64\n",
    "        )\n",
    "        # Efficiently perform the sum over all l terms\n",
    "        for i in range(num_terms):\n",
    "            tot += (input.mm(S1s[i])).mm(U1s[i]) + (input.mm(U2s[i])).mm(S2s[i])\n",
    "        return tot / (2 * num_terms) + bias\n",
    "\n",
    "    @staticmethod\n",
    "    # inputs is a Tuple of all of the inputs passed to forward.\n",
    "    # output is the output of the forward().\n",
    "    def setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any):\n",
    "        input, S1s, S2s, U1s, U2s, bias = inputs\n",
    "        ctx.save_for_backward(input, S1s, S2s, U1s, U2s, bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, *grad_output: Any) -> Any:\n",
    "        # dl/dS2_i = U1_i g h_in^T / 2 * l\n",
    "        # dl/dS1_i = g h_in^T U2_i^T / 2 * l\n",
    "        # dl/dh_in = 1/(2*l) * (sum_{i=1}^{l} (S1_i^T U1_i g) + sum_{i=1}^{l} (U2_i^T S2_i g))\n",
    "        # dl/db = g\n",
    "        input, S1s, S2s, U1s, U2s, _ = ctx.saved_tensors\n",
    "        num_terms = S2s.shape[0]\n",
    "        g = grad_output[0] / (2 * num_terms)\n",
    "\n",
    "        grad = torch.zeros(input.shape, device=input.device, dtype=torch.float64)\n",
    "\n",
    "        grad_S1s = torch.zeros(S1s.shape, device=input.device, dtype=torch.float64)\n",
    "\n",
    "        grad_S2s = torch.zeros(S2s.shape, device=input.device, dtype=torch.float64)\n",
    "\n",
    "        for i in range(num_terms):\n",
    "            grad += (g.mm(U1s[i].T)).mm(S1s[i].T) + (g.mm(S2s[i].T)).mm(U2s[i].T)\n",
    "            grad_S2s[i] = (U2s[i].T.mm(input.T)).mm(g)\n",
    "            grad_S1s[i] = input.T.mm(g.mm(U1s[i].T))\n",
    "\n",
    "        return (\n",
    "            grad,\n",
    "            grad_S1s,\n",
    "            grad_S2s,\n",
    "            None,\n",
    "            None,\n",
    "            # sum g on batch dimension input.shape[0]\n",
    "            g.reshape(input.shape[0], -1).sum(0),\n",
    "        )\n",
    "\n",
    "\n",
    "class SKLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        num_terms: int,\n",
    "        low_rank: int,\n",
    "        bias_init_std=0.01,\n",
    "        W=None,\n",
    "    ):\n",
    "        super(SKLinear, self).__init__()\n",
    "\n",
    "        # if 2 * num_terms * low_rank * (output_dim + input_dim) > output_dim * input_dim:\n",
    "        #     raise ValueError(\n",
    "        #         \"The number of parameters in the sketching layer is larger \"\n",
    "        #         + \"than the number of parameters in the fully connected layer.\"\n",
    "        #     )\n",
    "\n",
    "        self.l = num_terms\n",
    "        self.k = low_rank\n",
    "        self.d1 = output_dim\n",
    "        self.d2 = input_dim\n",
    "\n",
    "        def generate_U(k: int, d: int) -> torch.Tensor:\n",
    "            """\n",
    "            Generate a random matrix U with orthonormal rows.\n",
    "            """\n",
    "            return (\n",
    "                torch.randint(0, 2, (k, d), dtype=torch.float64) * 2 - 1\n",
    "            ) / torch.sqrt(torch.tensor(k, dtype=torch.float64))\n",
    "\n",
    "        # Register U1s and U2s as buffers since they are not learnable\n",
    "        self.register_buffer(\n",
    "            \"U1s\",\n",
    "            torch.stack([generate_U(low_rank, output_dim) for _ in range(num_terms)]),\n",
    "        )  # kxd1\n",
    "        self.register_buffer(\n",
    "            \"U2s\",\n",
    "            torch.stack([generate_U(input_dim, low_rank) for _ in range(num_terms)]),\n",
    "        )  # d2xk\n",
    "\n",
    "        # W is used to only initialize S\n",
    "\n",
    "        W = (\n",
    "            torch.randn(input_dim, output_dim, dtype=torch.float64)\n",
    "            if W is None\n",
    "            else W.T.clone().detach()\n",
    "        )\n",
    "\n",
    "        # S1s and S2s are precomputed but not updated in the backward pass\n",
    "        self.S1s = nn.Parameter(\n",
    "            torch.stack([torch.matmul(W, self.U1s[i].T) for i in range(num_terms)])\n",
    "        )  # d2xk\n",
    "        self.S2s = nn.Parameter(\n",
    "            torch.stack([torch.matmul(self.U2s[i].T, W) for i in range(num_terms)])\n",
    "        )  # kxd1\n",
    "\n",
    "        # Bias term initialized with a small standard deviation\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.randn(output_dim, dtype=torch.float64) * bias_init_std\n",
    "        )\n",
    "\n",
    "    def forward(self, h_in):\n",
    "        """\n",
    "        Forward pass calculation as per the given formula:\n",
    "        a = (1/(2*l)) * sum_{i=1}^{l} (U1_i^T S1_i h_in)\n",
    "            + (1/(2*l)) * sum_{i=1}^{l} (S2_i U2_i h_in)\n",
    "            + b\n",
    "        """\n",
    "        return SketchedLinearFunction.apply(\n",
    "            h_in, self.S1s, self.S2s, self.U1s, self.U2s, self.bias\n",
    "        )\n",
    "\n",
    "\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "\n",
    "def test_grad():\n",
    "    # Test parameters\n",
    "    d2 = 20  # input dimension\n",
    "    d1 = 30  # output dimension\n",
    "    l = 3  # number of sketches\n",
    "    k = 10  # sketch dimension\n",
    "    batch_size = 4\n",
    "\n",
    "    # Create inputs with double precision\n",
    "    h_in = torch.randn(batch_size, d2, dtype=torch.float64, requires_grad=True)\n",
    "    layer = SKLinear(input_dim=d2, output_dim=d1, num_terms=l, low_rank=k)\n",
    "\n",
    "    def test_func(h_in):\n",
    "        return layer(h_in)\n",
    "\n",
    "    test = gradcheck(test_func, h_in, eps=1e-6, atol=1e-4)\n",
    "    print(f\"Gradient check passed: {test}\")\n",
    "\n",
    "\n",
    "test_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybqzr5mF38TR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}