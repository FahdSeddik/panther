{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-19T00:01:41.727632Z",
     "iopub.status.busy": "2025-03-19T00:01:41.727317Z",
     "iopub.status.idle": "2025-03-19T00:01:56.736942Z",
     "shell.execute_reply": "2025-03-19T00:01:56.735645Z",
     "shell.execute_reply.started": "2025-03-19T00:01:41.727594Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision==0.20.1 in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio==2.5.1 in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1) (1.13.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.20.1) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.20.1) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.20.1) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.20.1) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.20.1) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.20.1) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.20.1) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.20.1) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision==0.20.1) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision==0.20.1) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.20.1) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision==0.20.1) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision==0.20.1) (2024.2.0)\n",
      "\u001b[33mWARNING: Skipping pytorch-triton as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/nightly/cu121\n",
      "Collecting triton\n",
      "  Downloading https://download.pytorch.org/whl/nightly/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.17.0)\n",
      "Installing collected packages: triton\n",
      "Successfully installed triton-2.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1\n",
    "# !pip uninstall -y pytorch-triton\n",
    "\n",
    "# !pip install -U --pre triton --index-url https://download.pytorch.org/whl/nightly/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T18:52:36.547956Z",
     "iopub.status.busy": "2025-03-19T18:52:36.547556Z",
     "iopub.status.idle": "2025-03-19T18:55:13.232726Z",
     "shell.execute_reply": "2025-03-19T18:55:13.231818Z",
     "shell.execute_reply.started": "2025-03-19T18:52:36.547914Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp310-cp310-linux_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.2.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n",
      "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl (848.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m956.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp310-cp310-linux_x86_64.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchaudio, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cu121\n",
      "    Uninstalling torch-2.5.1+cu121:\n",
      "      Successfully uninstalled torch-2.5.1+cu121\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.5.1+cu121\n",
      "    Uninstalling torchaudio-2.5.1+cu121:\n",
      "      Successfully uninstalled torchaudio-2.5.1+cu121\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.20.1+cu121\n",
      "    Uninstalling torchvision-0.20.1+cu121:\n",
      "      Successfully uninstalled torchvision-0.20.1+cu121\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118 torchaudio-2.6.0+cu118 torchvision-0.21.0+cu118 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \\\n",
    "  torch torchvision torchaudio \\\n",
    "  --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T18:57:11.424551Z",
     "iopub.status.busy": "2025-03-19T18:57:11.423994Z",
     "iopub.status.idle": "2025-03-19T18:57:11.430247Z",
     "shell.execute_reply": "2025-03-19T18:57:11.429297Z",
     "shell.execute_reply.started": "2025-03-19T18:57:11.424522Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"TRITON_INTERPRET\"] = \"1\"\n",
    "# print(os.environ[\"TRITON_INTERPRET\"], \"\\n\")\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "print(triton.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T18:21:57.217667Z",
     "iopub.status.busy": "2025-03-19T18:21:57.217269Z",
     "iopub.status.idle": "2025-03-19T18:22:00.794767Z",
     "shell.execute_reply": "2025-03-19T18:22:00.793906Z",
     "shell.execute_reply.started": "2025-03-19T18:21:57.217641Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!export LC_ALL=\"en_US.UTF-8\"\n",
    "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "!ldconfig /usr/lib64-nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stable one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T19:18:39.502441Z",
     "iopub.status.busy": "2025-03-19T19:18:39.502083Z",
     "iopub.status.idle": "2025-03-19T19:18:40.648578Z",
     "shell.execute_reply": "2025-03-19T19:18:40.647833Z",
     "shell.execute_reply.started": "2025-03-19T19:18:39.502412Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (Python time): 1.115365 seconds\n",
      "✅ Triton and Torch match 1\n",
      "✅ Triton and Torch match 2\n"
     ]
    }
   ],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_K\": 256,\n",
    "                \"BLOCK_SIZE_D2\": 64,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=1,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 64,\n",
    "                \"BLOCK_SIZE_K\": 256,\n",
    "                \"BLOCK_SIZE_D2\": 32,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_K\": 128,\n",
    "                \"BLOCK_SIZE_D2\": 32,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_K\": 64,\n",
    "                \"BLOCK_SIZE_D2\": 32,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 64,\n",
    "                \"BLOCK_SIZE_K\": 128,\n",
    "                \"BLOCK_SIZE_D2\": 32,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_K\": 32,\n",
    "                \"BLOCK_SIZE_D2\": 32,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 64,\n",
    "                \"BLOCK_SIZE_K\": 32,\n",
    "                \"BLOCK_SIZE_D2\": 32,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=5,\n",
    "            num_warps=2,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 32,\n",
    "                \"BLOCK_SIZE_K\": 64,\n",
    "                \"BLOCK_SIZE_D2\": 32,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=5,\n",
    "            num_warps=2,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_K\": 256,\n",
    "                \"BLOCK_SIZE_D2\": 128,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=3,\n",
    "            num_warps=8,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 256,\n",
    "                \"BLOCK_SIZE_K\": 128,\n",
    "                \"BLOCK_SIZE_D2\": 128,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=3,\n",
    "            num_warps=8,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 256,\n",
    "                \"BLOCK_SIZE_K\": 64,\n",
    "                \"BLOCK_SIZE_D2\": 128,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 64,\n",
    "                \"BLOCK_SIZE_K\": 256,\n",
    "                \"BLOCK_SIZE_D2\": 128,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_K\": 128,\n",
    "                \"BLOCK_SIZE_D2\": 128,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_K\": 64,\n",
    "                \"BLOCK_SIZE_D2\": 64,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 64,\n",
    "                \"BLOCK_SIZE_K\": 128,\n",
    "                \"BLOCK_SIZE_D2\": 64,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_K\": 32,\n",
    "                \"BLOCK_SIZE_D2\": 64,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "    ],\n",
    "    key=[\"BSIZE\", \"K\", \"d2\", \"L\"],\n",
    ")\n",
    "@triton.jit\n",
    "def first_pass_kernel(\n",
    "    hin_ptr,\n",
    "    S1s_ptr,\n",
    "    U2s_ptr,\n",
    "    out1_ptr,\n",
    "    out2_ptr,\n",
    "    BSIZE,\n",
    "    K,\n",
    "    d2,\n",
    "    L,\n",
    "    stride_hin_bsize,\n",
    "    stride_hin_d2,\n",
    "    stride_su_l,\n",
    "    stride_su_d2,\n",
    "    stride_su_k,\n",
    "    stride_out_l,\n",
    "    stride_out_bsize,\n",
    "    stride_out_k,\n",
    "    BLOCK_SIZE_BSIZE: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    BLOCK_SIZE_D2: tl.constexpr,\n",
    "    GROUP_SIZE_BSIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=1)\n",
    "    batch_id = tl.program_id(axis=0)\n",
    "\n",
    "    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n",
    "    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n",
    "    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_k\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n",
    "    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n",
    "    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n",
    "    pid_k = (pid % num_pid_in_group) // group_size_bsize\n",
    "\n",
    "    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "    offs_k = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
    "    offs_d2 = tl.arange(0, BLOCK_SIZE_D2)\n",
    "\n",
    "    offs_bsize = tl.max_contiguous(\n",
    "        tl.multiple_of(offs_bsize, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE\n",
    "    )\n",
    "    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_K), BLOCK_SIZE_K)\n",
    "    offs_d2 = tl.max_contiguous(tl.multiple_of(offs_d2, BLOCK_SIZE_D2), BLOCK_SIZE_D2)\n",
    "\n",
    "    hin_ptrs = hin_ptr + (\n",
    "        offs_bsize[:, None] * stride_hin_bsize + offs_d2[None, :] * stride_hin_d2\n",
    "    )\n",
    "\n",
    "    su_tmp = batch_id * stride_su_l + (\n",
    "        offs_d2[:, None] * stride_su_d2 + offs_k[None, :] * stride_su_k\n",
    "    )\n",
    "    S1s_ptrs = S1s_ptr + su_tmp\n",
    "    U2s_ptrs = U2s_ptr + su_tmp\n",
    "\n",
    "    accumulator1 = tl.full(\n",
    "        shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), value=0.0, dtype=tl.float32\n",
    "    )\n",
    "    accumulator2 = tl.full(\n",
    "        shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), value=0.0, dtype=tl.float32\n",
    "    )\n",
    "\n",
    "    for d2_i in range(0, tl.cdiv(d2, BLOCK_SIZE_D2)):\n",
    "        hin_mask = (offs_bsize[:, None] < BSIZE) & (\n",
    "            offs_d2[None, :] < d2 - d2_i * BLOCK_SIZE_D2\n",
    "        )\n",
    "        hin = tl.load(hin_ptrs, mask=hin_mask, other=0.0)\n",
    "\n",
    "        su_mask = (offs_d2[:, None] < d2 - d2_i * BLOCK_SIZE_D2) & (offs_k[None, :] < K)\n",
    "        S1s = tl.load(S1s_ptrs, mask=su_mask, other=0.0)\n",
    "        U2s = tl.load(U2s_ptrs, mask=su_mask, other=0.0)\n",
    "\n",
    "        accumulator1 += tl.dot(hin, S1s)\n",
    "        accumulator2 += tl.dot(hin, U2s)\n",
    "\n",
    "        hin_ptrs += BLOCK_SIZE_D2 * stride_hin_d2\n",
    "        S1s_ptrs += BLOCK_SIZE_D2 * stride_su_d2\n",
    "        U2s_ptrs += BLOCK_SIZE_D2 * stride_su_d2\n",
    "\n",
    "    accumulator1 = accumulator1.to(tl.float16)\n",
    "    accumulator2 = accumulator2.to(tl.float16)\n",
    "\n",
    "    out_tmp = (\n",
    "        batch_id * stride_out_l\n",
    "        + stride_out_bsize * offs_bsize[:, None]\n",
    "        + stride_out_k * offs_k[None, :]\n",
    "    )\n",
    "    out1_ptrs = out1_ptr + out_tmp\n",
    "    out2_ptrs = out2_ptr + out_tmp\n",
    "\n",
    "    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_k[None, :] < K)\n",
    "\n",
    "    tl.store(out1_ptrs, accumulator1, mask=out_mask)\n",
    "    tl.store(out2_ptrs, accumulator2, mask=out_mask)\n",
    "\n",
    "\n",
    "def first_pass(hin, S1s, U2s):\n",
    "    device = \"cuda\"\n",
    "    assert hin.shape[1] == S1s.shape[1], \"Incompatible dimensions\"\n",
    "    assert hin.shape[1] == U2s.shape[1], \"Incompatible dimensions\"\n",
    "    assert hin.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    assert S1s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    assert U2s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    assert S1s.stride() == U2s.stride(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    BSIZE, d2 = hin.shape\n",
    "    L, _, K = S1s.shape\n",
    "\n",
    "    out1 = torch.empty((L, BSIZE, K), dtype=torch.float16, device=device)\n",
    "    out2 = torch.empty((L, BSIZE, K), dtype=torch.float16, device=device)\n",
    "\n",
    "    stride_hin_bsize, stride_hin_d2 = hin.stride()\n",
    "    stride_su_l, stride_su_d2, stride_su_k = S1s.stride()\n",
    "    stride_out_l, stride_out_bsize, stride_out_k = out1.stride()\n",
    "\n",
    "    assert out1.stride() == out2.stride(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    grid = lambda META: (\n",
    "        L,\n",
    "        triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"])\n",
    "        * triton.cdiv(K, META[\"BLOCK_SIZE_K\"]),\n",
    "    )\n",
    "\n",
    "    first_pass_kernel[grid](\n",
    "        hin,\n",
    "        S1s,\n",
    "        U2s,\n",
    "        out1,\n",
    "        out2,\n",
    "        BSIZE,\n",
    "        K,\n",
    "        d2,\n",
    "        L,\n",
    "        stride_hin_bsize,\n",
    "        stride_hin_d2,\n",
    "        stride_su_l,\n",
    "        stride_su_d2,\n",
    "        stride_su_k,\n",
    "        stride_out_l,\n",
    "        stride_out_bsize,\n",
    "        stride_out_k,\n",
    "    )\n",
    "\n",
    "    return out1, out2\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "L = 4\n",
    "BSIZE, d2 = 128, 16\n",
    "K = 256\n",
    "\n",
    "torch.manual_seed(0)\n",
    "hin = torch.randn((BSIZE, d2), dtype=torch.float16, device=device)\n",
    "S1s = torch.randn((L, d2, K), dtype=torch.float16, device=device)\n",
    "U2s = torch.randn((L, d2, K), dtype=torch.float16, device=device)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "out1, out2 = first_pass(hin, S1s, U2s)\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f\"Time taken (Python time): {end_time - start_time:.6f} seconds\")\n",
    "\n",
    "torch_output1 = (hin.unsqueeze(0).expand(L, BSIZE, d2)).bmm(S1s)\n",
    "torch_output2 = (hin.unsqueeze(0).expand(L, BSIZE, d2)).bmm(U2s)\n",
    "\n",
    "rtol = 1e-2\n",
    "if torch.allclose(out1, torch_output1, atol=1e-2, rtol=rtol):\n",
    "    print(\"✅ Triton and Torch match 1\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch differ 1\")\n",
    "\n",
    "if torch.allclose(out2, torch_output2, atol=1e-2, rtol=rtol):\n",
    "    print(\"✅ Triton and Torch match 2\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch differ 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T19:13:38.271615Z",
     "iopub.status.busy": "2025-03-19T19:13:38.271323Z",
     "iopub.status.idle": "2025-03-19T19:13:39.200411Z",
     "shell.execute_reply": "2025-03-19T19:13:39.199638Z",
     "shell.execute_reply.started": "2025-03-19T19:13:38.271594Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Torch match\n"
     ]
    }
   ],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_D1\": 64,\n",
    "                \"BLOCK_SIZE_K\": 256,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=1,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 64,\n",
    "                \"BLOCK_SIZE_D1\": 32,\n",
    "                \"BLOCK_SIZE_K\": 256,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_D1\": 32,\n",
    "                \"BLOCK_SIZE_K\": 128,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_D1\": 32,\n",
    "                \"BLOCK_SIZE_K\": 64,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 64,\n",
    "                \"BLOCK_SIZE_D1\": 32,\n",
    "                \"BLOCK_SIZE_K\": 128,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_D1\": 32,\n",
    "                \"BLOCK_SIZE_K\": 32,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 64,\n",
    "                \"BLOCK_SIZE_D1\": 32,\n",
    "                \"BLOCK_SIZE_K\": 32,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=5,\n",
    "            num_warps=2,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 32,\n",
    "                \"BLOCK_SIZE_D1\": 32,\n",
    "                \"BLOCK_SIZE_K\": 64,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=5,\n",
    "            num_warps=2,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_D1\": 128,\n",
    "                \"BLOCK_SIZE_K\": 256,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=3,\n",
    "            num_warps=8,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 256,\n",
    "                \"BLOCK_SIZE_D1\": 128,\n",
    "                \"BLOCK_SIZE_K\": 128,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=3,\n",
    "            num_warps=8,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 256,\n",
    "                \"BLOCK_SIZE_D1\": 128,\n",
    "                \"BLOCK_SIZE_K\": 64,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 64,\n",
    "                \"BLOCK_SIZE_D1\": 128,\n",
    "                \"BLOCK_SIZE_K\": 256,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_D1\": 128,\n",
    "                \"BLOCK_SIZE_K\": 128,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_D1\": 64,\n",
    "                \"BLOCK_SIZE_K\": 64,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 64,\n",
    "                \"BLOCK_SIZE_D1\": 64,\n",
    "                \"BLOCK_SIZE_K\": 128,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_BSIZE\": 128,\n",
    "                \"BLOCK_SIZE_D1\": 64,\n",
    "                \"BLOCK_SIZE_K\": 32,\n",
    "                \"GROUP_SIZE_BSIZE\": 8,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "    ],\n",
    "    key=[\"BSIZE\", \"K\", \"d2\", \"L\"],\n",
    ")\n",
    "@triton.jit\n",
    "def second_pass_kernel(\n",
    "    in1_ptr,\n",
    "    in2_ptr,\n",
    "    U1s_ptr,\n",
    "    S2s_ptr,\n",
    "    bias_ptr,\n",
    "    out_ptr,\n",
    "    BSIZE,\n",
    "    d1,\n",
    "    K,\n",
    "    L,\n",
    "    stride_in12_l,\n",
    "    stride_in12_bsize,\n",
    "    stride_in12_k,\n",
    "    stride_us_l,\n",
    "    stride_us_k,\n",
    "    stride_us_d1,\n",
    "    stride_bias_bsize,\n",
    "    stride_bias_d1,\n",
    "    stride_out_bsize,\n",
    "    stride_out_d1,\n",
    "    BLOCK_SIZE_BSIZE: tl.constexpr,\n",
    "    BLOCK_SIZE_D1: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_BSIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n",
    "    num_pid_d1 = tl.cdiv(d1, BLOCK_SIZE_D1)\n",
    "    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_d1\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n",
    "    GROUP_SIZE_BSIZE = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n",
    "    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % GROUP_SIZE_BSIZE)\n",
    "    pid_d1 = (pid % num_pid_in_group) // GROUP_SIZE_BSIZE\n",
    "\n",
    "    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n",
    "    offs_d1 = pid_d1 * BLOCK_SIZE_D1 + tl.arange(0, BLOCK_SIZE_D1)\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "\n",
    "    offs_bsize = tl.max_contiguous(\n",
    "        tl.multiple_of(offs_bsize, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE\n",
    "    )\n",
    "    offs_d1 = tl.max_contiguous(tl.multiple_of(offs_d1, BLOCK_SIZE_D1), BLOCK_SIZE_D1)\n",
    "    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_K), BLOCK_SIZE_K)\n",
    "\n",
    "    in_tmp = offs_bsize[:, None] * stride_in12_bsize + offs_k[None, :] * stride_in12_k\n",
    "    us_tmp = offs_k[:, None] * stride_us_k + offs_d1[None, :] * stride_us_d1\n",
    "\n",
    "    accumulator = tl.full(\n",
    "        shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_D1), value=0.0, dtype=tl.float32\n",
    "    )\n",
    "\n",
    "    for l in range(0, L):\n",
    "        l_tmp_stride = l * stride_in12_l\n",
    "\n",
    "        in1_ptrs = l_tmp_stride + in1_ptr + in_tmp\n",
    "        in2_ptrs = l_tmp_stride + in2_ptr + in_tmp\n",
    "\n",
    "        U1s_ptrs = l_tmp_stride + U1s_ptr + us_tmp\n",
    "        S2s_ptrs = l_tmp_stride + S2s_ptr + us_tmp\n",
    "\n",
    "        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "            in_mask = offs_k[None, :] < K - k * BLOCK_SIZE_K\n",
    "            in1 = tl.load(in1_ptrs, mask=in_mask, other=0.0)\n",
    "            in2 = tl.load(in2_ptrs, mask=in_mask, other=0.0)\n",
    "\n",
    "            us_mask = offs_k[:, None] < K - k * BLOCK_SIZE_K\n",
    "            U1s = tl.load(U1s_ptrs, mask=us_mask, other=0.0)\n",
    "            S2s = tl.load(S2s_ptrs, mask=us_mask, other=0.0)\n",
    "\n",
    "            accumulator += tl.dot(in1, U1s)\n",
    "            accumulator += tl.dot(in2, S2s)\n",
    "\n",
    "            in_inc = BLOCK_SIZE_K * stride_in12_k\n",
    "            in1_ptrs += in_inc\n",
    "            in2_ptrs += in_inc\n",
    "\n",
    "            us_inc = BLOCK_SIZE_K * stride_us_k\n",
    "            U1s_ptrs += us_inc\n",
    "            S2s_ptrs += us_inc\n",
    "\n",
    "    accumulator = accumulator.to(tl.float16)\n",
    "\n",
    "    bias_ptrs = bias_ptr + offs_d1[None, :] * stride_bias_d1\n",
    "    bias_mask = offs_d1[None, :] < d1\n",
    "    bias = tl.load(bias_ptrs, mask=bias_mask, other=0.0)\n",
    "\n",
    "    accumulator *= 1.0 / (2.0 * L)\n",
    "    accumulator += bias\n",
    "\n",
    "    out_ptrs = (\n",
    "        out_ptr\n",
    "        + stride_out_bsize * offs_bsize[:, None]\n",
    "        + stride_out_d1 * offs_d1[None, :]\n",
    "    )\n",
    "    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_d1[None, :] < d1)\n",
    "\n",
    "    tl.store(out_ptrs, accumulator, mask=out_mask)\n",
    "\n",
    "\n",
    "def second_pass(in1, in2, U1s, S2s, bias):\n",
    "    assert in1.shape[2] == U1s.shape[1], \"Incompatible dimensions\"\n",
    "    assert in2.shape[2] == S2s.shape[1], \"Incompatible dimensions\"\n",
    "    assert in1.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    assert in2.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    assert U1s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    assert S2s.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    assert bias.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    assert U1s.stride() == S2s.stride(), \"Matrix A must be contiguous\"\n",
    "    assert in1.stride() == in2.stride(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    L, BSIZE, K = in1.shape\n",
    "    _, _, d1 = U1s.shape\n",
    "\n",
    "    out = torch.empty((BSIZE, d1), dtype=torch.float16, device=device)\n",
    "\n",
    "    stride_in12_l, stride_in12_bsize, stride_in12_k = in1.stride()\n",
    "    stride_us_l, stride_us_k, stride_us_d1 = U1s.stride()\n",
    "    stride_bias_bsize, stride_bias_d1 = bias.stride()\n",
    "    stride_out_bsize, stride_out_d1 = out.stride()\n",
    "\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"])\n",
    "        * triton.cdiv(d1, META[\"BLOCK_SIZE_D1\"]),\n",
    "    )\n",
    "\n",
    "    second_pass_kernel[grid](\n",
    "        in1,\n",
    "        in2,\n",
    "        U1s,\n",
    "        S2s,\n",
    "        bias,\n",
    "        out,\n",
    "        BSIZE,\n",
    "        d1,\n",
    "        K,\n",
    "        L,\n",
    "        stride_in12_l,\n",
    "        stride_in12_bsize,\n",
    "        stride_in12_k,\n",
    "        stride_us_l,\n",
    "        stride_us_k,\n",
    "        stride_us_d1,\n",
    "        stride_bias_bsize,\n",
    "        stride_bias_d1,\n",
    "        stride_out_bsize,\n",
    "        stride_out_d1,\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "########################################################\n",
    "device = \"cuda\"\n",
    "scale = 1\n",
    "in1 = out1 * scale  # in1 -> l * bsize * k\n",
    "in2 = out2 * scale  # in2 -> l * bsize * k\n",
    "\n",
    "d1 = 128\n",
    "bias = torch.randn((1, d1), dtype=torch.float16, device=device) * scale\n",
    "\n",
    "U1s = torch.randn((L, K, d1), dtype=torch.float16, device=device) * scale\n",
    "S2s = torch.randn((L, K, d1), dtype=torch.float16, device=device) * scale\n",
    "\n",
    "triton_output = second_pass(in1, in2, U1s, S2s, bias)\n",
    "torch_output = (in1.bmm(U1s).mean(0) / 2) + (in2.bmm(S2s).mean(0) / 2) + bias\n",
    "\n",
    "rtol = 10e-1\n",
    "if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=rtol):\n",
    "    print(\"✅ Triton and Torch match\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T19:13:46.742228Z",
     "iopub.status.busy": "2025-03-19T19:13:46.741907Z",
     "iopub.status.idle": "2025-03-19T19:13:46.746383Z",
     "shell.execute_reply": "2025-03-19T19:13:46.745511Z",
     "shell.execute_reply.started": "2025-03-19T19:13:46.742200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def forward(hin, S1s, U2s, bias, U1s, S2s):\n",
    "    in1, in2 = first_pass(hin, S1s, U2s)\n",
    "    triton_output = second_pass(in1, in2, U1s, S2s, bias)\n",
    "    return triton_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T19:13:48.466014Z",
     "iopub.status.busy": "2025-03-19T19:13:48.465743Z",
     "iopub.status.idle": "2025-03-19T19:13:48.476135Z",
     "shell.execute_reply": "2025-03-19T19:13:48.475488Z",
     "shell.execute_reply.started": "2025-03-19T19:13:48.465993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Torch match 1\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "L = 4\n",
    "BSIZE, d2 = 128, 16\n",
    "K = 256\n",
    "d1 = 128\n",
    "\n",
    "hin = torch.randn((BSIZE, d2), dtype=torch.float16, device=device)\n",
    "S1s = torch.randn((L, d2, K), dtype=torch.float16, device=device)\n",
    "S2s = torch.randn((L, K, d1), dtype=torch.float16, device=device)\n",
    "U1s = torch.randn((L, K, d1), dtype=torch.float16, device=device)\n",
    "U2s = torch.randn((L, d2, K), dtype=torch.float16, device=device)\n",
    "bias = torch.randn((1, d1), dtype=torch.float16, device=device)\n",
    "\n",
    "triton_out = forward(hin, S1s, U2s, bias, U1s, S2s)\n",
    "\n",
    "inputt = hin.unsqueeze(0).expand(L, hin.shape[0], hin.shape[1])\n",
    "torch_output = (\n",
    "    ((inputt.bmm(S1s)).bmm(U1s)).mean(0) / 2\n",
    "    + ((inputt.bmm(U2s)).bmm(S2s)).mean(0) / 2\n",
    "    + bias\n",
    ")\n",
    "\n",
    "rtol = 10e-1\n",
    "if torch.allclose(triton_out, torch_output, atol=1e-2, rtol=rtol):\n",
    "    print(\"✅ Triton and Torch match 1\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch differ 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T19:14:51.515737Z",
     "iopub.status.busy": "2025-03-19T19:14:51.515455Z",
     "iopub.status.idle": "2025-03-19T19:14:51.881107Z",
     "shell.execute_reply": "2025-03-19T19:14:51.880437Z",
     "shell.execute_reply.started": "2025-03-19T19:14:51.515715Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton and Torch match: False\n",
      "Triton and Compiled Torch match: False\n",
      "Triton and Compiled Triton match: True\n",
      "Warming up...\n",
      "Running benchmarks...\n",
      "\n",
      "Triton implementation: <torch.utils.benchmark.utils.common.Measurement object at 0x785214f3acb0>\n",
      "forward(hin, S1s, U2s, bias, U1s, S2s)\n",
      "  218.84 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "PyTorch implementation: <torch.utils.benchmark.utils.common.Measurement object at 0x785214f3b280>\n",
      "forward_torch(hin, S1s, U2s, bias, U1s, S2s)\n",
      "  120.91 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "Compiled PyTorch implementation: <torch.utils.benchmark.utils.common.Measurement object at 0x785232794910>\n",
      "forward_torch_compiled(hin, S1s, U2s, bias, U1s, S2s)\n",
      "  143.11 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "Compiled Triton implementation: <torch.utils.benchmark.utils.common.Measurement object at 0x7851f4e7a290>\n",
      "forward_compiled(hin, S1s, U2s, bias, U1s, S2s)\n",
      "  93.41 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "\n",
      "Speedup of Triton vs PyTorch: 0.55x\n",
      "Speedup of Triton vs Compiled PyTorch: 0.65x\n",
      "Speedup of Compiled PyTorch vs PyTorch: 0.84x\n",
      "Speedup of Compiled Triton vs Triton: 2.34x\n",
      "Speedup of Compiled Triton vs Compiled PyTorch: 1.53x\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.benchmark as benchmark\n",
    "import time\n",
    "\n",
    "\n",
    "def forward_torch(hin, S1s, U2s, bias, U1s, S2s):\n",
    "    L = S1s.shape[0]\n",
    "    inputt = hin.unsqueeze(0).expand(L, hin.shape[0], hin.shape[1])\n",
    "    return (\n",
    "        ((inputt.bmm(S1s)).bmm(U1s)).mean(0) / 2\n",
    "        + ((inputt.bmm(U2s)).bmm(S2s)).mean(0) / 2\n",
    "        + bias\n",
    "    )\n",
    "\n",
    "\n",
    "# Compile the PyTorch function\n",
    "forward_torch_compiled = torch.compile(forward_torch)\n",
    "\n",
    "# Compile the original forward function with full graph mode\n",
    "forward_compiled = torch.compile(forward, fullgraph=True)\n",
    "\n",
    "\n",
    "def benchmark_functions(hin, S1s, U2s, bias, U1s, S2s, num_warmup=10, num_runs=100):\n",
    "    # Move all tensors to CUDA\n",
    "    hin = hin.cuda()\n",
    "    S1s = S1s.cuda()\n",
    "    S2s = S2s.cuda()\n",
    "    U1s = U1s.cuda()\n",
    "    U2s = U2s.cuda()\n",
    "    bias = bias.cuda()\n",
    "\n",
    "    # Verify results match\n",
    "    triton_out = forward(hin, S1s, U2s, bias, U1s, S2s)\n",
    "    torch_out = forward_torch(hin, S1s, U2s, bias, U1s, S2s)\n",
    "    compiled_torch_out = forward_torch_compiled(hin, S1s, U2s, bias, U1s, S2s)\n",
    "    compiled_triton_out = forward_compiled(hin, S1s, U2s, bias, U1s, S2s)\n",
    "\n",
    "    print(\n",
    "        f\"Triton and Torch match: {torch.allclose(triton_out, torch_out, atol=1e-2, rtol=1e-2)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Triton and Compiled Torch match: {torch.allclose(triton_out, compiled_torch_out, atol=1e-2, rtol=1e-2)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Triton and Compiled Triton match: {torch.allclose(triton_out, compiled_triton_out, atol=1e-2, rtol=1e-2)}\"\n",
    "    )\n",
    "\n",
    "    # Warmup\n",
    "    print(\"Warming up...\")\n",
    "    for _ in range(num_warmup):\n",
    "        _ = forward(hin, S1s, U2s, bias, U1s, S2s)\n",
    "        _ = forward_torch(hin, S1s, U2s, bias, U1s, S2s)\n",
    "        _ = forward_torch_compiled(hin, S1s, U2s, bias, U1s, S2s)\n",
    "        _ = forward_compiled(hin, S1s, U2s, bias, U1s, S2s)\n",
    "\n",
    "    # Synchronize before timing\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Benchmark using torch.utils.benchmark\n",
    "    timer_triton = benchmark.Timer(\n",
    "        stmt=\"forward(hin, S1s, U2s, bias, U1s, S2s)\",\n",
    "        globals={\n",
    "            \"forward\": forward,\n",
    "            \"hin\": hin,\n",
    "            \"S1s\": S1s,\n",
    "            \"U2s\": U2s,\n",
    "            \"bias\": bias,\n",
    "            \"U1s\": U1s,\n",
    "            \"S2s\": S2s,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    timer_torch = benchmark.Timer(\n",
    "        stmt=\"forward_torch(hin, S1s, U2s, bias, U1s, S2s)\",\n",
    "        globals={\n",
    "            \"forward_torch\": forward_torch,\n",
    "            \"hin\": hin,\n",
    "            \"S1s\": S1s,\n",
    "            \"U2s\": U2s,\n",
    "            \"bias\": bias,\n",
    "            \"U1s\": U1s,\n",
    "            \"S2s\": S2s,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    timer_compiled_torch = benchmark.Timer(\n",
    "        stmt=\"forward_torch_compiled(hin, S1s, U2s, bias, U1s, S2s)\",\n",
    "        globals={\n",
    "            \"forward_torch_compiled\": forward_torch_compiled,\n",
    "            \"hin\": hin,\n",
    "            \"S1s\": S1s,\n",
    "            \"U2s\": U2s,\n",
    "            \"bias\": bias,\n",
    "            \"U1s\": U1s,\n",
    "            \"S2s\": S2s,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    timer_compiled_triton = benchmark.Timer(\n",
    "        stmt=\"forward_compiled(hin, S1s, U2s, bias, U1s, S2s)\",\n",
    "        globals={\n",
    "            \"forward_compiled\": forward_compiled,\n",
    "            \"hin\": hin,\n",
    "            \"S1s\": S1s,\n",
    "            \"U2s\": U2s,\n",
    "            \"bias\": bias,\n",
    "            \"U1s\": U1s,\n",
    "            \"S2s\": S2s,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Run benchmarks\n",
    "    print(\"Running benchmarks...\")\n",
    "    result_triton = timer_triton.timeit(num_runs)\n",
    "    result_torch = timer_torch.timeit(num_runs)\n",
    "    result_compiled_torch = timer_compiled_torch.timeit(num_runs)\n",
    "    result_compiled_triton = timer_compiled_triton.timeit(num_runs)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nTriton implementation: {result_triton}\")\n",
    "    print(f\"PyTorch implementation: {result_torch}\")\n",
    "    print(f\"Compiled PyTorch implementation: {result_compiled_torch}\")\n",
    "    print(f\"Compiled Triton implementation: {result_compiled_triton}\")\n",
    "\n",
    "    # Calculate speedups\n",
    "    triton_time = result_triton.mean\n",
    "    torch_time = result_torch.mean\n",
    "    compiled_torch_time = result_compiled_torch.mean\n",
    "    compiled_triton_time = result_compiled_triton.mean\n",
    "\n",
    "    print(f\"\\nSpeedup of Triton vs PyTorch: {torch_time / triton_time:.2f}x\")\n",
    "    print(\n",
    "        f\"Speedup of Triton vs Compiled PyTorch: {compiled_torch_time / triton_time:.2f}x\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Speedup of Compiled PyTorch vs PyTorch: {torch_time / compiled_torch_time:.2f}x\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Speedup of Compiled Triton vs Triton: {triton_time / compiled_triton_time:.2f}x\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Speedup of Compiled Triton vs Compiled PyTorch: {compiled_torch_time / compiled_triton_time:.2f}x\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Create test data\n",
    "    L = 4\n",
    "    BSIZE, d2 = 128, 16\n",
    "    K = 256\n",
    "    d1 = 128\n",
    "    hin = torch.randn((BSIZE, d2), dtype=torch.float16)\n",
    "    S1s = torch.randn((L, d2, K), dtype=torch.float16)\n",
    "    S2s = torch.randn((L, K, d1), dtype=torch.float16)\n",
    "    U1s = torch.randn((L, K, d1), dtype=torch.float16)\n",
    "    U2s = torch.randn((L, d2, K), dtype=torch.float16)\n",
    "    bias = torch.randn((1, d1), dtype=torch.float16)\n",
    "\n",
    "    # Run benchmarks\n",
    "    benchmark_functions(hin, S1s, U2s, bias, U1s, S2s)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
