{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10, 128])\n",
      "torch.Size([16, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "seq_length = 10\n",
    "batch_size = 16\n",
    "mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "q = torch.rand(batch_size, seq_length, embed_dim)\n",
    "k = torch.rand(batch_size, seq_length, embed_dim)\n",
    "v = torch.rand(batch_size, seq_length, embed_dim)\n",
    "output, attn_output_weights = mha(q, k, v)\n",
    "print(output.shape)\n",
    "print(attn_output_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "from panther.nn import Performers\n",
    "\n",
    "\n",
    "def create_projection_matrix(m, d, seed=42, scaling=False) -> torch.Tensor:\n",
    "    torch.manual_seed(seed)  # Set seed for reproducibility\n",
    "\n",
    "    nb_full_blocks = m // d  # Number of complete dxd blocks\n",
    "    block_list = []  # List to store blocks\n",
    "    # does QR on steps which is more effcient and numerically stable and guarantees orthogonality\n",
    "    for _ in range(nb_full_blocks):\n",
    "        # Generate a random dxd Gaussian matrix\n",
    "        unstructured_block = torch.randn(d, d)\n",
    "        # Perform QR decomposition to obtain an orthonormal matrix\n",
    "        q, _ = torch.linalg.qr(unstructured_block)\n",
    "        q = q.T  # Transpose so rows are the orthonormal vectors\n",
    "\n",
    "        block_list.append(q)  # Store the block\n",
    "\n",
    "    remaining_rows = m - nb_full_blocks * d  # Compute remaining rows\n",
    "\n",
    "    if remaining_rows > 0:\n",
    "        # Handle the last incomplete block\n",
    "        unstructured_block = torch.randn(d, d)\n",
    "        q, _ = torch.linalg.qr(unstructured_block)\n",
    "        q = q.T  # Transpose\n",
    "        block_list.append(q[:remaining_rows])  # Take only required rows\n",
    "\n",
    "    # Stack all blocks to form the final projection matrix\n",
    "    final_matrix = torch.vstack(block_list)\n",
    "\n",
    "    if scaling:\n",
    "        # If scaling is enabled, normalize rows to sqrt(d)\n",
    "        multiplier = torch.full((m,), torch.sqrt(torch.tensor(d)))\n",
    "    else:\n",
    "        # Otherwise, scale each row using chi(d) distribution\n",
    "        multiplier = torch.norm(torch.randn(m, d), dim=1)\n",
    "\n",
    "    # Apply scaling by multiplying with diagonal matrix\n",
    "    return torch.matmul(torch.diag(multiplier), final_matrix)\n",
    "\n",
    "\n",
    "def test_softmax_noncausal_attention_block_output_shape():\n",
    "    embed_dim = 128\n",
    "    num_heads = 8\n",
    "    seq_length = 10\n",
    "    batch_size = 16\n",
    "    num_random_features = 350\n",
    "    q = torch.rand(batch_size, seq_length, embed_dim)\n",
    "    k = torch.rand(batch_size, seq_length, embed_dim)\n",
    "    v = torch.rand(batch_size, seq_length, embed_dim)\n",
    "    mha = Performers(\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        num_random_features=num_random_features,\n",
    "    )\n",
    "    attention_block_output = mha(q, k, v)\n",
    "    print(attention_block_output.shape)\n",
    "\n",
    "\n",
    "test_softmax_noncausal_attention_block_output_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.03648376464844\n",
      "-1105.4796142578125\n",
      "-2349.63623046875\n",
      "-3687.888916015625\n",
      "-5174.4677734375\n",
      "-6857.13916015625\n",
      "-8774.76953125\n",
      "-10958.888671875\n",
      "-13435.8193359375\n",
      "-16228.6337890625\n",
      "-19358.806640625\n",
      "-22847.16796875\n",
      "-26713.87109375\n",
      "-30978.18359375\n",
      "-35658.4921875\n",
      "-40772.578125\n",
      "-46337.96484375\n",
      "-52372.1484375\n",
      "-58892.609375\n",
      "-65916.9375\n",
      "-73462.859375\n",
      "-81548.609375\n",
      "-90192.9296875\n",
      "-99414.609375\n",
      "-109232.0703125\n",
      "-119663.390625\n",
      "-130727.1015625\n",
      "-142442.296875\n",
      "-154828.515625\n",
      "-167904.875\n",
      "-181689.4375\n",
      "-196199.0625\n",
      "-211449.484375\n",
      "-227455.890625\n",
      "-244233.90625\n",
      "-261800.28125\n",
      "-280171.65625\n",
      "-299361.90625\n",
      "-319383.5\n",
      "-340250.84375\n",
      "-361981.5\n",
      "-384591.125\n",
      "-408089.34375\n",
      "-432482.46875\n",
      "-457778.84375\n",
      "-483989.625\n",
      "-511124.59375\n",
      "-539192.125\n",
      "-568201.5\n",
      "-598160.375\n",
      "-629073.625\n",
      "-660947.75\n",
      "-693790.5\n",
      "-727608.875\n",
      "-762406.3125\n",
      "-798188.6875\n",
      "-834964.25\n",
      "-872740.8125\n",
      "-911524.25\n",
      "-951319.5625\n",
      "-992134.8125\n",
      "-1033974.1875\n",
      "-1076840.875\n",
      "-1120741.875\n",
      "-1165682.75\n",
      "-1211664.75\n",
      "-1258690.875\n",
      "-1306765.75\n",
      "-1355889.75\n",
      "-1406056.375\n",
      "-1457260.25\n",
      "-1509495.125\n",
      "-1562761.75\n",
      "-1617058.375\n",
      "-1672390.125\n",
      "-1728761.5\n",
      "-1786179.25\n",
      "-1844643.875\n",
      "-1904155.0\n",
      "-1964721.75\n",
      "-2026345.625\n",
      "-2089025.875\n",
      "-2152767.0\n",
      "-2217579.0\n",
      "-2283466.0\n",
      "-2350427.0\n",
      "-2418457.0\n",
      "-2487544.5\n",
      "-2557681.5\n",
      "-2628865.5\n",
      "-2701096.5\n",
      "-2774368.0\n",
      "-2848675.5\n",
      "-2924011.75\n",
      "-3000382.0\n",
      "-3077786.75\n",
      "-3156224.0\n",
      "-3235707.5\n",
      "-3316232.25\n",
      "-3397805.75\n"
     ]
    }
   ],
   "source": [
    "def test_backprop():  # test backprop and optimizer\n",
    "    embed_dim = 128\n",
    "    num_heads = 8\n",
    "    seq_length = 10\n",
    "    batch_size = 16\n",
    "    num_random_features = 350\n",
    "    q = torch.rand(batch_size, seq_length, embed_dim)\n",
    "    k = torch.rand(batch_size, seq_length, embed_dim)\n",
    "    v = torch.rand(batch_size, seq_length, embed_dim)\n",
    "    mha = Performers(\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        num_random_features=num_random_features,\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(mha.parameters(), lr=0.001)\n",
    "    for i in range(100):\n",
    "        attention_block_output = mha(q, k, v)\n",
    "        loss = torch.sum(attention_block_output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(loss.item())\n",
    "\n",
    "\n",
    "test_backprop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panther--D9mjH-B-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
