{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1\n# !pip uninstall -y pytorch-triton\n\n# !pip install -U --pre triton --index-url https://download.pytorch.org/whl/nightly/cu121\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade \\\n  torch torchvision torchaudio \\\n  --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T22:24:22.865469Z","iopub.execute_input":"2025-03-23T22:24:22.865827Z","iopub.status.idle":"2025-03-23T22:26:56.271439Z","shell.execute_reply.started":"2025-03-23T22:24:22.865793Z","shell.execute_reply":"2025-03-23T22:26:56.270233Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting torch\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl.metadata (27 kB)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp310-cp310-linux_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nCollecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m27.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting triton==3.2.0 (from torch)\n  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl (848.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp310-cp310-linux_x86_64.whl (6.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchaudio, torchvision\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.5.1+cu121\n    Uninstalling torchaudio-2.5.1+cu121:\n      Successfully uninstalled torchaudio-2.5.1+cu121\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu121\n    Uninstalling torchvision-0.20.1+cu121:\n      Successfully uninstalled torchvision-0.20.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118 torchaudio-2.6.0+cu118 torchvision-0.21.0+cu118 triton-3.2.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# import os\n# os.environ[\"TRITON_INTERPRET\"] = \"1\"\n# print(os.environ[\"TRITON_INTERPRET\"], \"\\n\")\n\nimport torch\nprint(torch.__version__)\nimport triton\nimport triton.language as tl\nprint(triton.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T22:26:56.272915Z","iopub.execute_input":"2025-03-23T22:26:56.273226Z","iopub.status.idle":"2025-03-23T22:26:58.644973Z","shell.execute_reply.started":"2025-03-23T22:26:56.273199Z","shell.execute_reply":"2025-03-23T22:26:58.643963Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu118\n3.2.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!export LC_ALL=\"en_US.UTF-8\"\n!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n!ldconfig /usr/lib64-nvidia","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T21:59:15.207787Z","iopub.execute_input":"2025-03-23T21:59:15.208101Z","iopub.status.idle":"2025-03-23T21:59:15.716533Z","shell.execute_reply.started":"2025-03-23T21:59:15.208079Z","shell.execute_reply":"2025-03-23T21:59:15.715513Z"}},"outputs":[{"name":"stdout","text":"/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"stable one","metadata":{}},{"cell_type":"code","source":"import time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T22:27:04.658771Z","iopub.execute_input":"2025-03-23T22:27:04.659218Z","iopub.status.idle":"2025-03-23T22:27:04.663322Z","shell.execute_reply.started":"2025-03-23T22:27:04.659191Z","shell.execute_reply":"2025-03-23T22:27:04.662236Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"###  grad = g.bmm(U1s).bmm(S1s).sum(0) + g.bmm(S2s).bmm(U2s).sum(0)","metadata":{}},{"cell_type":"code","source":"@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=1,\n                      num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5,\n                       num_warps=2),\n         triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5,\n                       num_warps=2),\n         triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3,\n                       num_warps=8),\n         triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3,\n                       num_warps=8),\n         triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4)\n    ],\n    key=['BSIZE', 'K', 'd1', 'L'],\n)\n@triton.jit\ndef first_pass_gU1s_g_S2s_kernel(\n        g_ptr, U1s_ptr, S2s_ptr, g_U1s_ptr, g_S2s_ptr,\n        BSIZE, K, d1, L,\n        stride_g_bsize, stride_g_d1,\n        stride_su_l, stride_su_d1, stride_su_k,\n        stride_out_l, stride_out_bsize, stride_out_k,\n        BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_d1: tl.constexpr,\n        GROUP_SIZE_BSIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=1)\n    batch_id = tl.program_id(axis=0)\n    \n    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_k\n    group_id = pid // num_pid_in_group\n    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n    pid_k = (pid % num_pid_in_group) // group_size_bsize\n\n    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n    offs_k = pid_k *  BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_d1 = tl.arange(0, BLOCK_SIZE_d1)\n\n    g_ptrs = g_ptr + (offs_bsize[:, None] * stride_g_bsize + offs_d1[None, :] * stride_g_d1)\n\n    su_tmp = batch_id * stride_su_l + (offs_d1[:, None] * stride_su_d1 + offs_k[None, :] * stride_su_k)\n    U1s_ptrs = U1s_ptr + su_tmp\n    S2s_ptrs = S2s_ptr + su_tmp\n\n    accumulator1 = tl.full(shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), value=0.0, dtype=tl.float32)\n    accumulator2 = tl.full(shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_K), value=0.0, dtype=tl.float32)\n    \n    \n    for d1_i in range(0, tl.cdiv(d1, BLOCK_SIZE_d1)):\n        g = tl.load(g_ptrs, mask=(offs_d1[None, :] < d1 - d1_i * BLOCK_SIZE_d1), other=0.0)\n        \n        su_mask = (offs_d1[:, None] < d1 - d1_i * BLOCK_SIZE_d1)\n        U1s = tl.load(U1s_ptrs, mask=su_mask, other=0.0)\n        S2s = tl.load(S2s_ptrs, mask=su_mask, other=0.0)\n        \n        accumulator1 += tl.dot(g, U1s)\n        accumulator2 += tl.dot(g, S2s)\n        \n        g_ptrs += BLOCK_SIZE_d1 * stride_g_d1\n        U1s_ptrs += BLOCK_SIZE_d1 * stride_su_d1\n        S2s_ptrs += BLOCK_SIZE_d1 * stride_su_d1\n\n    out_tmp = batch_id * stride_out_l + stride_out_bsize * offs_bsize[:, None] + stride_out_k * offs_k[None, :]\n    g_U1s_ptrs = g_U1s_ptr + out_tmp\n    g_S2s_ptrs = g_S2s_ptr + out_tmp\n    \n    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_k[None, :] < K)\n    \n    tl.store(g_U1s_ptrs, accumulator1, mask=out_mask)\n    tl.store(g_S2s_ptrs, accumulator2, mask=out_mask)\n\ndef first_pass_gU1s_g_S2s(g, U1s, S2s):\n    assert g.shape[1] == U1s.shape[1], \"Incompatible dimensions\"\n    assert g.shape[1] == S2s.shape[1], \"Incompatible dimensions\"\n    assert g.is_contiguous(), \"Matrix A must be contiguous\"\n    assert U1s.is_contiguous(), \"Matrix A must be contiguous\"\n    assert S2s.is_contiguous(), \"Matrix A must be contiguous\"\n    assert U1s.stride() == S2s.stride(), \"Matrix A must be contiguous\"\n    \n    BSIZE, d1 = g.shape\n    L, _, K = U1s.shape\n    \n    g_U1s = torch.empty((L, BSIZE, K), dtype=torch.float16, device='cuda')\n    g_S2s = torch.empty((L, BSIZE, K), dtype=torch.float16, device='cuda')\n\n    stride_g_bsize, stride_g_d1 = g.stride()\n    stride_su_l, stride_su_d1, stride_su_k = U1s.stride()\n    stride_out_l, stride_out_bsize, stride_out_k = g_U1s.stride()\n    \n    assert g_U1s.stride() == g_S2s.stride(), \"Matrix A must be contiguous\"\n    \n    grid = lambda META: (L, triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]) * triton.cdiv(K, META[\"BLOCK_SIZE_K\"]), )\n    \n    first_pass_gU1s_g_S2s_kernel[grid](\n        g, U1s, S2s, g_U1s, g_S2s,\n        BSIZE, K, d1, L,\n        stride_g_bsize, stride_g_d1,\n        stride_su_l, stride_su_d1, stride_su_k,\n        stride_out_l, stride_out_bsize, stride_out_k\n    )\n    \n    return g_U1s, g_S2s\n\nL = 3\nBSIZE, d1 = 128, 2048\nK = 32\n\ndevice='cuda'\n\ntorch.manual_seed(0)\ng = torch.randn((BSIZE, d1), dtype=torch.float16, device=device)\nU1s = torch.randn((L, d1, K), dtype=torch.float16, device=device)\nS2s = torch.randn((L, d1, K), dtype=torch.float16, device=device)\n\nstart = time.perf_counter()\ng_U1s, g_S2s = first_pass_gU1s_g_S2s(g, U1s, S2s)\nend = time.perf_counter()\n\nprint(f\"time {end - start}\")\n\ntorch_output1 = (g.unsqueeze(0).expand(L, BSIZE, d1)).bmm(U1s)\ntorch_output2 = (g.unsqueeze(0).expand(L, BSIZE, d1)).bmm(S2s)\n\nrtol = 1e-2\nif torch.allclose(g_U1s, torch_output1, atol=1e-2, rtol=rtol):\n    print(\"✅ Triton and Torch match 1\")\nelse:\n    print(\"❌ Triton and Torch differ 1\")\n\nif torch.allclose(g_S2s, torch_output2, atol=1e-2, rtol=rtol):\n    print(\"✅ Triton and Torch match 2\")\nelse:\n    print(\"❌ Triton and Torch differ 2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T22:45:09.441523Z","iopub.execute_input":"2025-03-23T22:45:09.441927Z","iopub.status.idle":"2025-03-23T22:45:10.628635Z","shell.execute_reply.started":"2025-03-23T22:45:09.441901Z","shell.execute_reply":"2025-03-23T22:45:10.627571Z"}},"outputs":[{"name":"stdout","text":"time 1.1524532259995794\n✅ Triton and Torch match 1\n✅ Triton and Torch match 2\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=1,\n                      num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5,\n                       num_warps=2),\n         triton.Config({'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_BSIZE': 8}, num_stages=5,\n                       num_warps=2),\n         triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3,\n                       num_warps=8),\n         triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=3,\n                       num_warps=8),\n         triton.Config({'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4),\n         triton.Config({'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_BSIZE': 8}, num_stages=4,\n                       num_warps=4)\n    ],\n    key=['BSIZE', 'd2', 'K', 'L'],\n)\n@triton.jit\ndef second_pass_gUS11_22_kernel(\n        g_U1s_ptr, g_S2s_ptr, S1s_ptr, U2s_ptr, out_ptr,\n        BSIZE, d2, K, L,\n        stride_g_U1s2_l, stride_g_U1s2_bsize, stride_g_U1s2_k,\n        stride_us_l, stride_us_k, stride_us_d2,\n        stride_out_bsize, stride_out_d2,\n        BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_d2: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n        GROUP_SIZE_BSIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    \n    num_pid_bsize = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n    num_pid_d2 = tl.cdiv(d2, BLOCK_SIZE_d2)\n    num_pid_in_group = GROUP_SIZE_BSIZE * num_pid_d2\n    group_id = pid // num_pid_in_group\n    first_pid_bsize = group_id * GROUP_SIZE_BSIZE\n    GROUP_SIZE_BSIZE = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_BSIZE)\n    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % GROUP_SIZE_BSIZE)\n    pid_d2 = (pid % num_pid_in_group) // GROUP_SIZE_BSIZE\n\n    offs_bsize = pid_bsize * BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n    offs_d2 = pid_d2 *  BLOCK_SIZE_d2 + tl.arange(0, BLOCK_SIZE_d2)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    in_tmp = offs_bsize[:, None] * stride_g_U1s2_bsize + offs_k[None, :] * stride_g_U1s2_k\n    us_tmp = offs_k[:, None] * stride_us_k + offs_d2[None, :] * stride_us_d2\n\n    accumulator = tl.full(shape=(BLOCK_SIZE_BSIZE, BLOCK_SIZE_d2), value=0.0, dtype=tl.float32)\n    \n    for l in range(0, L):\n        l_tmp_stride = l * stride_g_U1s2_l\n        \n        g_U1s_ptrs = l_tmp_stride + g_U1s_ptr + in_tmp\n        g_S2s_ptrs = l_tmp_stride + g_S2s_ptr + in_tmp\n\n        S1s_ptrs = l_tmp_stride + S1s_ptr + us_tmp\n        U2s_ptrs = l_tmp_stride + U2s_ptr + us_tmp\n        \n        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n            in_mask = offs_k[None, :] < K - k * BLOCK_SIZE_K\n            g_U1s = tl.load(g_U1s_ptrs, mask=in_mask, other=0.0)\n            g_S2s = tl.load(g_S2s_ptrs, mask=in_mask, other=0.0)\n            \n            us_mask = offs_k[:, None] < K - k * BLOCK_SIZE_K\n            S1s = tl.load(S1s_ptrs, mask=us_mask, other=0.0)\n            U2s = tl.load(U2s_ptrs, mask=us_mask, other=0.0)\n            \n            accumulator += tl.dot(g_U1s, S1s)\n            accumulator += tl.dot(g_S2s, U2s)\n\n            in_inc = BLOCK_SIZE_K * stride_g_U1s2_k\n            g_U1s_ptrs += in_inc\n            g_S2s_ptrs += in_inc\n            \n            us_inc = BLOCK_SIZE_K * stride_us_k\n            S1s_ptrs += us_inc\n            U2s_ptrs += us_inc\n    \n    accumulator *= (1.0/ (2.0 * L))\n\n    out_ptrs = out_ptr + stride_out_bsize * offs_bsize[:, None] + stride_out_d2 * offs_d2[None, :]\n    out_mask = (offs_bsize[:, None] < BSIZE) & (offs_d2[None, :] < d2)\n    \n    tl.store(out_ptrs, accumulator, mask=out_mask)\n\ndef second_pass_gUS11_22(g_U1s, g_S2s, S1s, U2s):\n    assert g_U1s.shape[2] == S1s.shape[1], \"Incompatible dimensions\"\n    assert g_S2s.shape[2] == U2s.shape[1], \"Incompatible dimensions\"\n    assert g_U1s.is_contiguous(), \"Matrix A must be contiguous\"\n    assert g_S2s.is_contiguous(), \"Matrix A must be contiguous\"\n    assert S1s.is_contiguous(), \"Matrix A must be contiguous\"\n    assert U2s.is_contiguous(), \"Matrix A must be contiguous\"\n    assert S1s.stride() == U2s.stride(), \"Matrix A must be contiguous\"\n    assert g_U1s.stride() == g_S2s.stride(), \"Matrix A must be contiguous\"\n    \n    L, BSIZE, K = g_U1s.shape\n    _, _, d2 = S1s.shape\n    \n    out = torch.empty((BSIZE, d2), dtype=torch.float16, device='cuda')\n\n    stride_g_U1s2_l, stride_g_U1s2_bsize, stride_g_U1s2_k = g_U1s.stride()\n    stride_us_l, stride_us_k, stride_us_d2 = S1s.stride()\n    stride_out_bsize, stride_out_d2 = out.stride()\n    \n    grid = lambda META: (triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]) * triton.cdiv(d2, META[\"BLOCK_SIZE_d2\"]), )\n    \n    second_pass_gUS11_22_kernel[grid](\n        g_U1s, g_S2s, S1s, U2s, out,\n        BSIZE, d2, K, L,\n        stride_g_U1s2_l, stride_g_U1s2_bsize, stride_g_U1s2_k,\n        stride_us_l, stride_us_k, stride_us_d2,\n        stride_out_bsize, stride_out_d2,\n    )\n    \n    return out\n\ndevice= 'cuda'\nscale = 1\ng_U1s = g_U1s * scale\ng_S2s = g_S2s * scale\n\nd2 = 128\n\nS1s = torch.randn((L, K, d2), dtype=torch.float16, device=device) * scale\nU2s = torch.randn((L, K, d2), dtype=torch.float16, device=device) * scale\n\nstart = time.perf_counter()\ngrad = second_pass_gUS11_22(g_U1s, g_S2s, S1s, U2s)\nend = time.perf_counter()\n\nprint(f\"time : {end - start}\")\n\ntorch_output = (g_U1s.bmm(S1s).mean(0) / 2) + (g_S2s.bmm(U2s).mean(0) / 2)\n\nprint(grad)\nprint(torch_output)\n\nrtol = 1e-2\nif torch.allclose(grad, torch_output, atol=2e-1, rtol=rtol):\n    print(\"✅ Triton and Torch match\")\nelse:\n    print(\"❌ Triton and Torch differ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T22:56:12.880621Z","iopub.execute_input":"2025-03-23T22:56:12.880962Z","iopub.status.idle":"2025-03-23T22:56:14.039368Z","shell.execute_reply.started":"2025-03-23T22:56:12.880938Z","shell.execute_reply":"2025-03-23T22:56:14.038578Z"}},"outputs":[{"name":"stdout","text":"time : 1.120726488999935\ntensor([[ 143.5000,  -26.9531,  -94.9375,  ...,  -84.1875,  -49.3750,\n         -155.5000],\n        [  77.7500,  -98.7500, -152.2500,  ...,  155.6250,  115.3125,\n           95.8125],\n        [  70.5000,  -24.7031,  -60.9688,  ...,  -10.7422,  134.2500,\n           44.4062],\n        ...,\n        [  -8.1016,  -20.1875, -148.0000,  ...,  -30.9688,   73.0625,\n           21.6562],\n        [ -99.9375,  -41.3125,  -27.7188,  ...,  -83.8125,  -16.6406,\n           54.7500],\n        [ -71.3125,   -3.7520,  203.6250,  ...,   49.3438,   56.1562,\n           19.0000]], device='cuda:0', dtype=torch.float16)\ntensor([[ 143.3750,  -26.9688,  -95.0000,  ...,  -84.1875,  -49.3750,\n         -155.5000],\n        [  77.7500,  -98.8125, -152.2500,  ...,  155.6250,  115.3125,\n           95.7500],\n        [  70.5000,  -24.6875,  -60.9375,  ...,  -10.7344,  134.2500,\n           44.3750],\n        ...,\n        [  -8.1250,  -20.2031, -148.0000,  ...,  -30.9531,   73.0625,\n           21.6250],\n        [ -99.8750,  -41.3750,  -27.7344,  ...,  -83.8125,  -16.6406,\n           54.7500],\n        [ -71.3750,   -3.7500,  203.6250,  ...,   49.3125,   56.1250,\n           19.0000]], device='cuda:0', dtype=torch.float16)\n✅ Triton and Torch match\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"### grad_S1s = input.bmm(g.bmm(U1s))","metadata":{}},{"cell_type":"code","source":"@triton.autotune(\n    configs=[\n    triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 256, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_d2': 8}, num_stages=1, num_warps=4),\n    triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 256, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 64, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_d2': 32, 'BLOCK_SIZE_k': 64, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_d2': 8}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 256, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_d2': 256, 'BLOCK_SIZE_k': 64, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 256, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 64, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_d2': 64, 'BLOCK_SIZE_k': 128, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_d2': 128, 'BLOCK_SIZE_k': 32, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_d2': 8}, num_stages=4, num_warps=4)\n],\n    key=['d2', 'k', 'BSIZE', 'L'],\n)\n@triton.jit\ndef calc_grad_S1s_kernel(\n        hin_ptr, g_U1s_ptr, grad_g_S1s_ptr,\n        d2, k, BSIZE, L,\n        stride_hin_bsize, stride_hin_BSIZE,\n        stride_su_l, stride_su_BSIZE, stride_su_k,\n        stride_out_l, stride_out_bsize, stride_out_k,\n        BLOCK_SIZE_d2: tl.constexpr, BLOCK_SIZE_k: tl.constexpr, BLOCK_SIZE_BSIZE: tl.constexpr,\n        GROUP_SIZE_d2: tl.constexpr\n):\n    pid = tl.program_id(axis=1)\n    batch_id = tl.program_id(axis=0)\n    \n    num_pid_bsize = tl.cdiv(d2, BLOCK_SIZE_d2)\n    num_pid_k = tl.cdiv(k, BLOCK_SIZE_k)\n    num_pid_in_group = GROUP_SIZE_d2 * num_pid_k\n    group_id = pid // num_pid_in_group\n    first_pid_bsize = group_id * GROUP_SIZE_d2\n    group_size_bsize = min(num_pid_bsize - first_pid_bsize, GROUP_SIZE_d2)\n    pid_bsize = first_pid_bsize + ((pid % num_pid_in_group) % group_size_bsize)\n    pid_k = (pid % num_pid_in_group) // group_size_bsize\n\n    offs_bsize = pid_bsize * BLOCK_SIZE_d2 + tl.arange(0, BLOCK_SIZE_d2)\n    offs_k = pid_k *  BLOCK_SIZE_k + tl.arange(0, BLOCK_SIZE_k)\n    offs_BSIZE = tl.arange(0, BLOCK_SIZE_BSIZE)\n\n    offs_bsize = tl.max_contiguous(tl.multiple_of(offs_bsize, BLOCK_SIZE_d2), BLOCK_SIZE_d2)\n    offs_k = tl.max_contiguous(tl.multiple_of(offs_k, BLOCK_SIZE_k), BLOCK_SIZE_k)\n    offs_BSIZE = tl.max_contiguous(tl.multiple_of(offs_BSIZE, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n    \n    hin_ptrs = hin_ptr + (offs_bsize[:, None] * stride_hin_bsize + offs_BSIZE[None, :] * stride_hin_BSIZE)\n\n    su_tmp = batch_id * stride_su_l + (offs_BSIZE[:, None] * stride_su_BSIZE + offs_k[None, :] * stride_su_k)\n    g_U1s_ptrs = g_U1s_ptr + su_tmp\n\n    accumulator1 = tl.full(shape=(BLOCK_SIZE_d2, BLOCK_SIZE_k), value=0.0, dtype=tl.float32)\n    accumulator2 = tl.full(shape=(BLOCK_SIZE_d2, BLOCK_SIZE_k), value=0.0, dtype=tl.float32)\n    \n    for BSIZE_i in range(0, tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)):\n        hin_mask = (offs_bsize[:, None] < d2) & (offs_BSIZE[None, :] < BSIZE - BSIZE_i * BLOCK_SIZE_BSIZE)\n        hin = tl.load(hin_ptrs, mask=hin_mask, other=0.0)\n        \n        su_mask = (offs_BSIZE[:, None] < BSIZE - BSIZE_i * BLOCK_SIZE_BSIZE) & (offs_k[None, :] < k)\n        g_U1s = tl.load(g_U1s_ptrs, mask=su_mask, other=0.0)\n        \n        accumulator1 += tl.dot(hin, g_U1s)\n        \n        hin_ptrs += BLOCK_SIZE_BSIZE * stride_hin_BSIZE\n        g_U1s_ptrs += BLOCK_SIZE_BSIZE * stride_su_BSIZE\n\n    accumulator1 = accumulator1.to(tl.float16)\n    accumulator2 = accumulator2.to(tl.float16)\n\n    out_tmp = batch_id * stride_out_l + stride_out_bsize * offs_bsize[:, None] + stride_out_k * offs_k[None, :]\n    grad_g_S1s_ptrs = grad_g_S1s_ptr + out_tmp\n    \n    out_mask = (offs_bsize[:, None] < d2) & (offs_k[None, :] < k)\n    \n    tl.store(grad_g_S1s_ptrs, accumulator1, mask=out_mask)\n\ndef calc_grad_S1s(hin, g_U1s):\n    device = 'cuda'\n    assert hin.shape[1] == g_U1s.shape[1], \"Incompatible dimensions\"\n    assert hin.is_contiguous(), \"Matrix A must be contiguous\"\n    assert g_U1s.is_contiguous(), \"Matrix A must be contiguous\"\n    \n    d2, BSIZE = hin.shape\n    L, _, k = g_U1s.shape\n    \n    grad_g_S1s = torch.empty((L, d2, k), dtype=torch.float16, device=device)\n\n    stride_hin_bsize, stride_hin_BSIZE = hin.stride()\n    stride_su_l, stride_su_BSIZE, stride_su_k = g_U1s.stride()\n    stride_out_l, stride_out_bsize, stride_out_k = grad_g_S1s.stride()\n    \n    grid = lambda META: (L, triton.cdiv(d2, META[\"BLOCK_SIZE_d2\"]) * triton.cdiv(k, META[\"BLOCK_SIZE_k\"]), )\n    \n    calc_grad_S1s_kernel[grid](\n        hin, g_U1s, grad_g_S1s,\n        d2, k, BSIZE, L,\n        stride_hin_bsize, stride_hin_BSIZE,\n        stride_su_l, stride_su_BSIZE, stride_su_k,\n        stride_out_l, stride_out_bsize, stride_out_k\n    )\n    \n    return grad_g_S1s\n\ndevice = 'cuda'\nk = K\nd2 = 1024\n\ntorch.manual_seed(0)\nhin = torch.randn((d2, BSIZE), dtype=torch.float16, device=device)\ng_U1s = torch.randn((L, BSIZE, k), dtype=torch.float16, device=device)\n\nstart = time.perf_counter()\ngrad_g_S1s = calc_grad_S1s(hin, g_U1s)\nend = time.perf_counter()\n\nprint(f\"Time Triton: {end - start}\")\n\ntorch_output1 = (hin.unsqueeze(0).expand(L, d2, BSIZE)).bmm(g_U1s)\n\nrtol = 1e-2\nif torch.allclose(grad_g_S1s, torch_output1, atol=1e-2, rtol=rtol):\n    print(\"✅ Triton and Torch match 1\")\nelse:\n    print(\"❌ Triton and Torch differ 1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T22:59:45.763465Z","iopub.execute_input":"2025-03-23T22:59:45.763812Z","iopub.status.idle":"2025-03-23T22:59:47.142610Z","shell.execute_reply.started":"2025-03-23T22:59:45.763787Z","shell.execute_reply":"2025-03-23T22:59:47.141771Z"}},"outputs":[{"name":"stdout","text":"Time Triton: 1.3459060850000242\n✅ Triton and Torch match 1\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"###  grad_S2s = (U2s.bmm(input)).bmm(g)","metadata":{}},{"cell_type":"code","source":"@triton.autotune(\n    configs=[\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 64, 'GROUP_SIZE_K': 8}, num_stages=1, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 32, 'GROUP_SIZE_K': 8}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 256, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 64, 'BLOCK_SIZE_d2': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_BSIZE': 128, 'BLOCK_SIZE_d2': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_BSIZE': 32, 'BLOCK_SIZE_d2': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4)\n],\n    key=['K', 'd2', 'BSIZE', 'L'],\n)\n@triton.jit\ndef first_pass_U2s_hin_d2ernel(\n        hin_ptr, U2s_ptr, U2s_h_in_ptr,\n        K, d2, BSIZE, L,\n        stride_hin_d2, stride_hin_BSIZE,\n        stride_su_l, stride_su_K, stride_su_d2,\n        stride_out_l, stride_out_K, stride_out_BSIZE,\n        BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_BSIZE: tl.constexpr, BLOCK_SIZE_d2: tl.constexpr,\n        GROUP_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(axis=1)\n    batch_id = tl.program_id(axis=0)\n    \n    num_pid_K = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_BSIZE = tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)\n    num_pid_in_group = GROUP_SIZE_K * num_pid_BSIZE\n    group_id = pid // num_pid_in_group\n    first_pid_K = group_id * GROUP_SIZE_K\n    group_size_BSIZE = min(num_pid_K - first_pid_K, GROUP_SIZE_K)\n    pid_K = first_pid_K + ((pid % num_pid_in_group) % group_size_BSIZE)\n    pid_BSIZE = (pid % num_pid_in_group) // group_size_BSIZE\n\n    offs_K = pid_K * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_BSIZE = pid_BSIZE *  BLOCK_SIZE_BSIZE + tl.arange(0, BLOCK_SIZE_BSIZE)\n    offs_d2 = tl.arange(0, BLOCK_SIZE_d2)\n\n    offs_K = tl.max_contiguous(tl.multiple_of(offs_K, BLOCK_SIZE_K), BLOCK_SIZE_K)\n    offs_BSIZE = tl.max_contiguous(tl.multiple_of(offs_BSIZE, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n    offs_d2 = tl.max_contiguous(tl.multiple_of(offs_d2, BLOCK_SIZE_d2), BLOCK_SIZE_d2)\n    \n    hin_ptrs = hin_ptr + (offs_d2[:, None] * stride_hin_d2 + offs_BSIZE[None, :] * stride_hin_BSIZE)\n\n    su_tmp = batch_id * stride_su_l + (offs_K[:, None] * stride_su_K + offs_d2[None, :] * stride_su_d2)\n    U2s_ptrs = U2s_ptr + su_tmp\n\n    accumulator1 = tl.full(shape=(BLOCK_SIZE_K, BLOCK_SIZE_BSIZE), value=0.0, dtype=tl.float32)\n    \n    for d2_i in range(0, tl.cdiv(d2, BLOCK_SIZE_d2)):\n        hin_mask = (offs_d2[:, None] < d2 - d2_i * BLOCK_SIZE_d2) & (offs_BSIZE[None, :] < BSIZE)\n        hin = tl.load(hin_ptrs, mask=hin_mask, other=0.0)\n        \n        su_mask = (offs_K[:, None] < K) & (offs_d2[None, :] < d2 - d2_i * BLOCK_SIZE_d2)\n        U2s = tl.load(U2s_ptrs, mask=su_mask, other=0.0)\n        \n        accumulator1 += tl.dot(U2s, hin)\n        \n        hin_ptrs += BLOCK_SIZE_d2 * stride_hin_d2\n        U2s_ptrs += BLOCK_SIZE_d2 * stride_su_d2\n\n    accumulator1 = accumulator1.to(tl.float16)\n\n    out_tmp = batch_id * stride_out_l + stride_out_K * offs_K[:, None] + stride_out_BSIZE * offs_BSIZE[None, :]\n    U2s_h_in_ptrs = U2s_h_in_ptr + out_tmp\n    \n    out_mask = (offs_K[:, None] < K) & (offs_BSIZE[None, :] < BSIZE)\n    \n    tl.store(U2s_h_in_ptrs, accumulator1, mask=out_mask)\n\ndef first_pass_U2s_hin(U2s, hin):\n    device = 'cuda'\n    assert U2s.shape[2] == hin.shape[0], \"Incompatible dimensions\"\n    assert hin.is_contiguous(), \"Matrix A must be contiguous\"\n    assert U2s.is_contiguous(), \"Matrix A must be contiguous\"\n    \n    L, K, d2 = U2s.shape\n    _, BSIZE = hin.shape\n    \n    U2s_h_in = torch.empty((L, K, BSIZE), dtype=torch.float16, device=device)\n\n    stride_hin_d2, stride_hin_BSIZE = hin.stride()\n    stride_su_l, stride_su_K, stride_su_d2 = U2s.stride()\n    stride_out_l, stride_out_K, stride_out_BSIZE = U2s_h_in.stride()\n\n    BLOCK_SIZE_K, BLOCK_SIZE_BSIZE, BLOCK_SIZE_d2 = 128, 256, 64\n    GROUP_SIZE_K = 8\n    \n    grid = lambda META: (L, triton.cdiv(K, META[\"BLOCK_SIZE_K\"]) * triton.cdiv(BSIZE, META[\"BLOCK_SIZE_BSIZE\"]), )\n    \n    first_pass_U2s_hin_d2ernel[grid](\n        hin, U2s, U2s_h_in,\n        K, d2, BSIZE, L,\n        stride_hin_d2, stride_hin_BSIZE,\n        stride_su_l, stride_su_K, stride_su_d2,\n        stride_out_l, stride_out_K, stride_out_BSIZE\n    )\n    \n    return U2s_h_in\n\ndevice = 'cuda'\n\nd2 = 1024\n\ntorch.manual_seed(0)\nU2s = torch.randn((L, K, d2), dtype=torch.float16, device=device)\nhin = torch.randn((d2, BSIZE), dtype=torch.float16, device=device)\n\nstart = time.perf_counter()\nU2s_h_in = first_pass_U2s_hin(U2s, hin)\nend = time.perf_counter()\n\nprint(f\"Time: {end - start:.2f}s\")\n\ntorch_output1 = U2s.bmm(hin.unsqueeze(0).expand(L, d2, BSIZE))\n\nrtol = 1e-2\nif torch.allclose(U2s_h_in, torch_output1, atol=1e-2, rtol=rtol):\n    print(\"✅ Triton and Torch match 1\")\nelse:\n    print(\"❌ Triton and Torch differ 1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T23:03:10.873753Z","iopub.execute_input":"2025-03-23T23:03:10.874119Z","iopub.status.idle":"2025-03-23T23:03:12.277690Z","shell.execute_reply.started":"2025-03-23T23:03:10.874088Z","shell.execute_reply":"2025-03-23T23:03:12.276724Z"}},"outputs":[{"name":"stdout","text":"Time: 1.37s\n✅ Triton and Torch match 1\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"@triton.autotune(\n    configs=[\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 256, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_K': 8}, num_stages=1, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 256, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 64, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 32, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_d1': 64, 'BLOCK_SIZE_BSIZE': 32, 'GROUP_SIZE_K': 8}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 256, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_d1': 64, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 256, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 128, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 64, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_d1': 128, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_d1': 32, 'BLOCK_SIZE_BSIZE': 64, 'GROUP_SIZE_K': 8}, num_stages=4, num_warps=4)\n],\n    key=['K', 'BSIZE', 'd1', 'L'],\n)\n@triton.jit\ndef calc_grad_S2s_BSIZEernel(\n        g_ptr, U2s_hin_ptr, grad_S2s_ptr,\n        K, BSIZE, d1, L,\n        stride_g_BSIZE, stride_g_d1,\n        stride_su_l, stride_su_K, stride_su_BSIZE,\n        stride_out_l, stride_out_K, stride_out_d1,\n        BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_d1: tl.constexpr, BLOCK_SIZE_BSIZE: tl.constexpr,\n        GROUP_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(axis=1)\n    batch_id = tl.program_id(axis=0)\n    \n    num_pid_K = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_d1 = tl.cdiv(d1, BLOCK_SIZE_d1)\n    num_pid_in_group = GROUP_SIZE_K * num_pid_d1\n    group_id = pid // num_pid_in_group\n    first_pid_K = group_id * GROUP_SIZE_K\n    group_size_d1 = min(num_pid_K - first_pid_K, GROUP_SIZE_K)\n    pid_K = first_pid_K + ((pid % num_pid_in_group) % group_size_d1)\n    pid_d1 = (pid % num_pid_in_group) // group_size_d1\n\n    offs_K = pid_K * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_d1 = pid_d1 *  BLOCK_SIZE_d1 + tl.arange(0, BLOCK_SIZE_d1)\n    offs_BSIZE = tl.arange(0, BLOCK_SIZE_BSIZE)\n\n    offs_K = tl.max_contiguous(tl.multiple_of(offs_K, BLOCK_SIZE_K), BLOCK_SIZE_K)\n    offs_d1 = tl.max_contiguous(tl.multiple_of(offs_d1, BLOCK_SIZE_d1), BLOCK_SIZE_d1)\n    offs_BSIZE = tl.max_contiguous(tl.multiple_of(offs_BSIZE, BLOCK_SIZE_BSIZE), BLOCK_SIZE_BSIZE)\n    \n    g_ptrs = g_ptr + (offs_BSIZE[:, None] * stride_g_BSIZE + offs_d1[None, :] * stride_g_d1)\n\n    su_tmp = batch_id * stride_su_l + (offs_K[:, None] * stride_su_K + offs_BSIZE[None, :] * stride_su_BSIZE)\n    U2s_hin_ptrs = U2s_hin_ptr + su_tmp\n\n    accumulator1 = tl.full(shape=(BLOCK_SIZE_K, BLOCK_SIZE_d1), value=0.0, dtype=tl.float32)\n    \n    for BSIZE_i in range(0, tl.cdiv(BSIZE, BLOCK_SIZE_BSIZE)):\n        g_mask = (offs_BSIZE[:, None] < BSIZE - BSIZE_i * BLOCK_SIZE_BSIZE) & (offs_d1[None, :] < d1)\n        g = tl.load(g_ptrs, mask=g_mask, other=0.0)\n        \n        su_mask = (offs_K[:, None] < K) & (offs_BSIZE[None, :] < BSIZE - BSIZE_i * BLOCK_SIZE_BSIZE)\n        U2s_hin = tl.load(U2s_hin_ptrs, mask=su_mask, other=0.0)\n        \n        accumulator1 += tl.dot(U2s_hin, g)\n        \n        g_ptrs += BLOCK_SIZE_BSIZE * stride_g_BSIZE\n        U2s_hin_ptrs += BLOCK_SIZE_BSIZE * stride_su_BSIZE\n\n    accumulator1 = accumulator1.to(tl.float16)\n\n    out_tmp = batch_id * stride_out_l + stride_out_K * offs_K[:, None] + stride_out_d1 * offs_d1[None, :]\n    grad_S2s_ptrs = grad_S2s_ptr + out_tmp\n    \n    out_mask = (offs_K[:, None] < K) & (offs_d1[None, :] < d1)\n    \n    tl.store(grad_S2s_ptrs, accumulator1, mask=out_mask)\n\ndef calc_grad_S2s(U2s_hin, g):\n    device = 'cuda'\n    assert U2s_hin.shape[2] == g.shape[0], \"Incompatible dimensions\"\n    assert g.is_contiguous(), \"Matrix A must be contiguous\"\n    assert U2s_hin.is_contiguous(), \"Matrix A must be contiguous\"\n    \n    L, K, BSIZE = U2s_hin.shape\n    _, d1 = g.shape\n    \n    grad_S2s = torch.empty((L, K, d1), dtype=torch.float16, device=device)\n\n    stride_g_BSIZE, stride_g_d1 = g.stride()\n    stride_su_l, stride_su_K, stride_su_BSIZE = U2s_hin.stride()\n    stride_out_l, stride_out_K, stride_out_d1 = grad_S2s.stride()\n\n    BLOCK_SIZE_K, BLOCK_SIZE_d1, BLOCK_SIZE_BSIZE = 128, 256, 64\n    GROUP_SIZE_K = 8\n    \n    grid = lambda META: (L, triton.cdiv(K, META[\"BLOCK_SIZE_K\"]) * triton.cdiv(d1, META[\"BLOCK_SIZE_d1\"]), )\n    \n    calc_grad_S2s_BSIZEernel[grid](\n        g, U2s_hin, grad_S2s,\n        K, BSIZE, d1, L,\n        stride_g_BSIZE, stride_g_d1,\n        stride_su_l, stride_su_K, stride_su_BSIZE,\n        stride_out_l, stride_out_K, stride_out_d1\n    )\n    \n    return grad_S2s\n\ndevice = 'cuda'\n\nBSIZE = 1024\n\ntorch.manual_seed(0)\nU2s_hin = torch.randn((L, K, BSIZE), dtype=torch.float16, device=device)\ng = torch.randn((BSIZE, d1), dtype=torch.float16, device=device)\n\nstart = time.perf_counter()\ngrad_S2s = calc_grad_S2s(U2s_hin, g)\nend = time.perf_counter()\n\nprint(f\"time : {end - start}\")\n\nstart = time.perf_counter()\ntorch_output1 = U2s_hin.bmm(g.unsqueeze(0).expand(L, BSIZE, d1))\nend = time.perf_counter()\n\nprint(f\"time : {end - start}\")\n\nrtol = 1e-2\nif torch.allclose(grad_S2s, torch_output1, atol=1e-2, rtol=rtol):\n    print(\"✅ Triton and Torch match 1\")\nelse:\n    print(\"❌ Triton and Torch differ 1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T23:08:10.653918Z","iopub.execute_input":"2025-03-23T23:08:10.654230Z","iopub.status.idle":"2025-03-23T23:08:12.068197Z","shell.execute_reply.started":"2025-03-23T23:08:10.654206Z","shell.execute_reply":"2025-03-23T23:08:12.067294Z"}},"outputs":[{"name":"stdout","text":"time : 1.3820285359997797\ntime : 0.0006935760002306779\n✅ Triton and Torch match 1\n","output_type":"stream"}],"execution_count":38}]}